{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11142046,"sourceType":"datasetVersion","datasetId":6950309},{"sourceId":11524796,"sourceType":"datasetVersion","datasetId":6963199},{"sourceId":11532352,"sourceType":"datasetVersion","datasetId":7233192},{"sourceId":11532549,"sourceType":"datasetVersion","datasetId":7233298},{"sourceId":11532992,"sourceType":"datasetVersion","datasetId":7233555},{"sourceId":11533018,"sourceType":"datasetVersion","datasetId":7233568}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T17:36:49.831940Z","iopub.execute_input":"2025-05-11T17:36:49.832317Z","iopub.status.idle":"2025-05-11T17:36:50.162310Z","shell.execute_reply.started":"2025-05-11T17:36:49.832291Z","shell.execute_reply":"2025-05-11T17:36:50.161565Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/dev-testing/EXIST2025_dev.json\n/kaggle/input/datasets-translated/EXIST2025_training_task1_1_gold_soft.json\n/kaggle/input/datasets-translated/EXIST2025_training_translated_en.json\n/kaggle/input/datasets-translated/EXIST2025_training_translated_es.json\n/kaggle/input/dev-gold-hard/EXIST2025_dev_task1_1_gold_hard.json\n/kaggle/input/dev-gold-soft/EXIST2025_dev_task1_1_gold_soft.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import wandb\n\nwandb.login(key=\"a40bf999db96c982783dc52dd0594d3347848f02\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T17:36:50.162962Z","iopub.execute_input":"2025-05-11T17:36:50.163258Z","iopub.status.idle":"2025-05-11T17:36:57.727241Z","shell.execute_reply.started":"2025-05-11T17:36:50.163228Z","shell.execute_reply":"2025-05-11T17:36:57.726689Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfaisalsara124\u001b[0m (\u001b[33mfaisalsara124-habib-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# Newer version","metadata":{}},{"cell_type":"code","source":"print(data_en[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T18:25:30.534675Z","iopub.execute_input":"2025-05-11T18:25:30.535350Z","iopub.status.idle":"2025-05-11T18:25:30.540292Z","shell.execute_reply.started":"2025-05-11T18:25:30.535327Z","shell.execute_reply":"2025-05-11T18:25:30.539511Z"}},"outputs":[{"name":"stdout","text":"{'id': '100001', 'tweet': '@TheChiflis Ignora to the other, he\\'s a jerk.The problem with this youtuber denounces harassment... when it doesn\\'t affect left-wing people.For example, in his video about the gamergate he presents as \"normal\" the harassment that Fisher, Anita or ZÃ¶ey receive when there were even bomb threats.'}\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import json\nimport torch\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import (\n    BertTokenizer,\n    BertForSequenceClassification,\n    AutoModelForSequenceClassification,\n    Trainer,\n    TrainingArguments\n)\nfrom torch.utils.data import Dataset\n\n# === Load Tweets ===\nwith open(\"/kaggle/working/augment_en.json\", \"r\", encoding=\"utf-8\") as f:\n    data_en = json.load(f)\n\nwith open(\"/kaggle/working/augment_es.json\", \"r\", encoding=\"utf-8\") as f:\n    data_es = json.load(f)\n\n# === Load gold_soft_train ===\nwith open(\"/kaggle/working/augment_gold.json\", \"r\", encoding=\"utf-8\") as f:\n    gold_soft = json.load(f)\n\n# Convert gold_soft to a dict for fast access\ngold_soft_dict = {entry[\"id\"]: entry[\"value\"] for entry in gold_soft}\n\n# Define binary labels\nCORRECT_LABELS = [\"YES\", \"NO\"]\n\n# === Process Tweets with Corresponding Soft Labels ===\ndef process_data_with_soft_labels(data):\n    tweets = []\n    labels = []\n    ids = []\n\n    for entry in data:\n        tweet_id = entry[\"id\"]\n        tweet = entry[\"tweet\"]\n\n        if tweet_id not in gold_soft_dict:\n            continue  # Skip if soft label not found\n\n        soft_label_dict = gold_soft_dict[tweet_id]\n\n        # Build binary soft label vector [YES_score, NO_score]\n        soft_label_vector = [soft_label_dict.get(label, 0.0) for label in CORRECT_LABELS]\n\n        tweets.append(tweet)\n        labels.append(soft_label_vector)\n        ids.append(tweet_id)\n\n    return tweets, labels, ids\n\n# Process both English and Spanish tweets\ntweets_en, labels_en, ids_en = process_data_with_soft_labels(data_en)\ntweets_es, labels_es, ids_es = process_data_with_soft_labels(data_es)\n\n# === Tokenizer ===\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n\n# === Custom Dataset Class ===\nclass TweetDataset(Dataset):\n    def __init__(self, texts, labels, ids, tokenizer, max_length=256):\n        self.texts = texts\n        self.labels = labels\n        self.ids = ids\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        tweet_id = self.ids[idx]\n        labels = torch.tensor(self.labels[idx], dtype=torch.float)\n        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n\n        return {\n            \"id\": tweet_id,\n            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n            \"labels\": labels\n        }\n\n# === Train-validation split ===\ndef get_datasets(tweets, labels, ids):\n    train_texts, val_texts, train_labels, val_labels, train_ids, val_ids = train_test_split(\n        tweets, labels, ids, test_size=0.2, random_state=42\n    )\n    train_dataset = TweetDataset(train_texts, train_labels, train_ids, tokenizer)\n    val_dataset = TweetDataset(val_texts, val_labels, val_ids, tokenizer)\n    return train_dataset, val_dataset\n\n# === Create datasets ===\ntrain_dataset_en, val_dataset_en = get_datasets(tweets_en, labels_en, ids_en)\ntrain_dataset_es, val_dataset_es = get_datasets(tweets_es, labels_es, ids_es)\n\n# === Train Model ===\ndef train_model(train_dataset, val_dataset, output_dir):\n    model = AutoModelForSequenceClassification.from_pretrained(\n        \"bert-base-multilingual-cased\",\n        num_labels=2\n        \n    )\n\n    training_args = TrainingArguments(\n    output_dir=output_dir,\n    do_train=True,\n    do_eval=True,\n    num_train_epochs=4,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    logging_dir=\"./logs\",\n    logging_steps=250,\n    save_total_limit=2\n)\n\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset\n    )\n\n    trainer.train()\n    return trainer\n\n# === Train English and Spanish models ===\ntrainer_en = train_model(train_dataset_en, val_dataset_en, output_dir=\"./results/en\")\ntrainer_es = train_model(train_dataset_es, val_dataset_es, output_dir=\"./results/es\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T18:25:58.420991Z","iopub.execute_input":"2025-05-11T18:25:58.421651Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='356' max='1312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 356/1312 05:32 < 14:58, 1.06 it/s, Epoch 1.08/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>250</td>\n      <td>0.581300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"# **Dev Testing starts from here**","metadata":{}},{"cell_type":"code","source":"with open(\"/kaggle/input/dev-testing/EXIST2025_dev.json\", \"r\", encoding=\"utf-8\") as f:\n    dev_data = json.load(f)\n\n# Extract tweets and IDs\ndev_tweets = [entry[\"tweet\"] for entry in dev_data.values()]\ndev_ids = [entry[\"id_EXIST\"] for entry in dev_data.values()]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T16:04:00.201583Z","iopub.execute_input":"2025-05-11T16:04:00.201847Z","iopub.status.idle":"2025-05-11T16:04:00.246548Z","shell.execute_reply.started":"2025-05-11T16:04:00.201826Z","shell.execute_reply":"2025-05-11T16:04:00.245949Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import json\n\n# Load the dev dataset\nwith open(\"/kaggle/input/dev-testing/EXIST2025_dev.json\", \"r\", encoding=\"utf-8\") as f:\n    dev_data = json.load(f)\n\n# Split into English & Spanish\nenglish_dev_tweets = []\nenglish_dev_ids = []\nspanish_dev_tweets = []\nspanish_dev_ids = []\n\nfor entry in dev_data.values():\n    tweet_id = entry[\"id_EXIST\"]\n    tweet = entry[\"tweet\"]\n    lang = entry[\"lang\"]\n\n    if lang == \"en\":\n        english_dev_tweets.append(tweet)\n        english_dev_ids.append(tweet_id)\n    elif lang == \"es\":\n        spanish_dev_tweets.append(tweet)\n        spanish_dev_ids.append(tweet_id)\n\n# Debugging: Check split sizes\nprint(f\"English Dev Samples: {len(english_dev_tweets)}\")\nprint(f\"Spanish Dev Samples: {len(spanish_dev_tweets)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T16:04:04.959514Z","iopub.execute_input":"2025-05-11T16:04:04.960024Z","iopub.status.idle":"2025-05-11T16:04:05.510895Z","shell.execute_reply.started":"2025-05-11T16:04:04.960004Z","shell.execute_reply":"2025-05-11T16:04:05.510221Z"}},"outputs":[{"name":"stdout","text":"English Dev Samples: 489\nSpanish Dev Samples: 549\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import os\nfrom transformers import BertForSequenceClassification\n\n# Function to get the latest checkpoint\ndef get_latest_checkpoint(directory=\"./results\"):\n    checkpoints = [d for d in os.listdir(directory) if d.startswith(\"checkpoint-\")]\n    if not checkpoints:\n        raise ValueError(f\"No checkpoints found in {directory}\")\n    latest_checkpoint = sorted(checkpoints, key=lambda x: int(x.split('-')[-1]))[-1]\n    return os.path.join(directory, latest_checkpoint)\n\n# Load the best model checkpoint for English and Spanish\nlatest_checkpoint_en = get_latest_checkpoint(\"./results/en\")\nlatest_checkpoint_es = get_latest_checkpoint(\"./results/es\")\n\nprint(f\"Using latest checkpoint for English: {latest_checkpoint_en}\")\nprint(f\"Using latest checkpoint for Spanish: {latest_checkpoint_es}\")\n\n# Load models\nmodel_en = BertForSequenceClassification.from_pretrained(latest_checkpoint_en)\nmodel_es = BertForSequenceClassification.from_pretrained(latest_checkpoint_es)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T16:04:08.795119Z","iopub.execute_input":"2025-05-11T16:04:08.795679Z","iopub.status.idle":"2025-05-11T16:04:08.968490Z","shell.execute_reply.started":"2025-05-11T16:04:08.795656Z","shell.execute_reply":"2025-05-11T16:04:08.967767Z"}},"outputs":[{"name":"stdout","text":"Using latest checkpoint for English: ./results/en/checkpoint-519\nUsing latest checkpoint for Spanish: ./results/es/checkpoint-519\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# This is Soft Soft","metadata":{}},{"cell_type":"code","source":"\ndef predict_on_dev(tweets, ids, model, tokenizer, label_classes, output_file):\n    model.eval()\n    results = []\n\n    for tweet, tweet_id in zip(tweets, ids):\n        encoding = tokenizer(tweet, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n\n        with torch.no_grad():\n            outputs = model(**encoding)\n\n        logits = outputs.logits.squeeze()\n        probs = torch.sigmoid(logits).cpu().numpy()\n\n        # Convert probabilities to dictionary format and sort by highest probability\n        soft_label_dict = {label_classes[i]: float(probs[i]) for i in range(len(label_classes))}\n        sorted_soft_label_dict = dict(sorted(soft_label_dict.items(), key=lambda item: item[1], reverse=True))  # Sort descending\n\n        results.append({\n            \"test_case\": \"EXIST2025\",\n            \"id\": tweet_id,\n            \"value\": sorted_soft_label_dict  # Rename \"soft_label\" to \"value\" and sort it\n        })\n\n    # Save results\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(results, f, indent=4)\n\n    print(f\"Predictions saved to {output_file}\")\n    \nlabel_classes = CORRECT_LABELS\n\n\n# Run predictions\npredict_on_dev(english_dev_tweets, english_dev_ids, model_en, tokenizer, label_classes, \"EXIST2025_dev_predictions_en.json\")\npredict_on_dev(spanish_dev_tweets, spanish_dev_ids, model_es, tokenizer, label_classes, \"EXIST2025_dev_predictions_es.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T16:04:11.793721Z","iopub.execute_input":"2025-05-11T16:04:11.794405Z","iopub.status.idle":"2025-05-11T16:08:19.678001Z","shell.execute_reply.started":"2025-05-11T16:04:11.794380Z","shell.execute_reply":"2025-05-11T16:08:19.677262Z"}},"outputs":[{"name":"stdout","text":"Predictions saved to EXIST2025_dev_predictions_en.json\nPredictions saved to EXIST2025_dev_predictions_es.json\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# This be Hard Hard","metadata":{}},{"cell_type":"code","source":"def predict_hard_labels_from_soft_model(tweets, ids, model, tokenizer, label_classes, output_file):\n    \"\"\"\n    Uses the soft model to predict a single hard label: \"YES\" or \"NO\".\n    - Assigns the label with the higher probability.\n    \"\"\"\n    model.eval()\n    results = []\n\n    for tweet, tweet_id in zip(tweets, ids):\n        encoding = tokenizer(tweet, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n\n        with torch.no_grad():\n            outputs = model(**encoding)\n\n        logits = outputs.logits.squeeze()\n        probs = torch.sigmoid(logits).cpu().numpy()\n\n        # Pick the label with the highest probability (YES or NO)\n        max_index = int(probs.argmax())\n        predicted_label = label_classes[max_index]\n\n        results.append({\n            \"test_case\": \"EXIST2025\",\n            \"id\": tweet_id,\n            \"value\": [predicted_label]  # Only one label\n        })\n\n    # Save results\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(results, f, indent=4)\n\n    print(f\"Hard label predictions saved to {output_file}\")\n\npredict_hard_labels_from_soft_model(english_dev_tweets, english_dev_ids, model_en, tokenizer, label_classes, \"EXIST2025_dev_predictions_hard_en.json\")\npredict_hard_labels_from_soft_model(spanish_dev_tweets, spanish_dev_ids, model_es, tokenizer, label_classes, \"EXIST2025_dev_predictions_hard_es.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T02:44:52.027030Z","iopub.execute_input":"2025-04-24T02:44:52.027555Z","iopub.status.idle":"2025-04-24T02:49:06.339733Z","shell.execute_reply.started":"2025-04-24T02:44:52.027530Z","shell.execute_reply":"2025-04-24T02:49:06.338510Z"}},"outputs":[{"name":"stdout","text":"Hard label predictions saved to EXIST2025_dev_predictions_hard_en.json\nHard label predictions saved to EXIST2025_dev_predictions_hard_es.json\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Merging soft models ka dev set predictions","metadata":{}},{"cell_type":"code","source":"import json\n\n# Load the Spanish predictions\nwith open(\"/kaggle/working/EXIST2025_dev_predictions_es.json\", \"r\", encoding=\"utf-8\") as f:\n    es_data = json.load(f)\n\n# Load the English predictions\nwith open(\"/kaggle/working/EXIST2025_dev_predictions_en.json\", \"r\", encoding=\"utf-8\") as f:\n    en_data = json.load(f)\n\n# Assuming both files contain lists of predictions, merge them\nif isinstance(es_data, list) and isinstance(en_data, list):\n    merged_data = es_data + en_data\nelse:\n    raise ValueError(\"JSON structure is not a list. Ensure both files contain lists.\")\n\n# Save to a new file\noutput_filename = \"EXIST2025_dev_predictions_merged_soft.json\"\nwith open(output_filename, \"w\", encoding=\"utf-8\") as f:\n    json.dump(merged_data, f, indent=4, ensure_ascii=False)\n\nprint(f\"Merging complete! Saved to {output_filename}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T16:08:36.762111Z","iopub.execute_input":"2025-05-11T16:08:36.762878Z","iopub.status.idle":"2025-05-11T16:08:36.786721Z","shell.execute_reply.started":"2025-05-11T16:08:36.762838Z","shell.execute_reply":"2025-05-11T16:08:36.785944Z"}},"outputs":[{"name":"stdout","text":"Merging complete! Saved to EXIST2025_dev_predictions_merged_soft.json\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import json\nimport numpy as np\n\n# Load your predictions file\nwith open('EXIST2025_dev_predictions_merged_soft.json', 'r') as f:\n    predictions = json.load(f)\n\n# Define the snapping values (multiples of 1/6)\nsnap_vals = np.array([i / 6 for i in range(7)])  # [0.0, 0.1667, ..., 1.0]\n\ndef snap_to_nearest_sixth(value):\n    return float(snap_vals[np.argmin(np.abs(snap_vals - value))])\n\n# Snap each value in the 'value' dict\nfor entry in predictions:\n    entry['value'] = {k: snap_to_nearest_sixth(v) for k, v in entry['value'].items()}\n\n# Save the snapped predictions to a new file\nwith open('EXIST2025_dev_predictions_snapped_soft.json', 'w') as f:\n    json.dump(predictions, f, indent=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T16:08:41.785166Z","iopub.execute_input":"2025-05-11T16:08:41.785475Z","iopub.status.idle":"2025-05-11T16:08:41.813594Z","shell.execute_reply.started":"2025-05-11T16:08:41.785453Z","shell.execute_reply":"2025-05-11T16:08:41.812902Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"# Merging hard models ka dev set predictions","metadata":{}},{"cell_type":"code","source":"import json\n\n# Load the Spanish predictions\nwith open(\"/kaggle/working/EXIST2025_dev_predictions_hard_es.json\", \"r\", encoding=\"utf-8\") as f:\n    es_data = json.load(f)\n\n# Load the English predictions\nwith open(\"/kaggle/working/EXIST2025_dev_predictions_hard_en.json\", \"r\", encoding=\"utf-8\") as f:\n    en_data = json.load(f)\n\n# Assuming both files contain lists of predictions, merge them\nif isinstance(es_data, list) and isinstance(en_data, list):\n    merged_data = es_data + en_data\nelse:\n    raise ValueError(\"JSON structure is not a list. Ensure both files contain lists.\")\n\n# Save to a new file\noutput_filename = \"EXIST2025_dev_predictions_merged_hard.json\"\nwith open(output_filename, \"w\", encoding=\"utf-8\") as f:\n    json.dump(merged_data, f, indent=4, ensure_ascii=False)\n\nprint(f\"Merging complete! Saved to {output_filename}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T02:49:18.623147Z","iopub.execute_input":"2025-04-24T02:49:18.623407Z","iopub.status.idle":"2025-04-24T02:49:18.638558Z","shell.execute_reply.started":"2025-04-24T02:49:18.623388Z","shell.execute_reply":"2025-04-24T02:49:18.638002Z"}},"outputs":[{"name":"stdout","text":"Merging complete! Saved to EXIST2025_dev_predictions_merged_hard.json\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import json\n\ndef convert_prediction_format(input_file, output_file):\n    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n        predictions = json.load(f)\n\n    converted = []\n    for entry in predictions:\n        # Convert the \"value\" list to a single string (first label only)\n        new_entry = {\n            \"test_case\": entry[\"test_case\"],\n            \"id\": entry[\"id\"],\n            \"value\": entry[\"value\"][0] if isinstance(entry[\"value\"], list) else entry[\"value\"]\n        }\n        converted.append(new_entry)\n\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(converted, f, indent=4)\n\n    print(f\"Predictions converted to gold format and saved to {output_file}\")\n\nconvert_prediction_format(\"EXIST2025_dev_predictions_merged_hard.json\", \"EXIST2025_dev_predictions_merged_hard_flat.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T02:49:22.914154Z","iopub.execute_input":"2025-04-24T02:49:22.914837Z","iopub.status.idle":"2025-04-24T02:49:22.927858Z","shell.execute_reply.started":"2025-04-24T02:49:22.914815Z","shell.execute_reply":"2025-04-24T02:49:22.927271Z"}},"outputs":[{"name":"stdout","text":"Predictions converted to gold format and saved to EXIST2025_dev_predictions_merged_hard_flat.json\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# AEDA Technique","metadata":{}},{"cell_type":"code","source":"pip install nltk transformers torch tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T15:27:10.802985Z","iopub.execute_input":"2025-05-11T15:27:10.803163Z","iopub.status.idle":"2025-05-11T15:28:30.030647Z","shell.execute_reply.started":"2025-05-11T15:27:10.803149Z","shell.execute_reply":"2025-05-11T15:28:30.029960Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m:03\u001b[0mmm\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Rija AEDA ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport torch\nimport random\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer\nfrom torch.utils.data import Dataset\nfrom collections import defaultdict\n\n# === AEDA helper ===\nPUNCTUATIONS = ['.', ',', '!', '?', ';', ':']\ndef aeda(sentence, num_insertions=3):\n    words = sentence.split()\n    if not words:\n        return sentence\n    new_words = words.copy()\n    for _ in range(num_insertions):\n        insert_pos = random.randint(0, len(new_words))\n        punct = random.choice(PUNCTUATIONS)\n        new_words.insert(insert_pos, punct)\n    return ' '.join(new_words)\n\n# === Load Data ===\nwith open(\"/kaggle/input/datasets-translated/EXIST2025_training_translated_en.json\", \"r\", encoding=\"utf-8\") as f:\n    data_en = json.load(f)\nwith open(\"/kaggle/input/datasets-translated/EXIST2025_training_translated_es.json\", \"r\", encoding=\"utf-8\") as f:\n    data_es = json.load(f)\nwith open(\"/kaggle/input/datasets-translated/EXIST2025_training_task1_1_gold_soft.json\", \"r\", encoding=\"utf-8\") as f:\n    gold_soft = json.load(f)\n\ngold_soft_dict = {entry[\"id\"]: entry[\"value\"] for entry in gold_soft}\nlabel_classes = [\n    \"IDEOLOGICAL-INEQUALITY\",\n    \"MISOGYNY-NON-SEXUAL-VIOLENCE\",\n    \"OBJECTIFICATION\",\n    \"SEXUAL-VIOLENCE\",\n    \"STEREOTYPING-DOMINANCE\",\n    \"NO\"  # Represents non-sexist tweets (previously \"-\")\n]\n\n# === Count Label Distribution ===\nlabel_counts = defaultdict(int)\nfor soft in gold_soft_dict.values():\n    max_label = max(soft, key=soft.get)\n    label_counts[max_label] += 1\n\n# === Identify underrepresented labels (you can tune this threshold) ===\navg_count = np.mean(list(label_counts.values()))\nunderrepresented_labels = [label for label, count in label_counts.items() if count < avg_count]\n\n# === Process Tweets & Augment Underrepresented Only ===\ndef process_data_with_soft_labels(data, augment=True, augment_n=2):\n    tweets, labels, ids = [], [], []\n\n    for entry in data.values():\n        tweet_id = entry[\"id_EXIST\"]\n        tweet = entry[\"tweet\"]\n\n        if tweet_id not in gold_soft_dict:\n            continue\n\n        soft_label_dict = gold_soft_dict[tweet_id]\n        soft_label_vector = [soft_label_dict.get(label, 0.0) for label in label_classes]\n\n        # Original tweet\n        tweets.append(tweet)\n        labels.append(soft_label_vector)\n        ids.append(tweet_id)\n\n        # Determine primary label\n        main_label = max(soft_label_dict, key=soft_label_dict.get)\n\n        # Augment only if underrepresented\n        if augment and main_label in underrepresented_labels:\n            for i in range(augment_n):\n                augmented_tweet = aeda(tweet)\n                tweets.append(augmented_tweet)\n                labels.append(soft_label_vector)\n                ids.append(f\"{tweet_id}_aug{i+1}\")\n\n    return tweets, labels, ids\n\n# Process English and Spanish data\ntweets_en, labels_en, ids_en = process_data_with_soft_labels(data_en)\ntweets_es, labels_es, ids_es = process_data_with_soft_labels(data_es)\n\nclass TweetDataset(Dataset):\n    def __init__(self, texts, labels, ids, tokenizer, max_length=256):\n        self.texts = texts\n        self.labels = labels\n        self.ids = ids\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        tweet_id = self.ids[idx]\n        label = torch.tensor(self.labels[idx], dtype=torch.float)\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n\n        return {\n            \"id\": tweet_id,\n            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n            \"labels\": label\n        }\n\n# === Train-validation split ===\ndef get_datasets(tweets, labels, ids):\n    train_texts, val_texts, train_labels, val_labels, train_ids, val_ids = train_test_split(\n        tweets, labels, ids, test_size=0.2, random_state=42\n    )\n    train_dataset = TweetDataset(train_texts, train_labels, train_ids, tokenizer)\n    val_dataset = TweetDataset(val_texts, val_labels, val_ids, tokenizer)\n    return train_dataset, val_dataset\n\n# === Create datasets ===\ntrain_dataset_en, val_dataset_en = get_datasets(tweets_en, labels_en, ids_en)\ntrain_dataset_es, val_dataset_es = get_datasets(tweets_es, labels_es, ids_es)\n\nprint(f\"English train set size: {len(train_dataset_en)} (with selective augmentation)\")\nprint(f\"Spanish train set size: {len(train_dataset_es)} (with selective augmentation)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T18:22:28.960486Z","iopub.execute_input":"2025-05-11T18:22:28.960986Z","iopub.status.idle":"2025-05-11T18:22:30.330352Z","shell.execute_reply.started":"2025-05-11T18:22:28.960962Z","shell.execute_reply":"2025-05-11T18:22:30.329592Z"}},"outputs":[{"name":"stdout","text":"English train set size: 10496 (with selective augmentation)\nSpanish train set size: 10496 (with selective augmentation)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# from transformers import BertForSequenceClassification\n\n# model = BertForSequenceClassification.from_pretrained(\"./results/en\")  # or ./results/es\nimport os\n\ndef get_latest_checkpoint(path):\n    checkpoints = [d for d in os.listdir(path) if d.startswith(\"checkpoint-\")]\n    checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n    return os.path.join(path, checkpoints[-1]) if checkpoints else None\n\nlatest_ckpt = get_latest_checkpoint(\"./results/en\")\nmodel = BertForSequenceClassification.from_pretrained(latest_ckpt)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T17:28:31.652968Z","iopub.execute_input":"2025-05-11T17:28:31.653257Z","iopub.status.idle":"2025-05-11T17:28:31.752603Z","shell.execute_reply.started":"2025-05-11T17:28:31.653234Z","shell.execute_reply":"2025-05-11T17:28:31.751876Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def fine_tune_model(model, train_dataset, val_dataset, output_dir):\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        do_train=True,\n        do_eval=True,\n        num_train_epochs=2,  # You can reduce this, since it's fine-tuning\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        logging_dir=\"./logs\",\n        logging_steps=150,\n        save_total_limit=2\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset\n    )\n\n    trainer.train()\n    return trainer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T17:28:35.870601Z","iopub.execute_input":"2025-05-11T17:28:35.870873Z","iopub.status.idle":"2025-05-11T17:28:35.876882Z","shell.execute_reply.started":"2025-05-11T17:28:35.870851Z","shell.execute_reply":"2025-05-11T17:28:35.876051Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"print(train_dataset_en[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T16:20:51.750421Z","iopub.execute_input":"2025-05-11T16:20:51.751330Z","iopub.status.idle":"2025-05-11T16:20:51.758429Z","shell.execute_reply.started":"2025-05-11T16:20:51.751294Z","shell.execute_reply":"2025-05-11T16:20:51.757393Z"}},"outputs":[{"name":"stdout","text":"{'id': '200294_aug2', 'text': '@mtgreenee Why, you have personally demonstrated your capabilities and desire , to use advanced ; ! weapons. Are you saying your not as good as a man when it comes to warfare? Pick a lane.', 'labels': tensor([0.8333, 0.1667])}\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"latest_ckpt = get_latest_checkpoint(\"./results/en\")\n# Load from base training checkpoint\n# model_en = BertForSequenceClassification.from_pretrained(latest_ckpt)\nmodel_en = BertForSequenceClassification.from_pretrained(\n    latest_ckpt,\n    num_labels=2,\n    problem_type=\"multi_label_classification\"\n)\nlatest_ckpt = get_latest_checkpoint(\"./results/es\")\n# model_es = BertForSequenceClassification.from_pretrained(latest_ckpt, num_labels=2)\nmodel_es = BertForSequenceClassification.from_pretrained(\n    latest_ckpt,\n    num_labels=2,\n    problem_type=\"multi_label_classification\"\n)\n\n# Fine-tune on augmented data\ntrainer_en_aug = fine_tune_model(model_en, train_dataset_en, val_dataset_en, output_dir=\"./results/en_finetuned\")\ntrainer_es_aug = fine_tune_model(model_es, train_dataset_es, val_dataset_es, output_dir=\"./results/es_finetuned\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install pyevall","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T17:48:32.888852Z","iopub.execute_input":"2025-05-08T17:48:32.889463Z","iopub.status.idle":"2025-05-08T17:49:17.610905Z","shell.execute_reply.started":"2025-05-08T17:48:32.889443Z","shell.execute_reply":"2025-05-08T17:49:17.610083Z"}},"outputs":[{"name":"stdout","text":"Collecting pyevall\n  Downloading PyEvALL-0.1.78.tar.gz (39 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting jsbeautifier==1.14.9 (from pyevall)\n  Downloading jsbeautifier-1.14.9.tar.gz (75 kB)\n\u001b[2K     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: jsonschema==4.23.0 in /usr/local/lib/python3.11/dist-packages (from pyevall) (4.23.0)\nRequirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (from pyevall) (1.26.4)\nRequirement already satisfied: pandas==2.2.3 in /usr/local/lib/python3.11/dist-packages (from pyevall) (2.2.3)\nCollecting setuptools==69.5.1 (from pyevall)\n  Downloading setuptools-69.5.1-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.11/dist-packages (from pyevall) (0.9.0)\nRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from jsbeautifier==1.14.9->pyevall) (1.17.0)\nCollecting editorconfig>=0.12.2 (from jsbeautifier==1.14.9->pyevall)\n  Downloading EditorConfig-0.17.0-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema==4.23.0->pyevall) (25.3.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema==4.23.0->pyevall) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema==4.23.0->pyevall) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema==4.23.0->pyevall) (0.22.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->pyevall) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->pyevall) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->pyevall) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->pyevall) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->pyevall) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->pyevall) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.3->pyevall) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.3->pyevall) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.3->pyevall) (2025.2)\nRequirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema==4.23.0->pyevall) (4.13.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.26.4->pyevall) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.26.4->pyevall) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy==1.26.4->pyevall) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy==1.26.4->pyevall) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy==1.26.4->pyevall) (2024.2.0)\nDownloading setuptools-69.5.1-py3-none-any.whl (894 kB)\n\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m894.6/894.6 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading EditorConfig-0.17.0-py3-none-any.whl (16 kB)\nBuilding wheels for collected packages: pyevall, jsbeautifier\n  Building wheel for pyevall (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pyevall: filename=PyEvALL-0.1.78-py3-none-any.whl size=34777 sha256=2d636256d467108c6d85ca895b690e7c5fc8913dbb25a87df2f69e73eb19226e\n  Stored in directory: /root/.cache/pip/wheels/1f/71/ae/bf04901d4d2616e45aee909fba432a0ef67dd0d276f86eaab7\n  Building wheel for jsbeautifier (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for jsbeautifier: filename=jsbeautifier-1.14.9-py3-none-any.whl size=94155 sha256=bfc93093cdcaca2c3427c0cc37cb063475f2670b03f825a9d01a5d2baf7f27b3\n  Stored in directory: /root/.cache/pip/wheels/b9/bd/4d/f11d410af889a445a4f36209ecc5fa5802aa9040a0bef2b235\nSuccessfully built pyevall jsbeautifier\nInstalling collected packages: editorconfig, setuptools, jsbeautifier, pyevall\n  Attempting uninstall: setuptools\n    Found existing installation: setuptools 75.1.0\n    Uninstalling setuptools-75.1.0:\n      Successfully uninstalled setuptools-75.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed editorconfig-0.17.0 jsbeautifier-1.14.9 pyevall-0.1.78 setuptools-69.5.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# Step 1: Use your existing trainer to generate predictions\npredictions_output = model_en.predict(val_dataset_en)\n\n# Step 2: Convert logits to probabilities using softmax\nimport torch\nimport torch.nn.functional as F\n\nsoft_preds = F.softmax(torch.tensor(predictions_output.predictions), dim=1).tolist()\n\n# Step 3: Get tweet IDs\n# tweet_ids = val_dataset_en.ids  # This must match how you constructed val_dataset_en\n# Ensure IDs and predictions are unique and aligned\nseen_ids = set()\nfiltered_preds = []\nfiltered_ids = []\n\nfor i, tweet_id in enumerate(val_dataset_en.ids):\n    if tweet_id not in seen_ids:\n        seen_ids.add(tweet_id)\n        filtered_ids.append(tweet_id)\n        filtered_preds.append(soft_preds[i])\n\n\n# Step 4: Format predictions for PyEvALL\npredictions_json = []\nfor tweet_id, probs in zip(filtered_ids, filtered_preds):\n    predictions_json.append({\n        \"id\": tweet_id,\n        \"test_case\": \"EXIST2025\",  # Use exact match from gold\n        \"value\": {\n            \"sexist\": probs[0],\n            \"not-sexist\": probs[1]\n        }\n    })\n\n# Step 5: Save predictions to JSON\noutput_path = \"/kaggle/working/EXIST2025_dev_predictions_merged_soft_aeda.json\"\nwith open(output_path, \"w\") as f:\n    json.dump(predictions_json, f, indent=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T18:06:07.905310Z","iopub.execute_input":"2025-05-08T18:06:07.905939Z","iopub.status.idle":"2025-05-08T18:06:58.246488Z","shell.execute_reply.started":"2025-05-08T18:06:07.905915Z","shell.execute_reply":"2025-05-08T18:06:58.245957Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"import json\nwith open(\"/kaggle/input/dev-gold-soft/EXIST2025_dev_task1_1_gold_soft.json\") as f:\n    print(json.load(f)[:5])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T18:03:20.203495Z","iopub.execute_input":"2025-05-08T18:03:20.204285Z","iopub.status.idle":"2025-05-08T18:03:20.222945Z","shell.execute_reply.started":"2025-05-08T18:03:20.204260Z","shell.execute_reply":"2025-05-08T18:03:20.222193Z"}},"outputs":[{"name":"stdout","text":"[{'test_case': 'EXIST2025', 'id': '300001', 'value': {'NO': 0.5, 'YES': 0.5}}, {'test_case': 'EXIST2025', 'id': '300002', 'value': {'YES': 0.8333333333333334, 'NO': 0.16666666666666666}}, {'test_case': 'EXIST2025', 'id': '300003', 'value': {'NO': 1.0, 'YES': 0.0}}, {'test_case': 'EXIST2025', 'id': '300004', 'value': {'NO': 0.16666666666666666, 'YES': 0.8333333333333334}}, {'test_case': 'EXIST2025', 'id': '300005', 'value': {'NO': 0.8333333333333334, 'YES': 0.16666666666666666}}]\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"from pyevall.evaluation import PyEvALLEvaluation\nfrom pyevall.utils.utils import PyEvALLUtils\n\n# Use the new AEDA-based model predictions\npredictions = \"/kaggle/working/EXIST2025_dev_predictions_merged_soft_aeda.json\"\ngold = \"/kaggle/input/dev-gold-soft/EXIST2025_dev_task1_1_gold_soft.json\"  # No change\n\nevaluator = PyEvALLEvaluation()\n\nparams = dict()\nparams[PyEvALLUtils.PARAM_REPORT] = PyEvALLUtils.PARAM_OPTION_REPORT_EMBEDDED\n\nmetrics = [\"ICMSoft\", \"ICMSoftNorm\", \"CrossEntropy\"]\n\n# Run evaluation\nreport = evaluator.evaluate(predictions, gold, metrics, **params)\nreport.print_report()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T18:07:05.569767Z","iopub.execute_input":"2025-05-08T18:07:05.570523Z","iopub.status.idle":"2025-05-08T18:07:10.905936Z","shell.execute_reply.started":"2025-05-08T18:07:05.570499Z","shell.execute_reply":"2025-05-08T18:07:10.905134Z"}},"outputs":[{"name":"stdout","text":"2025-05-08 18:07:05,575 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICMSoft', 'ICMSoftNorm', 'CrossEntropy']\n2025-05-08 18:07:06,396 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Soft evaluation method\n2025-05-08 18:07:07,637 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM-Soft Normalized evaluation method\n2025-05-08 18:07:07,640 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Soft evaluation method\n2025-05-08 18:07:09,348 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Soft evaluation method\n2025-05-08 18:07:10,193 - pyevall.metrics.metrics - INFO -             evaluate() - Executing Cross Entropy evaluation method\n{\n  \"metrics\": {\n    \"ICMSoft\": {\n      \"name\": \"Information Contrast Model Soft\",\n      \"acronym\": \"ICM-Soft\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": -3.0950313012228485\n        }],\n        \"average_per_test_case\": -3.0950313012228485\n      }\n    },\n    \"ICMSoftNorm\": {\n      \"name\": \"Normalized Information Contrast Model Soft\",\n      \"acronym\": \"ICM-Soft-Norm\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 5.739382469891044e-16\n        }],\n        \"average_per_test_case\": 5.739382469891044e-16\n      }\n    },\n    \"CrossEntropy\": {\n      \"name\": \"Cross Entropy\",\n      \"acronym\": \"CE\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 0.0\n        }],\n        \"average_per_test_case\": 0.0\n      }\n    }\n  },\n  \"files\": {\n    \"EXIST2025_dev_predictions_merged_soft_aeda.json\": {\n      \"name\": \"EXIST2025_dev_predictions_merged_soft_aeda.json\",\n      \"status\": \"OK\",\n      \"gold\": false,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_predictions_merged_soft_aeda.json.\",\n      \"errors\": {}\n    },\n    \"EXIST2025_dev_task1_1_gold_soft.json\": {\n      \"name\": \"EXIST2025_dev_task1_1_gold_soft.json\",\n      \"status\": \"OK\",\n      \"gold\": true,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_task1_1_gold_soft.json.\",\n      \"errors\": {}\n    }\n  }\n}\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"# Metric Calculation","metadata":{}},{"cell_type":"code","source":"pip install pyevall","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:41:35.157052Z","iopub.execute_input":"2025-05-08T08:41:35.157600Z","iopub.status.idle":"2025-05-08T08:41:57.425220Z","shell.execute_reply.started":"2025-05-08T08:41:35.157577Z","shell.execute_reply":"2025-05-08T08:41:57.424302Z"}},"outputs":[{"name":"stdout","text":"Collecting pyevall\n  Downloading PyEvALL-0.1.78.tar.gz (39 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting jsbeautifier==1.14.9 (from pyevall)\n  Downloading jsbeautifier-1.14.9.tar.gz (75 kB)\n\u001b[2K     \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: jsonschema==4.23.0 in /usr/local/lib/python3.11/dist-packages (from pyevall) (4.23.0)\nRequirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (from pyevall) (1.26.4)\nRequirement already satisfied: pandas==2.2.3 in /usr/local/lib/python3.11/dist-packages (from pyevall) (2.2.3)\nCollecting setuptools==69.5.1 (from pyevall)\n  Downloading setuptools-69.5.1-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.11/dist-packages (from pyevall) (0.9.0)\nRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from jsbeautifier==1.14.9->pyevall) (1.17.0)\nCollecting editorconfig>=0.12.2 (from jsbeautifier==1.14.9->pyevall)\n  Downloading EditorConfig-0.17.0-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema==4.23.0->pyevall) (25.3.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema==4.23.0->pyevall) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema==4.23.0->pyevall) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema==4.23.0->pyevall) (0.22.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->pyevall) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->pyevall) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->pyevall) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->pyevall) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->pyevall) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->pyevall) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.3->pyevall) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.3->pyevall) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.3->pyevall) (2025.2)\nRequirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema==4.23.0->pyevall) (4.13.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.26.4->pyevall) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.26.4->pyevall) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy==1.26.4->pyevall) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy==1.26.4->pyevall) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy==1.26.4->pyevall) (2024.2.0)\nDownloading setuptools-69.5.1-py3-none-any.whl (894 kB)\n\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m894.6/894.6 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading EditorConfig-0.17.0-py3-none-any.whl (16 kB)\nBuilding wheels for collected packages: pyevall, jsbeautifier\n  Building wheel for pyevall (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pyevall: filename=PyEvALL-0.1.78-py3-none-any.whl size=34777 sha256=f57ae11799d5f59797d404e0f48dc8cd93c2c958377b6108cbf8c083e567df4b\n  Stored in directory: /root/.cache/pip/wheels/1f/71/ae/bf04901d4d2616e45aee909fba432a0ef67dd0d276f86eaab7\n  Building wheel for jsbeautifier (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for jsbeautifier: filename=jsbeautifier-1.14.9-py3-none-any.whl size=94155 sha256=ae25faf1d5207013d9c3ad8dbdd3444197f65dc10fa23300e97221263e98edf1\n  Stored in directory: /root/.cache/pip/wheels/b9/bd/4d/f11d410af889a445a4f36209ecc5fa5802aa9040a0bef2b235\nSuccessfully built pyevall jsbeautifier\nInstalling collected packages: editorconfig, setuptools, jsbeautifier, pyevall\n  Attempting uninstall: setuptools\n    Found existing installation: setuptools 75.1.0\n    Uninstalling setuptools-75.1.0:\n      Successfully uninstalled setuptools-75.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed editorconfig-0.17.0 jsbeautifier-1.14.9 pyevall-0.1.78 setuptools-69.5.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"# This is hard metrics","metadata":{}},{"cell_type":"code","source":"from pyevall.evaluation import PyEvALLEvaluation\nfrom pyevall.utils.utils import PyEvALLUtils\n\n# Define file paths\npredictions = \"/kaggle/working/EXIST2025_dev_predictions_merged_hard_flat.json\"\ngold = \"/kaggle/input/dev-gold-hard/EXIST2025_dev_task1_1_gold_hard.json\"\n\n# Initialize evaluator\nevaluator = PyEvALLEvaluation()\n\n# Set evaluation parameters\nparams = dict()\nparams[PyEvALLUtils.PARAM_REPORT] = PyEvALLUtils.PARAM_OPTION_REPORT_EMBEDDED  # Embedded report\n\n# Define metrics for hard-label binary classification\nmetrics = [\"ICM\", \"ICMNorm\", \"FMeasure\"]\n\n# Run evaluation\nreport = evaluator.evaluate(predictions, gold, metrics, **params)\n\n# Print report\nreport.print_report()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T09:03:59.509784Z","iopub.execute_input":"2025-05-08T09:03:59.510383Z","iopub.status.idle":"2025-05-08T09:03:59.632893Z","shell.execute_reply.started":"2025-05-08T09:03:59.510355Z","shell.execute_reply":"2025-05-08T09:03:59.631988Z"}},"outputs":[{"name":"stdout","text":"2025-05-08 09:03:59,566 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM', 'ICMNorm', 'FMeasure']\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1418779966.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Run evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Print report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyevall/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, predictions, goldstandard, lst_metrics, load_config, **params)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyevall_report\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mPyEvALLReport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyevall_report\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyEvALLFormat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyevall_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoldstandard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mmetric_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'metric_params'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyevall/comparators/formats.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pyevall_report, pred_file, gold_file, evaluation_id)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgold_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_files_json_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m#check the format of the file, convert if is tsv or csv, ,and return the json.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyevall/comparators/formats.py\u001b[0m in \u001b[0;36mparse_files_json_format\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"        \n\u001b[1;32m    248\u001b[0m         \u001b[0;31m#if predictions contains errors we stop evaluation and inform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_dict\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_format_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyevall/comparators/formats.py\u001b[0m in \u001b[0;36mparser_json\u001b[0;34m(self, path, file_name)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/EXIST2025_dev_predictions_merged_hard_flat.json'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/EXIST2025_dev_predictions_merged_hard_flat.json'","output_type":"error"}],"execution_count":24},{"cell_type":"markdown","source":"# These are soft metrics","metadata":{}},{"cell_type":"code","source":"from pyevall.evaluation import PyEvALLEvaluation\nfrom pyevall.utils.utils import PyEvALLUtils\n\n# Define file paths\npredictions = \"/kaggle/working/EXIST2025_dev_predictions_merged_soft.json\"  # Your prediction file\ngold = \"/kaggle/input/dev-gold-soft/EXIST2025_dev_task1_1_gold_soft.json\"    # Your gold file\n\n# Initialize PyEvALL evaluation\nevaluator = PyEvALLEvaluation()\n\n# Set evaluation parameters\nparams = dict()\nparams[PyEvALLUtils.PARAM_REPORT] = PyEvALLUtils.PARAM_OPTION_REPORT_EMBEDDED  # Embedded report\n\n# Define evaluation metrics for soft binary classification\nmetrics = [\"ICMSoft\", \"ICMSoftNorm\", \"CrossEntropy\"]\n\n# Run evaluation\nreport = evaluator.evaluate(predictions, gold, metrics, **params)\n\n# Print evaluation report\nreport.print_report()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T09:04:08.171972Z","iopub.execute_input":"2025-05-08T09:04:08.172232Z","iopub.status.idle":"2025-05-08T09:04:11.544505Z","shell.execute_reply.started":"2025-05-08T09:04:08.172214Z","shell.execute_reply":"2025-05-08T09:04:11.543779Z"}},"outputs":[{"name":"stdout","text":"2025-05-08 09:04:08,178 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICMSoft', 'ICMSoftNorm', 'CrossEntropy']\n2025-05-08 09:04:08,430 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Soft evaluation method\n2025-05-08 09:04:09,339 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM-Soft Normalized evaluation method\n2025-05-08 09:04:09,343 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Soft evaluation method\n2025-05-08 09:04:10,243 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Soft evaluation method\n2025-05-08 09:04:11,115 - pyevall.metrics.metrics - INFO -             evaluate() - Executing Cross Entropy evaluation method\ncargado 29\n{\n  \"metrics\": {\n    \"ICMSoft\": {\n      \"name\": \"Information Contrast Model Soft\",\n      \"acronym\": \"ICM-Soft\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 0.5267991561733849\n        }],\n        \"average_per_test_case\": 0.5267991561733849\n      }\n    },\n    \"ICMSoftNorm\": {\n      \"name\": \"Normalized Information Contrast Model Soft\",\n      \"acronym\": \"ICM-Soft-Norm\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 0.5851040110588293\n        }],\n        \"average_per_test_case\": 0.5851040110588293\n      }\n    },\n    \"CrossEntropy\": {\n      \"name\": \"Cross Entropy\",\n      \"acronym\": \"CE\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 0.8100229131730801\n        }],\n        \"average_per_test_case\": 0.8100229131730801\n      }\n    }\n  },\n  \"files\": {\n    \"EXIST2025_dev_predictions_merged_soft.json\": {\n      \"name\": \"EXIST2025_dev_predictions_merged_soft.json\",\n      \"status\": \"OK\",\n      \"gold\": false,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_predictions_merged_soft.json.\",\n      \"errors\": {}\n    },\n    \"EXIST2025_dev_task1_1_gold_soft.json\": {\n      \"name\": \"EXIST2025_dev_task1_1_gold_soft.json\",\n      \"status\": \"OK\",\n      \"gold\": true,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_task1_1_gold_soft.json.\",\n      \"errors\": {}\n    }\n  }\n}\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# Finetuning with ensemble model","metadata":{}},{"cell_type":"code","source":"import json\nimport torch\nimport numpy as np\nimport os\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom torch.utils.data import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom typing import List, Dict, Tuple\n\n# === Configuration ===\nCORRECT_LABELS = [\"YES\", \"NO\"]\nMAX_LENGTH = 256\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef get_latest_checkpoint(model_dir: str) -> str:\n    \"\"\"Find the latest checkpoint directory in a model directory.\"\"\"\n    checkpoints = [d for d in os.listdir(model_dir) if d.startswith(\"checkpoint-\")]\n    if not checkpoints:\n        raise ValueError(f\"No checkpoints found in {model_dir}\")\n    # Sort by checkpoint number (extract the number after 'checkpoint-')\n    checkpoints.sort(key=lambda x: int(x.split(\"-\")[1]))\n    return os.path.join(model_dir, checkpoints[-1])\n\n# === Load Data ===\ndef load_data(translated_path: str, gold_soft_path: str) -> Tuple[List[str], List[List[float]], List[str]]:\n    \"\"\"Load and process data with soft labels.\"\"\"\n    with open(translated_path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    \n    with open(gold_soft_path, \"r\", encoding=\"utf-8\") as f:\n        gold_soft = json.load(f)\n    \n    gold_soft_dict = {entry[\"id\"]: entry[\"value\"] for entry in gold_soft}\n    \n    tweets, labels, ids = [], [], []\n    for entry in data.values():\n        tweet_id = entry[\"id_EXIST\"]\n        if tweet_id in gold_soft_dict:\n            soft_label_dict = gold_soft_dict[tweet_id]\n            labels.append([soft_label_dict.get(label, 0.0) for label in CORRECT_LABELS])\n            tweets.append(entry[\"tweet\"])\n            ids.append(tweet_id)\n    \n    return tweets, labels, ids\n\n# === Ensemble Model Class ===\nclass EnsembleSoftClassifier:\n    def __init__(self, model_paths: List[str], tokenizer_path: str = \"bert-base-multilingual-cased\"):\n        self.models = []\n        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n        \n        for path in model_paths:\n            # Handle local paths correctly\n            if os.path.isdir(path):\n                # Find the latest checkpoint\n                checkpoint_path = get_latest_checkpoint(path)\n                # Verify model files exist in the checkpoint\n                if not os.path.exists(os.path.join(checkpoint_path, \"pytorch_model.bin\")) and \\\n                   not os.path.exists(os.path.join(checkpoint_path, \"model.safetensors\")):\n                    raise ValueError(f\"Model files not found in {checkpoint_path}\")\n                model = BertForSequenceClassification.from_pretrained(checkpoint_path)\n            else:\n                # For HF model IDs\n                model = BertForSequenceClassification.from_pretrained(path)\n                \n            model.to(DEVICE)\n            model.eval()\n            self.models.append(model)\n    \n    def predict_proba(self, texts: List[str]) -> np.ndarray:\n        \"\"\"Predict soft probabilities using ensemble averaging.\"\"\"\n        encodings = self.tokenizer(texts, truncation=True, padding='max_length', \n                                 max_length=MAX_LENGTH, return_tensors='pt')\n        \n        input_ids = encodings['input_ids'].to(DEVICE)\n        attention_mask = encodings['attention_mask'].to(DEVICE)\n        \n        all_probs = []\n        with torch.no_grad():\n            for model in self.models:\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n                logits = outputs.logits.detach().cpu()\n                probs = torch.sigmoid(logits).numpy()\n                all_probs.append(probs)\n        \n        # Average probabilities across models\n        avg_probs = np.mean(all_probs, axis=0)\n        return avg_probs\n\n# === Main Execution ===\nif __name__ == \"__main__\":\n    # Use paths to the model directories (not the checkpoints)\n    model_paths = [\n        \"/kaggle/working/results/en\",  # Your English model directory\n        \"/kaggle/working/results/es\",  # Your Spanish model directory\n        \"bert-base-multilingual-cased\"  # Base model for diversity\n    ]\n    \n    # Step 2: Create ensemble\n    ensemble = EnsembleSoftClassifier(model_paths)\n    \n    # Step 3: Predict on dev set\n    predict_ensemble_dev(\n        ensemble,\n        \"/kaggle/input/dev-testing/EXIST2025_dev.json\",\n        \"EXIST2025_dev_predictions_ensemble_soft.json\"\n    )\n    \n    # Optional: Snap probabilities to nearest 1/6\n    with open(\"EXIST2025_dev_predictions_ensemble_soft.json\", \"r\") as f:\n        predictions = json.load(f)\n    \n    snap_vals = np.array([i / 6 for i in range(7)])\n    for entry in predictions:\n        entry['value'] = {k: float(snap_vals[np.argmin(np.abs(snap_vals - v))]) \n                         for k, v in entry['value'].items()}\n    \n    with open(\"EXIST2025_dev_predictions_ensemble_soft_snapped.json\", \"w\") as f:\n        json.dump(predictions, f, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T19:37:38.994970Z","iopub.execute_input":"2025-04-23T19:37:38.995683Z","iopub.status.idle":"2025-04-23T19:38:29.727779Z","shell.execute_reply.started":"2025-04-23T19:37:38.995656Z","shell.execute_reply":"2025-04-23T19:38:29.727160Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Ensemble predictions saved to EXIST2025_dev_predictions_ensemble_soft.json\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# ensemble soft eval","metadata":{}},{"cell_type":"code","source":"from pyevall.evaluation import PyEvALLEvaluation\nfrom pyevall.utils.utils import PyEvALLUtils\n\n# Define file paths - using your ensemble predictions\npredictions = \"/kaggle/working/EXIST2025_dev_predictions_ensemble_soft.json\"  # Ensemble predictions\ngold = \"/kaggle/input/dev-gold-soft/EXIST2025_dev_task1_1_gold_soft.json\"    # Gold file\n\n# Initialize PyEvALL evaluation\nevaluator = PyEvALLEvaluation()\n\n# Set evaluation parameters\nparams = dict()\nparams[PyEvALLUtils.PARAM_REPORT] = PyEvALLUtils.PARAM_OPTION_REPORT_EMBEDDED  # Embedded report\n\n# Define evaluation metrics for soft binary classification\nmetrics = [\"ICMSoft\", \"ICMSoftNorm\"]  # Soft metrics for probability outputs\n\n# Run evaluation\nprint(\"Evaluating ensemble model performance...\")\nreport = evaluator.evaluate(predictions, gold, metrics, **params)\n\n# Print evaluation report\nprint(\"\\n=== Ensemble Model Evaluation Results ===\")\nreport.print_report()\n\n# Optional: Also evaluate the snapped version\nprint(\"\\nEvaluating snapped probabilities version...\")\nsnapped_predictions = \"/kaggle/working/EXIST2025_dev_predictions_ensemble_soft_snapped.json\"\nsnapped_report = evaluator.evaluate(snapped_predictions, gold, metrics, **params)\nprint(\"\\n=== Snapped Probabilities Evaluation Results ===\")\nsnapped_report.print_report()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T09:04:29.186223Z","iopub.execute_input":"2025-05-08T09:04:29.186990Z","iopub.status.idle":"2025-05-08T09:04:29.226624Z","shell.execute_reply.started":"2025-05-08T09:04:29.186962Z","shell.execute_reply":"2025-05-08T09:04:29.225591Z"}},"outputs":[{"name":"stdout","text":"Evaluating ensemble model performance...\n2025-05-08 09:04:29,194 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICMSoft', 'ICMSoftNorm']\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1182723625.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Run evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating ensemble model performance...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Print evaluation report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyevall/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, predictions, goldstandard, lst_metrics, load_config, **params)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyevall_report\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mPyEvALLReport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyevall_report\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyEvALLFormat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyevall_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoldstandard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mmetric_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'metric_params'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyevall/comparators/formats.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pyevall_report, pred_file, gold_file, evaluation_id)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgold_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_files_json_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m#check the format of the file, convert if is tsv or csv, ,and return the json.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyevall/comparators/formats.py\u001b[0m in \u001b[0;36mparse_files_json_format\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"        \n\u001b[1;32m    248\u001b[0m         \u001b[0;31m#if predictions contains errors we stop evaluation and inform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_dict\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_format_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyevall/comparators/formats.py\u001b[0m in \u001b[0;36mparser_json\u001b[0;34m(self, path, file_name)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/EXIST2025_dev_predictions_ensemble_soft.json'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/EXIST2025_dev_predictions_ensemble_soft.json'","output_type":"error"}],"execution_count":26},{"cell_type":"markdown","source":"# trying to improve ensemble","metadata":{}},{"cell_type":"code","source":"import json\nimport torch\nimport numpy as np\nimport os\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom typing import List, Dict\n\n# === Configuration ===\nCORRECT_LABELS = [\"YES\", \"NO\"]\nMAX_LENGTH = 256\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nBATCH_SIZE = 16\n\nMODEL_WEIGHTS = {\n    \"./results/en\": 0.6,\n    \"./results/es\": 0.2,\n    \"roberta-base\": 0.2\n}\n\n# Base models for tokenizer loading (update if needed)\nBASE_MODEL_MAP = {\n    \"./results/en\": \"bert-base-uncased\",\n    \"./results/es\": \"bert-base-uncased\",\n}\n\ndef get_latest_checkpoint(model_dir: str) -> str:\n    checkpoints = [d for d in os.listdir(model_dir) if d.startswith(\"checkpoint-\")]\n    if not checkpoints:\n        raise ValueError(f\"No checkpoints found in {model_dir}\")\n    checkpoints.sort(key=lambda x: int(x.split(\"-\")[1]))\n    return os.path.join(model_dir, checkpoints[-1])\n\nclass WeightedEnsembleClassifier:\n    def __init__(self, model_weights: Dict[str, float]):\n        self.model_weights = model_weights\n        self.tokenizers = {}\n\n        # Preload tokenizers only\n        for model_path in model_weights:\n            if os.path.isdir(model_path):\n                base_model = BASE_MODEL_MAP.get(model_path)\n                if base_model is None:\n                    raise ValueError(f\"Tokenizer base not found for {model_path}\")\n                self.tokenizers[model_path] = AutoTokenizer.from_pretrained(base_model)\n            else:\n                self.tokenizers[model_path] = AutoTokenizer.from_pretrained(model_path)\n\n        # Normalize weights\n        total = sum(model_weights.values())\n        self.normalized_weights = {k: v / total for k, v in model_weights.items()}\n\n    def predict_proba(self, texts: List[str]) -> np.ndarray:\n        all_probs = np.zeros((len(texts), 2))\n\n        for model_path, weight in self.normalized_weights.items():\n            tokenizer = self.tokenizers[model_path]\n            if os.path.isdir(model_path):\n                model_dir = get_latest_checkpoint(model_path)\n            else:\n                model_dir = model_path\n\n            model = AutoModelForSequenceClassification.from_pretrained(model_dir).to(DEVICE)\n            model.eval()\n\n            model_probs = []\n\n            for i in range(0, len(texts), BATCH_SIZE):\n                batch = texts[i:i + BATCH_SIZE]\n                encodings = tokenizer(batch, padding=\"max_length\", truncation=True,\n                                      max_length=MAX_LENGTH, return_tensors=\"pt\").to(DEVICE)\n\n                with torch.no_grad():\n                    logits = model(**encodings).logits\n                    probs = torch.sigmoid(logits).cpu().numpy()\n                    model_probs.extend(probs)\n\n                torch.cuda.empty_cache()\n\n            model_probs = np.array(model_probs)\n            all_probs += weight * model_probs\n\n            # Free memory\n            del model\n            torch.cuda.empty_cache()\n\n        return all_probs\n\ndef predict_to_file(ensemble, dev_path: str, output_prefix: str):\n    with open(dev_path, \"r\", encoding=\"utf-8\") as f:\n        dev_data = json.load(f)\n\n    all_tweets, all_ids = [], []\n    for entry in dev_data.values():\n        all_tweets.append(entry[\"tweet\"])\n        all_ids.append(entry[\"id_EXIST\"])\n\n    probs = ensemble.predict_proba(all_tweets)\n\n    raw_results = []\n    for tweet_id, prob in zip(all_ids, probs):\n        raw_results.append({\n            \"test_case\": \"EXIST2025\",\n            \"id\": tweet_id,\n            \"value\": {CORRECT_LABELS[i]: float(prob[i]) for i in range(len(CORRECT_LABELS))}\n        })\n\n    raw_output = f\"{output_prefix}_raw.json\"\n    with open(raw_output, \"w\", encoding=\"utf-8\") as f:\n        json.dump(raw_results, f, indent=4)\n\n    # Snap predictions to nearest i/6\n    snap_vals = np.array([i / 6 for i in range(7)])\n    snapped_results = []\n    for entry in raw_results:\n        snapped_results.append({\n            \"test_case\": entry[\"test_case\"],\n            \"id\": entry[\"id\"],\n            \"value\": {k: float(snap_vals[np.argmin(np.abs(snap_vals - v))])\n                      for k, v in entry[\"value\"].items()}\n        })\n\n    snapped_output = f\"{output_prefix}_snapped.json\"\n    with open(snapped_output, \"w\", encoding=\"utf-8\") as f:\n        json.dump(snapped_results, f, indent=4)\n\n    print(f\"â Predictions saved to:\\n- {raw_output}\\n- {snapped_output}\")\n\n# === MAIN ===\nif __name__ == \"__main__\":\n    ensemble = WeightedEnsembleClassifier(MODEL_WEIGHTS)\n    predict_to_file(\n        ensemble,\n        \"/kaggle/input/dev-testing/EXIST2025_dev.json\",\n        \"EXIST2025_dev_predictions_weighted_ensemble\"\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T03:45:24.466434Z","iopub.execute_input":"2025-04-24T03:45:24.467209Z","iopub.status.idle":"2025-04-24T03:46:11.592274Z","shell.execute_reply.started":"2025-04-24T03:45:24.467185Z","shell.execute_reply":"2025-04-24T03:46:11.591663Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"â Predictions saved to:\n- EXIST2025_dev_predictions_weighted_ensemble_raw.json\n- EXIST2025_dev_predictions_weighted_ensemble_snapped.json\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import json\nfrom pyevall.evaluation import PyEvALLEvaluation\nfrom pyevall.utils.utils import PyEvALLUtils\n\ndef evaluate_predictions(prediction_file: str, gold_file: str):\n    \"\"\"Evaluate predictions against gold standard.\"\"\"\n    evaluator = PyEvALLEvaluation()\n    params = {PyEvALLUtils.PARAM_REPORT: PyEvALLUtils.PARAM_OPTION_REPORT_EMBEDDED}\n    \n    print(f\"\\nEvaluating {prediction_file}...\")\n    report = evaluator.evaluate(\n        prediction_file,\n        gold_file,\n        [\"ICMSoft\", \"ICMSoftNorm\"],\n        **params\n    )\n    report.print_report()\n\nif __name__ == \"__main__\":\n    # File paths - update these based on your actual files\n    GOLD_FILE = \"/kaggle/input/dev-gold-soft/EXIST2025_dev_task1_1_gold_soft.json\"\n    \n    # Evaluate raw predictions\n    evaluate_predictions(\n        \"EXIST2025_dev_predictions_weighted_ensemble_raw.json\",\n        GOLD_FILE\n    )\n    \n    # Evaluate snapped predictions\n    evaluate_predictions(\n        \"EXIST2025_dev_predictions_weighted_ensemble_snapped.json\",\n        GOLD_FILE\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T03:46:23.670355Z","iopub.execute_input":"2025-04-24T03:46:23.671013Z","iopub.status.idle":"2025-04-24T03:46:29.652798Z","shell.execute_reply.started":"2025-04-24T03:46:23.670986Z","shell.execute_reply":"2025-04-24T03:46:29.652021Z"}},"outputs":[{"name":"stdout","text":"\nEvaluating EXIST2025_dev_predictions_weighted_ensemble_raw.json...\n2025-04-24 03:46:23,677 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICMSoft', 'ICMSoftNorm']\n2025-04-24 03:46:23,900 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Soft evaluation method\n2025-04-24 03:46:25,241 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM-Soft Normalized evaluation method\n2025-04-24 03:46:25,244 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Soft evaluation method\n2025-04-24 03:46:26,074 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Soft evaluation method\n{\n  \"metrics\": {\n    \"ICMSoft\": {\n      \"name\": \"Information Contrast Model Soft\",\n      \"acronym\": \"ICM-Soft\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": -1.4009106699084555\n        }],\n        \"average_per_test_case\": -1.4009106699084555\n      }\n    },\n    \"ICMSoftNorm\": {\n      \"name\": \"Normalized Information Contrast Model Soft\",\n      \"acronym\": \"ICM-Soft-Norm\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 0.2736839253685491\n        }],\n        \"average_per_test_case\": 0.2736839253685491\n      }\n    }\n  },\n  \"files\": {\n    \"EXIST2025_dev_predictions_weighted_ensemble_raw.json\": {\n      \"name\": \"EXIST2025_dev_predictions_weighted_ensemble_raw.json\",\n      \"status\": \"OK\",\n      \"gold\": false,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_predictions_weighted_ensemble_raw.json.\",\n      \"errors\": {}\n    },\n    \"EXIST2025_dev_task1_1_gold_soft.json\": {\n      \"name\": \"EXIST2025_dev_task1_1_gold_soft.json\",\n      \"status\": \"OK\",\n      \"gold\": true,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_task1_1_gold_soft.json.\",\n      \"errors\": {}\n    }\n  }\n}\n\nEvaluating EXIST2025_dev_predictions_weighted_ensemble_snapped.json...\n2025-04-24 03:46:26,942 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICMSoft', 'ICMSoftNorm']\n2025-04-24 03:46:27,167 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Soft evaluation method\n2025-04-24 03:46:27,979 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM-Soft Normalized evaluation method\n2025-04-24 03:46:27,982 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Soft evaluation method\n2025-04-24 03:46:28,838 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Soft evaluation method\n{\n  \"metrics\": {\n    \"ICMSoft\": {\n      \"name\": \"Information Contrast Model Soft\",\n      \"acronym\": \"ICM-Soft\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": -1.400375145052235\n        }],\n        \"average_per_test_case\": -1.400375145052235\n      }\n    },\n    \"ICMSoftNorm\": {\n      \"name\": \"Normalized Information Contrast Model Soft\",\n      \"acronym\": \"ICM-Soft-Norm\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 0.27377043900994724\n        }],\n        \"average_per_test_case\": 0.27377043900994724\n      }\n    }\n  },\n  \"files\": {\n    \"EXIST2025_dev_predictions_weighted_ensemble_snapped.json\": {\n      \"name\": \"EXIST2025_dev_predictions_weighted_ensemble_snapped.json\",\n      \"status\": \"OK\",\n      \"gold\": false,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_predictions_weighted_ensemble_snapped.json.\",\n      \"errors\": {}\n    },\n    \"EXIST2025_dev_task1_1_gold_soft.json\": {\n      \"name\": \"EXIST2025_dev_task1_1_gold_soft.json\",\n      \"status\": \"OK\",\n      \"gold\": true,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_task1_1_gold_soft.json.\",\n      \"errors\": {}\n    }\n  }\n}\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}