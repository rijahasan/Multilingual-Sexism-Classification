{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11135128,"sourceType":"datasetVersion","datasetId":6945024},{"sourceId":11176705,"sourceType":"datasetVersion","datasetId":6975840},{"sourceId":11473710,"sourceType":"datasetVersion","datasetId":7190756},{"sourceId":11511209,"sourceType":"datasetVersion","datasetId":7218178},{"sourceId":11511237,"sourceType":"datasetVersion","datasetId":7218204},{"sourceId":11511823,"sourceType":"datasetVersion","datasetId":7218663},{"sourceId":11517677,"sourceType":"datasetVersion","datasetId":7223177},{"sourceId":11524963,"sourceType":"datasetVersion","datasetId":7227915},{"sourceId":11527922,"sourceType":"datasetVersion","datasetId":7230250},{"sourceId":11529474,"sourceType":"datasetVersion","datasetId":7231409},{"sourceId":11730304,"sourceType":"datasetVersion","datasetId":7363549},{"sourceId":11732784,"sourceType":"datasetVersion","datasetId":7365410},{"sourceId":11760229,"sourceType":"datasetVersion","datasetId":7382815},{"sourceId":11760710,"sourceType":"datasetVersion","datasetId":7383138},{"sourceId":11761454,"sourceType":"datasetVersion","datasetId":7383656}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## File cleaning\nimport json\n\n# Input and output files\ninput_files = ['/kaggle/input/dataset/EXIST2025_training.json', '/kaggle/input/dataset/EXIST2025_dev.json', '/kaggle/input/testing/EXIST2025_test_clean.json'] \n\nfor infile in input_files:\n    with open(infile, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n\n    # Containers for language-specific cleaned data\n    data_en = {}\n    data_es = {}\n\n    for key, entry in data.items():\n        # Remove unnecessary fields\n        entry.pop('labels_task1_2', None)\n        entry.pop('labels_task1_3', None)\n\n        # Split based on language\n        if entry['lang'] == 'en':\n            data_en[key] = entry\n        elif entry['lang'] == 'es':\n            data_es[key] = entry\n\n    # Base name without path and extension\n    base_name = infile.split('/')[-1].split('.')[0]\n\n    # Save cleaned English and Spanish files in the writable `/kaggle/working/` directory\n    with open(f\"/kaggle/working/{base_name}_cleaned_en.json\", 'w', encoding='utf-8') as f_en:\n        json.dump(data_en, f_en, ensure_ascii=False, indent=2)\n\n    with open(f\"/kaggle/working/{base_name}_cleaned_es.json\", 'w', encoding='utf-8') as f_es:\n        json.dump(data_es, f_es, ensure_ascii=False, indent=2)\n\n    print(f\"{base_name}_cleaned_en.json and {base_name}_cleaned_es.json saved successfully.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:07:46.726218Z","iopub.execute_input":"2025-05-10T15:07:46.726650Z","iopub.status.idle":"2025-05-10T15:07:47.777042Z","shell.execute_reply.started":"2025-05-10T15:07:46.726609Z","shell.execute_reply":"2025-05-10T15:07:47.776135Z"}},"outputs":[{"name":"stdout","text":"EXIST2025_training_cleaned_en.json and EXIST2025_training_cleaned_es.json saved successfully.\nEXIST2025_dev_cleaned_en.json and EXIST2025_dev_cleaned_es.json saved successfully.\nEXIST2025_test_clean_cleaned_en.json and EXIST2025_test_clean_cleaned_es.json saved successfully.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install transformers datasets pandas scikit-learn torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:07:58.636084Z","iopub.execute_input":"2025-05-10T15:07:58.636354Z","iopub.status.idle":"2025-05-10T15:08:04.090078Z","shell.execute_reply.started":"2025-05-10T15:07:58.636334Z","shell.execute_reply":"2025-05-10T15:08:04.089204Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import wandb\n\nwandb.login(key=\"a40bf999db96c982783dc52dd0594d3347848f02\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:08:07.185836Z","iopub.execute_input":"2025-05-10T15:08:07.186153Z","iopub.status.idle":"2025-05-10T15:08:17.570127Z","shell.execute_reply.started":"2025-05-10T15:08:07.186124Z","shell.execute_reply":"2025-05-10T15:08:17.569422Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfaisalsara124\u001b[0m (\u001b[33mfaisalsara124-habib-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"pip install PyEvALL","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T16:52:29.915692Z","iopub.execute_input":"2025-04-22T16:52:29.916000Z","iopub.status.idle":"2025-04-22T16:52:57.937902Z","shell.execute_reply.started":"2025-04-22T16:52:29.915972Z","shell.execute_reply":"2025-04-22T16:52:57.936901Z"}},"outputs":[{"name":"stdout","text":"Collecting PyEvALL\n  Downloading PyEvALL-0.1.78.tar.gz (39 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting jsbeautifier==1.14.9 (from PyEvALL)\n  Downloading jsbeautifier-1.14.9.tar.gz (75 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: jsonschema==4.23.0 in /usr/local/lib/python3.10/dist-packages (from PyEvALL) (4.23.0)\nRequirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (from PyEvALL) (1.26.4)\nRequirement already satisfied: pandas==2.2.3 in /usr/local/lib/python3.10/dist-packages (from PyEvALL) (2.2.3)\nCollecting setuptools==69.5.1 (from PyEvALL)\n  Downloading setuptools-69.5.1-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.10/dist-packages (from PyEvALL) (0.9.0)\nRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from jsbeautifier==1.14.9->PyEvALL) (1.17.0)\nCollecting editorconfig>=0.12.2 (from jsbeautifier==1.14.9->PyEvALL)\n  Downloading EditorConfig-0.17.0-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.23.0->PyEvALL) (25.1.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.23.0->PyEvALL) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.23.0->PyEvALL) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.23.0->PyEvALL) (0.22.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->PyEvALL) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->PyEvALL) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->PyEvALL) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->PyEvALL) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->PyEvALL) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->PyEvALL) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->PyEvALL) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->PyEvALL) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->PyEvALL) (2025.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy==1.26.4->PyEvALL) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy==1.26.4->PyEvALL) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy==1.26.4->PyEvALL) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy==1.26.4->PyEvALL) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy==1.26.4->PyEvALL) (2024.2.0)\nDownloading setuptools-69.5.1-py3-none-any.whl (894 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m894.6/894.6 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading EditorConfig-0.17.0-py3-none-any.whl (16 kB)\nBuilding wheels for collected packages: PyEvALL, jsbeautifier\n  Building wheel for PyEvALL (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for PyEvALL: filename=PyEvALL-0.1.78-py3-none-any.whl size=34777 sha256=06608e7ce695d16bf606859374d4e0fe32f5b52fc46ad4e278af703c262ffea1\n  Stored in directory: /root/.cache/pip/wheels/f0/3a/51/f8c268e67356c15a602eef8ac7a5e18ba4677b4ec8b45b8a25\n  Building wheel for jsbeautifier (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for jsbeautifier: filename=jsbeautifier-1.14.9-py3-none-any.whl size=94157 sha256=a6f7886a37d0694d944076ac936f4273011447e8797715d53f21053d36f99314\n  Stored in directory: /root/.cache/pip/wheels/c4/5c/25/09f8b2e8dddb4fc3d70817c67b375a9069a2628847ffbdfc65\nSuccessfully built PyEvALL jsbeautifier\nInstalling collected packages: editorconfig, setuptools, jsbeautifier, PyEvALL\n  Attempting uninstall: setuptools\n    Found existing installation: setuptools 75.1.0\n    Uninstalling setuptools-75.1.0:\n      Successfully uninstalled setuptools-75.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed PyEvALL-0.1.78 editorconfig-0.17.0 jsbeautifier-1.14.9 setuptools-69.5.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"##Ensemble for english\n##Models used: DistilRoberta, bert-base, roberta-base","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T12:22:59.400769Z","iopub.execute_input":"2025-04-22T12:22:59.401066Z","iopub.status.idle":"2025-04-22T12:22:59.404401Z","shell.execute_reply.started":"2025-04-22T12:22:59.401044Z","shell.execute_reply":"2025-04-22T12:22:59.403484Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\nfrom datasets import Dataset\nimport pandas as pd\nimport numpy as np\nimport json\nimport os\n\ndef prepare_gold_dataset(clean_path, gold_path, output_path):\n    with open(clean_path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    with open(gold_path, \"r\", encoding=\"utf-8\") as f:\n        gold_labels = json.load(f)\n    label_dict = {entry[\"id\"]: 1 if entry[\"value\"] == \"YES\" else 0 for entry in gold_labels}\n    updated_data = {}\n    for tweet_id, tweet_info in data.items():\n        tweet_info = tweet_info.copy()\n        gold_id = tweet_info.get(\"id_EXIST\")\n        if gold_id in label_dict:\n            tweet_info[\"label\"] = label_dict[gold_id]\n        updated_data[tweet_id] = tweet_info\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(updated_data, f, indent=2, ensure_ascii=False)\n    print(f\"✅ Gold-labeled training set saved to {output_path}\")\n\ndef load_and_tokenize_dataset(json_path, tokenizer, max_length=256):\n    df = pd.read_json(json_path)\n    df = df.T\n    df = df[df['lang'] == 'en']\n    df = df.dropna(subset=['label'])\n    df = df[['tweet', 'label']].rename(columns={'tweet': 'text'})\n    df = df.sample(frac=1, random_state=42)\n    dataset = Dataset.from_pandas(df)\n    return dataset.train_test_split(test_size=0.1)\n\ndef train_model(json_path, model_checkpoint, save_name):\n    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n    splits = load_and_tokenize_dataset(json_path, tokenizer)\n    \n    def preprocess(example):\n        return tokenizer(example['text'], truncation=True, padding='max_length', max_length=256)\n    \n    train_ds = splits['train'].map(preprocess, batched=True)\n    val_ds = splits['test'].map(preprocess, batched=True)\n\n    training_args = TrainingArguments(\n        output_dir=f\"results/{save_name}\",\n        num_train_epochs=3,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=8,\n        warmup_steps=100,\n        weight_decay=0.01,\n        logging_dir=f\"logs/{save_name}\",\n        logging_steps=50,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_ds,\n        eval_dataset=val_ds,\n        tokenizer=tokenizer,\n        data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n    )\n\n    trainer.train()\n    model.save_pretrained(f\"{save_name}_sexism_classifier\")\n    tokenizer.save_pretrained(f\"{save_name}_sexism_classifier\")\n    print(f\"✅ Model saved to {save_name}_sexism_classifier\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T07:43:25.667933Z","iopub.execute_input":"2025-05-08T07:43:25.668224Z","iopub.status.idle":"2025-05-08T07:43:47.261038Z","shell.execute_reply.started":"2025-05-08T07:43:25.668202Z","shell.execute_reply":"2025-05-08T07:43:47.260039Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"prepare_gold_dataset(\n    clean_path=\"/kaggle/input/translated/EXIST2025_training_translated_en.json\",\n    gold_path=\"/kaggle/input/gold-hard/EXIST2025_training_task1_1_gold_hard.json\",\n    output_path=\"EXIST2025_training_with_gold.json\"\n)\ntrain_model(\"EXIST2025_training_with_gold.json\", \"distilroberta-base\", \"distilroberta-base\")\ntrain_model(\"EXIST2025_training_with_gold.json\", \"bert-base-uncased\", \"bert-base-uncased\")\ntrain_model(\"EXIST2025_training_with_gold.json\", \"roberta-base\", \"roberta-base\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T07:44:17.085295Z","iopub.execute_input":"2025-05-08T07:44:17.085967Z","iopub.status.idle":"2025-05-08T07:55:59.966126Z","shell.execute_reply.started":"2025-05-08T07:44:17.085937Z","shell.execute_reply":"2025-05-08T07:55:59.965232Z"}},"outputs":[{"name":"stdout","text":"✅ Gold-labeled training set saved to EXIST2025_training_with_gold.json\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f818656b6f34fb0bbec7d30f856ebd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ec223c947e54bd3a5a56a3f989a7d6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db001314dad04e35870eeb89baaeeb09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bae006b630cd4259811b9b4819c1b6bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b49f2e2203db446bb4defe9b5cab8c8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecdcd6286865404bbe5a963c9b095bbe"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2583 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db48860f95414bbabe65eab827725bcf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/287 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33ed03235a5c4ab6af24d2c0e5700929"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-5-91d14f078f03>:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250508_074424-raip0q28</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/faisalsara124-habib-university/huggingface/runs/raip0q28' target=\"_blank\">results/distilroberta-base</a></strong> to <a href='https://wandb.ai/faisalsara124-habib-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/faisalsara124-habib-university/huggingface' target=\"_blank\">https://wandb.ai/faisalsara124-habib-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/faisalsara124-habib-university/huggingface/runs/raip0q28' target=\"_blank\">https://wandb.ai/faisalsara124-habib-university/huggingface/runs/raip0q28</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='486' max='486' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [486/486 02:19, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.456400</td>\n      <td>0.422307</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.345500</td>\n      <td>0.445668</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.209000</td>\n      <td>0.455466</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model saved to distilroberta-base_sexism_classifier\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0ef823192fa4a8e9ed9cf4b16ee79e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c1be413694b4454b5b3267a96b5a99f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5ae3c402447422c84f409febf6e2e10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d45cfad35a664e9e82c9c26928954bec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01baa9b991714257bfd7806b315ab82e"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2583 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dacf2e3a87834e5fac4d4663d5b114b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/287 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e77d603207d84fd080325f441cd9c48b"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-5-91d14f078f03>:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='486' max='486' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [486/486 04:19, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.480900</td>\n      <td>0.381859</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.285400</td>\n      <td>0.363987</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.184600</td>\n      <td>0.389703</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model saved to bert-base-uncased_sexism_classifier\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8ef48c457a04aef85166e148fc4df18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5eb85ed3099e480a8e7a8654d1e9a266"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53b5f7c503e24a32a12329ee84dde594"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58266b3d425e41b7ad1c45b3eba0ee35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eebc1569ad424305b5c60db2631e8a7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7095894b7c5f483894e98b2cae2102c7"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2583 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37a55a8731b14ffe9556572f5e3fdf85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/287 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a752f1f882864971a25a1135ea18e005"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-5-91d14f078f03>:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='486' max='486' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [486/486 04:29, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.501200</td>\n      <td>0.418501</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.356800</td>\n      <td>0.370732</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.242400</td>\n      <td>0.451769</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model saved to roberta-base_sexism_classifier\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"##Checking how the models are doing individually","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport pandas as pd\nimport json\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n\n# === Load Gold-Labeled Dev Set ===\ndf = pd.read_json(\"EXIST2025_dev_cleaned_en.json\").T\ndf = df[df['lang'] == 'en']\n\n# Load gold labels for DEV set only\nwith open(\"/kaggle/input/gold-hard-dev/EXIST2025_dev_task1_1_gold_hard (1).json\", \"r\", encoding=\"utf-8\") as f:\n    gold = json.load(f)\nlabel_map = {entry[\"id\"]: 1 if entry[\"value\"] == \"YES\" else 0 for entry in gold}\n\n# Add labels to df\ndf['label'] = df['id_EXIST'].map(label_map)\ndf = df.dropna(subset=['label'])  # drop rows without gold labels\ndf['label'] = df['label'].astype(int)\ndf = df[['id_EXIST', 'tweet', 'label']].rename(columns={'id_EXIST': 'id', 'tweet': 'text'})\n\nprint(f\"✅ Total gold-labeled dev tweets: {len(df)}\")\n\n# === Models to Evaluate ===\nmodel_paths = {\n    \"DistilRoBERTa\": \"distilroberta-base_sexism_classifier\",\n    \"BERT-base\": \"bert-base-uncased_sexism_classifier\",\n    \"RoBERTa-base\": \"roberta-base_sexism_classifier\"\n}\n\n# === Evaluate each model ===\nfor model_name, path in model_paths.items():\n    tokenizer = AutoTokenizer.from_pretrained(path)\n    model = AutoModelForSequenceClassification.from_pretrained(path)\n    model.eval()\n\n    preds = []\n    for text in df['text']:\n        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n        with torch.no_grad():\n            logits = model(**inputs).logits\n            pred = torch.argmax(logits, dim=1).item()\n            preds.append(pred)\n\n    true_labels = df['label'].tolist()\n    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, preds, average='binary')\n    accuracy = accuracy_score(true_labels, preds)\n\n    print(f\"\\n🔍 Evaluation for {model_name}\")\n    print(f\"Accuracy:  {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall:    {recall:.4f}\")\n    print(f\"F1 Score:  {f1:.4f}\")\n    print(\"\\n📄 Classification Report:\")\n    print(classification_report(true_labels, preds, target_names=[\"non-sexist\", \"sexist\"]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T07:58:55.964178Z","iopub.execute_input":"2025-05-08T07:58:55.964589Z","iopub.status.idle":"2025-05-08T08:00:36.190806Z","shell.execute_reply.started":"2025-05-08T07:58:55.964556Z","shell.execute_reply":"2025-05-08T08:00:36.189980Z"}},"outputs":[{"name":"stdout","text":"✅ Total gold-labeled dev tweets: 444\n\n🔍 Evaluation for DistilRoBERTa\nAccuracy:  0.8311\nPrecision: 0.8251\nRecall:    0.7784\nF1 Score:  0.8011\n\n📄 Classification Report:\n              precision    recall  f1-score   support\n\n  non-sexist       0.84      0.87      0.85       250\n      sexist       0.83      0.78      0.80       194\n\n    accuracy                           0.83       444\n   macro avg       0.83      0.83      0.83       444\nweighted avg       0.83      0.83      0.83       444\n\n\n🔍 Evaluation for BERT-base\nAccuracy:  0.8063\nPrecision: 0.8971\nRecall:    0.6289\nF1 Score:  0.7394\n\n📄 Classification Report:\n              precision    recall  f1-score   support\n\n  non-sexist       0.77      0.94      0.85       250\n      sexist       0.90      0.63      0.74       194\n\n    accuracy                           0.81       444\n   macro avg       0.83      0.79      0.79       444\nweighted avg       0.82      0.81      0.80       444\n\n\n🔍 Evaluation for RoBERTa-base\nAccuracy:  0.8423\nPrecision: 0.8647\nRecall:    0.7577\nF1 Score:  0.8077\n\n📄 Classification Report:\n              precision    recall  f1-score   support\n\n  non-sexist       0.83      0.91      0.87       250\n      sexist       0.86      0.76      0.81       194\n\n    accuracy                           0.84       444\n   macro avg       0.85      0.83      0.84       444\nweighted avg       0.84      0.84      0.84       444\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"## Checking Accuracy for the emsemble model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport pandas as pd\nimport json\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n\n# === Load Gold-Labeled Dev Set ===\ndf = pd.read_json(\"EXIST2025_dev_cleaned_en.json\").T\ndf = df[df['lang'] == 'en']\n\nwith open(\"/kaggle/input/gold-hard-dev/EXIST2025_dev_task1_1_gold_hard (1).json\", \"r\", encoding=\"utf-8\") as f:\n    gold = json.load(f)\nlabel_map = {entry[\"id\"]: 1 if entry[\"value\"] == \"YES\" else 0 for entry in gold}\n\ndf['label'] = df['id_EXIST'].map(label_map)\ndf = df.dropna(subset=['label'])\ndf['label'] = df['label'].astype(int)\ndf = df[['id_EXIST', 'tweet', 'label']].rename(columns={'id_EXIST': 'id', 'tweet': 'text'})\n\nprint(f\"✅ Total gold-labeled dev tweets: {len(df)}\")\n\n# === Ensemble models (equal weights by default) ===\nmodel_paths = [\n    \"distilroberta-base_sexism_classifier\",\n    \"bert-base-uncased_sexism_classifier\",\n    \"roberta-base_sexism_classifier\"\n]\n\n# weights = [0.5, 0.2, 0.3]  # Equal weighting\n# weights = [0.4, 0.3, 0.3] \n# weights = [0.5, 0.1, 0.4] \nweights = [0.5, 0.1, 0.4] \nweights = np.array(weights) / sum(weights)  # Normalize\n\nall_model_probs = []\n\n# === Collect probabilities from each model ===\nfor path in model_paths:\n    tokenizer = AutoTokenizer.from_pretrained(path)\n    model = AutoModelForSequenceClassification.from_pretrained(path)\n    model.eval()\n\n    model_probs = []\n    for text in df['text']:\n        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n        with torch.no_grad():\n            logits = model(**inputs).logits\n            probs = torch.softmax(logits, dim=1).squeeze(0).cpu().numpy()\n            model_probs.append(probs)\n\n    all_model_probs.append(np.array(model_probs))  # shape: [num_samples, 2]\n\n# === Weighted soft voting ===\nall_model_probs = np.array(all_model_probs)  # shape: [num_models, num_samples, 2]\nweighted_probs = np.average(all_model_probs, axis=0, weights=weights)  # shape: [num_samples, 2]\nensemble_preds = np.argmax(weighted_probs, axis=1)\n\n# === Evaluation ===\ntrue_labels = df['label'].tolist()\nprecision, recall, f1, _ = precision_recall_fscore_support(true_labels, ensemble_preds, average='binary')\naccuracy = accuracy_score(true_labels, ensemble_preds)\n\nprint(\"\\n🔗 Ensemble Evaluation on GOLD Dev Set (Soft Voting, Equal Weights):\")\nprint(f\"Accuracy:  {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall:    {recall:.4f}\")\nprint(f\"F1 Score:  {f1:.4f}\")\nprint(\"\\n📄 Classification Report:\")\nprint(classification_report(true_labels, ensemble_preds, target_names=[\"non-sexist\", \"sexist\"]))\n\n# === Save Output for PyEvALL ===\noutput = []\nfor tweet_id, pred in zip(df['id'], ensemble_preds):\n    output.append({\n        \"test_case\": \"EXIST2025\",\n        \"id\": str(tweet_id),\n        \"value\": \"YES\" if pred == 1 else \"NO\"\n    })\n\n# Sort by ID to ensure consistency\noutput_sorted = sorted(output, key=lambda x: int(x[\"id\"]))\n\n# Save to JSON\nwith open(\"ensemble_predictions_output.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(output_sorted, f, indent=2, ensure_ascii=False)\n\nprint(\"✅ Predictions saved for PyEvALL evaluation\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:01:22.180902Z","iopub.execute_input":"2025-05-08T08:01:22.181236Z","iopub.status.idle":"2025-05-08T08:03:07.482056Z","shell.execute_reply.started":"2025-05-08T08:01:22.181212Z","shell.execute_reply":"2025-05-08T08:03:07.481192Z"}},"outputs":[{"name":"stdout","text":"✅ Total gold-labeled dev tweets: 444\n\n🔗 Ensemble Evaluation on GOLD Dev Set (Soft Voting, Equal Weights):\nAccuracy:  0.8559\nPrecision: 0.8916\nRecall:    0.7629\nF1 Score:  0.8222\n\n📄 Classification Report:\n              precision    recall  f1-score   support\n\n  non-sexist       0.83      0.93      0.88       250\n      sexist       0.89      0.76      0.82       194\n\n    accuracy                           0.86       444\n   macro avg       0.86      0.85      0.85       444\nweighted avg       0.86      0.86      0.85       444\n\n✅ Predictions saved for PyEvALL evaluation\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"##ICM Score ensemble model predictions english","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyevall.evaluation import PyEvALLEvaluation\nfrom pyevall.utils.utils import PyEvALLUtils\n\npredictions = \"ensemble_predictions_output.json\"\ngold = \"/kaggle/input/gold-hard-dev/EXIST2025_dev_task1_1_gold_hard (1).json\"\n\n# Initialize evaluator\nevaluator = PyEvALLEvaluation()\n\n# Set parameters\nparams = {\n    PyEvALLUtils.PARAM_REPORT: PyEvALLUtils.PARAM_OPTION_REPORT_EMBEDDED\n}\n\n# Choose metrics (ICM for hard labels)\nmetrics = [\"ICM\", \"ICMNorm\", \"FMeasure\"]  # You can also try ICMSoft for soft scores\n\n# Run evaluation\nreport = evaluator.evaluate(predictions, gold, metrics, **params)\nreport.print_report()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:04:45.941177Z","iopub.execute_input":"2025-05-08T08:04:45.941520Z","iopub.status.idle":"2025-05-08T08:04:47.425367Z","shell.execute_reply.started":"2025-05-08T08:04:45.941490Z","shell.execute_reply":"2025-05-08T08:04:47.424640Z"}},"outputs":[{"name":"stdout","text":"2025-05-08 08:04:45,947 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM', 'ICMNorm', 'FMeasure']\n2025-05-08 08:04:46,037 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-08 08:04:46,385 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Normalized evaluation method\n2025-05-08 08:04:46,389 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-08 08:04:46,722 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-08 08:04:47,110 - pyevall.metrics.metrics - INFO -             evaluate() - Executing fmeasure evaluation method\n{\n  \"metrics\": {\n    \"ICM\": {\n      \"name\": \"Information Contrast model\",\n      \"acronym\": \"ICM\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": -0.25954456461537356\n        }],\n        \"average_per_test_case\": -0.25954456461537356\n      }\n    },\n    \"ICMNorm\": {\n      \"name\": \"Normalized Information Contrast Model\",\n      \"acronym\": \"ICM-Norm\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 0.37016587193062145\n        }],\n        \"average_per_test_case\": 0.37016587193062145\n      }\n    },\n    \"FMeasure\": {\n      \"name\": \"F-Measure\",\n      \"acronym\": \"F1\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"classes\": {\n            \"YES\": 0.4766505636070854,\n            \"NO\": 0.6129458388375164\n          },\n          \"average\": 0.5447982012223009\n        }],\n        \"average_per_test_case\": 0.5447982012223009\n      }\n    }\n  },\n  \"files\": {\n    \"ensemble_predictions_output.json\": {\n      \"name\": \"ensemble_predictions_output.json\",\n      \"status\": \"OK\",\n      \"gold\": false,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: ensemble_predictions_output.json.\",\n      \"errors\": {}\n    },\n    \"EXIST2025_dev_task1_1_gold_hard (1).json\": {\n      \"name\": \"EXIST2025_dev_task1_1_gold_hard (1).json\",\n      \"status\": \"OK\",\n      \"gold\": true,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_task1_1_gold_hard (1).json.\",\n      \"errors\": {}\n    }\n  }\n}\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"## Trying AEDA+ensemble for english","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -q nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T17:24:43.039271Z","iopub.execute_input":"2025-05-08T17:24:43.039650Z","iopub.status.idle":"2025-05-08T17:24:46.389631Z","shell.execute_reply.started":"2025-05-08T17:24:43.039619Z","shell.execute_reply":"2025-05-08T17:24:46.388712Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import random\n\n# Set of punctuations AEDA uses\nPUNCTUATIONS = ['.', ',', '!', '?', ';', ':']\n\ndef aeda(sentence, punc_ratio=0.3, max_insert=3):\n    words = sentence.split()\n    n = len(words)\n    num_puncs = min(max_insert, max(1, int(punc_ratio * n)))\n\n    insert_positions = random.sample(range(n), num_puncs)\n    for pos in insert_positions:\n        punct = random.choice(PUNCTUATIONS)\n        words[pos] = words[pos] + punct\n    return ' '.join(words)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T09:53:45.785761Z","iopub.execute_input":"2025-05-08T09:53:45.786133Z","iopub.status.idle":"2025-05-08T09:53:45.792370Z","shell.execute_reply.started":"2025-05-08T09:53:45.786097Z","shell.execute_reply":"2025-05-08T09:53:45.791578Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\nfrom datasets import Dataset\nimport pandas as pd\nimport numpy as np\nimport json\nimport os\n\ndef prepare_gold_dataset(clean_path, gold_path, output_path):\n    with open(clean_path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    with open(gold_path, \"r\", encoding=\"utf-8\") as f:\n        gold_labels = json.load(f)\n    label_dict = {entry[\"id\"]: 1 if entry[\"value\"] == \"YES\" else 0 for entry in gold_labels}\n    updated_data = {}\n    for tweet_id, tweet_info in data.items():\n        tweet_info = tweet_info.copy()\n        gold_id = tweet_info.get(\"id_EXIST\")\n        if gold_id in label_dict:\n            tweet_info[\"label\"] = label_dict[gold_id]\n        updated_data[tweet_id] = tweet_info\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(updated_data, f, indent=2, ensure_ascii=False)\n    print(f\"✅ Gold-labeled training set saved to {output_path}\")\n\ndef load_and_tokenize_dataset(json_path, tokenizer, max_length=256, apply_aeda=False):\n    df = pd.read_json(json_path).T\n    df = df[df['lang'] == 'en']\n    df = df.dropna(subset=['label'])\n    df = df[['tweet', 'label']].rename(columns={'tweet': 'text'})\n\n    if apply_aeda:\n        augmented_rows = []\n        for _, row in df.iterrows():\n            aug_text = aeda(row['text'])\n            augmented_rows.append({'text': aug_text, 'label': row['label']})\n        aug_df = pd.DataFrame(augmented_rows)\n        df = pd.concat([df, aug_df], ignore_index=True)  # add augmented examples\n\n    df = df.sample(frac=1, random_state=42)\n    dataset = Dataset.from_pandas(df)\n    return dataset.train_test_split(test_size=0.1)\n\n\ndef train_model(json_path, model_checkpoint, save_name, use_aeda=False):\n    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n    splits = load_and_tokenize_dataset(json_path, tokenizer, apply_aeda=True)\n    \n    def preprocess(example):\n        return tokenizer(example['text'], truncation=True, padding='max_length', max_length=256)\n    \n    train_ds = splits['train'].map(preprocess, batched=True)\n    val_ds = splits['test'].map(preprocess, batched=True)\n\n    training_args = TrainingArguments(\n        output_dir=f\"results/{save_name}\",\n        num_train_epochs=3,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=8,\n        warmup_steps=100,\n        weight_decay=0.01,\n        logging_dir=f\"logs/{save_name}\",\n        logging_steps=50,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_ds,\n        eval_dataset=val_ds,\n        tokenizer=tokenizer,\n        data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n    )\n\n    trainer.train()\n    model.save_pretrained(f\"{save_name}_sexism_classifier_aeda\")\n    tokenizer.save_pretrained(f\"{save_name}_sexism_classifier_aeda\")\n    print(f\"✅ Model saved to {save_name}_sexism_classifier_aeda\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T09:56:15.309604Z","iopub.execute_input":"2025-05-08T09:56:15.309895Z","iopub.status.idle":"2025-05-08T09:56:15.321906Z","shell.execute_reply.started":"2025-05-08T09:56:15.309872Z","shell.execute_reply":"2025-05-08T09:56:15.321248Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"prepare_gold_dataset(\n    clean_path=\"/kaggle/input/translated/EXIST2025_training_translated_en.json\",\n    gold_path=\"/kaggle/input/gold-hard/EXIST2025_training_task1_1_gold_hard.json\",\n    output_path=\"EXIST2025_training_with_gold.json\"\n)\ntrain_model(\"EXIST2025_training_with_gold.json\", \"distilroberta-base\", \"distilroberta-base\", use_aeda=True)\ntrain_model(\"EXIST2025_training_with_gold.json\", \"bert-base-uncased\", \"bert-base-uncased\", use_aeda=True)\ntrain_model(\"EXIST2025_training_with_gold.json\", \"roberta-base\", \"roberta-base\", use_aeda=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T09:56:21.754315Z","iopub.execute_input":"2025-05-08T09:56:21.754631Z"}},"outputs":[{"name":"stdout","text":"✅ Gold-labeled training set saved to EXIST2025_training_with_gold.json\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5166 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5940a8b5b8ec4fc9a1497d6bcff5d8ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/574 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04ae2a62390c43c188185f05ac3fe0cc"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-14-60f572398734>:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='828' max='969' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [828/969 03:52 < 00:39, 3.56 it/s, Epoch 2.56/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.389300</td>\n      <td>0.319167</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.219500</td>\n      <td>0.231406</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport pandas as pd\nimport json\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\nfrom scipy.optimize import minimize\n\n# === Load Gold-Labeled Dev Set ===\ndf = pd.read_json(\"EXIST2025_dev_cleaned_en.json\").T\ndf = df[df['lang'] == 'en']\n\nwith open(\"/kaggle/input/gold-hard-dev/EXIST2025_dev_task1_1_gold_hard (1).json\", \"r\", encoding=\"utf-8\") as f:\n    gold = json.load(f)\n\nlabel_map = {entry[\"id\"]: 1 if entry[\"value\"] == \"YES\" else 0 for entry in gold}\ndf['label'] = df['id_EXIST'].map(label_map)\ndf = df.dropna(subset=['label'])\ndf['label'] = df['label'].astype(int)\ndf = df[['id_EXIST', 'tweet', 'label']].rename(columns={'id_EXIST': 'id', 'tweet': 'text'})\n\nprint(f\"✅ Total gold-labeled dev tweets: {len(df)}\")\n\n# === Load Predictions from Each Model ===\nmodel_paths = [\n    \"distilroberta-base_sexism_classifier_aeda\",\n    \"bert-base-uncased_sexism_classifier_aeda\",\n    \"roberta-base_sexism_classifier_aeda\"\n]\n\nall_model_probs = []\n\nfor path in model_paths:\n    tokenizer = AutoTokenizer.from_pretrained(path)\n    model = AutoModelForSequenceClassification.from_pretrained(path)\n    model.eval()\n\n    model_probs = []\n    for text in df['text']:\n        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n        with torch.no_grad():\n            logits = model(**inputs).logits\n            probs = torch.softmax(logits, dim=1).squeeze(0).cpu().numpy()\n            model_probs.append(probs)\n\n    all_model_probs.append(np.array(model_probs))\n\nall_model_probs = np.array(all_model_probs)  # shape: [num_models, num_samples, 2]\ntrue_labels = df['label'].tolist()\n\n# === Define Objective Function for Optimization (maximize F1 → minimize -F1) ===\ndef evaluate_weights(weights, all_model_probs, true_labels):\n    weights = np.array(weights)\n    weights = weights / weights.sum()  # Normalize weights\n    weighted_probs = np.average(all_model_probs, axis=0, weights=weights)\n    preds = np.argmax(weighted_probs, axis=1)\n    _, _, f1, _ = precision_recall_fscore_support(true_labels, preds, average='binary')\n    return -f1  # Minimize negative F1\n\n# === Optimize Weights ===\ninitial_weights = np.ones(len(model_paths)) / len(model_paths)\nbounds = [(0, 1)] * len(model_paths)\nconstraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n\nresult = minimize(\n    evaluate_weights,\n    initial_weights,\n    args=(all_model_probs, true_labels),\n    method='SLSQP',\n    bounds=bounds,\n    constraints=constraints\n)\n\nbest_weights = result.x\nprint(f\"\\n Optimal Ensemble Weights: {best_weights}\")\n\n# === Use Optimal Weights to Make Final Prediction ===\nweighted_probs = np.average(all_model_probs, axis=0, weights=best_weights)\nensemble_preds = np.argmax(weighted_probs, axis=1)\n\n# === Evaluation ===\nprecision, recall, f1, _ = precision_recall_fscore_support(true_labels, ensemble_preds, average='binary')\naccuracy = accuracy_score(true_labels, ensemble_preds)\n\nprint(\"\\n🔗 Ensemble Evaluation on GOLD Dev Set (Optimized Soft Voting):\")\nprint(f\"Accuracy:  {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall:    {recall:.4f}\")\nprint(f\"F1 Score:  {f1:.4f}\")\nprint(\"\\n📄 Classification Report:\")\nprint(classification_report(true_labels, ensemble_preds, target_names=[\"non-sexist\", \"sexist\"]))\n\n# === Save Output for PyEvALL ===\noutput = []\nfor tweet_id, pred in zip(df['id'], ensemble_preds):\n    output.append({\n        \"test_case\": \"EXIST2025\",\n        \"id\": str(tweet_id),\n        \"value\": \"YES\" if pred == 1 else \"NO\"\n    })\n\noutput_sorted = sorted(output, key=lambda x: int(x[\"id\"]))\nwith open(\"ensemble_predictions_output.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(output_sorted, f, indent=2, ensure_ascii=False)\n\nprint(\"✅ Predictions saved for PyEvALL evaluation\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Results for AEDA+ensemble for english (Better than just ensemble)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyevall.evaluation import PyEvALLEvaluation\nfrom pyevall.utils.utils import PyEvALLUtils\n\npredictions = \"/kaggle/input/ensemble-aeda/ensemble_predictions_output_aeda.json\"\ngold = \"/kaggle/input/gold-hard-dev/EXIST2025_dev_task1_1_gold_hard (1).json\"\n\n# Initialize evaluator\nevaluator = PyEvALLEvaluation()\n\n# Set parameters\nparams = {\n    PyEvALLUtils.PARAM_REPORT: PyEvALLUtils.PARAM_OPTION_REPORT_EMBEDDED\n}\n\n# Choose metrics (ICM for hard labels)\nmetrics = [\"ICM\", \"ICMNorm\", \"FMeasure\"]  # You can also try ICMSoft for soft scores\n\n# Run evaluation\nreport = evaluator.evaluate(predictions, gold, metrics, **params)\nreport.print_report()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T17:26:27.379932Z","iopub.execute_input":"2025-05-08T17:26:27.380312Z","iopub.status.idle":"2025-05-08T17:26:29.392239Z","shell.execute_reply.started":"2025-05-08T17:26:27.380283Z","shell.execute_reply":"2025-05-08T17:26:29.391468Z"}},"outputs":[{"name":"stdout","text":"2025-05-08 17:26:27,850 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM', 'ICMNorm', 'FMeasure']\n2025-05-08 17:26:27,955 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-08 17:26:28,308 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Normalized evaluation method\n2025-05-08 17:26:28,310 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-08 17:26:28,635 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-08 17:26:29,035 - pyevall.metrics.metrics - INFO -             evaluate() - Executing fmeasure evaluation method\ncargado 29\n{\n  \"metrics\": {\n    \"ICM\": {\n      \"name\": \"Information Contrast model\",\n      \"acronym\": \"ICM\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": -0.2553385493787333\n        }],\n        \"average_per_test_case\": -0.2553385493787333\n      }\n    },\n    \"ICMNorm\": {\n      \"name\": \"Normalized Information Contrast Model\",\n      \"acronym\": \"ICM-Norm\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 0.3722698817822821\n        }],\n        \"average_per_test_case\": 0.3722698817822821\n      }\n    },\n    \"FMeasure\": {\n      \"name\": \"F-Measure\",\n      \"acronym\": \"F1\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"classes\": {\n            \"YES\": 0.49845201238390097,\n            \"NO\": 0.6010928961748634\n          },\n          \"average\": 0.5497724542793822\n        }],\n        \"average_per_test_case\": 0.5497724542793822\n      }\n    }\n  },\n  \"files\": {\n    \"ensemble_predictions_output_aeda.json\": {\n      \"name\": \"ensemble_predictions_output_aeda.json\",\n      \"status\": \"OK\",\n      \"gold\": false,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: ensemble_predictions_output_aeda.json.\",\n      \"errors\": {}\n    },\n    \"EXIST2025_dev_task1_1_gold_hard (1).json\": {\n      \"name\": \"EXIST2025_dev_task1_1_gold_hard (1).json\",\n      \"status\": \"OK\",\n      \"gold\": true,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_task1_1_gold_hard (1).json.\",\n      \"errors\": {}\n    }\n  }\n}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"##For spanish (Base model was distilroberta-base)\n##Ensemble model (distilroberta, bert-spanish, xlmroberta)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport numpy as np\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding\n)\n\n# === Step 1: Prepare gold-labeled Spanish training data ===\ndef prepare_gold_labeled_training_data(cleaned_path, gold_path, output_path):\n    with open(cleaned_path, \"r\", encoding=\"utf-8\") as f:\n        cleaned_data = json.load(f)\n\n    with open(gold_path, \"r\", encoding=\"utf-8\") as f:\n        gold_labels = json.load(f)\n\n    # Map gold labels without checking for 'language'\n    label_map = {entry[\"id\"]: 1 if entry[\"value\"] == \"YES\" else 0 for entry in gold_labels}\n\n    updated_data = {}\n    for tweet_id, tweet in cleaned_data.items():\n        tweet = tweet.copy()\n        id_ = tweet.get(\"id_EXIST\")\n        if tweet.get(\"lang\") == \"es\" and id_ in label_map:\n            tweet[\"label\"] = label_map[id_]\n            updated_data[tweet_id] = tweet\n\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(updated_data, f, indent=2, ensure_ascii=False)\n\n    print(f\"✅ Gold-labeled Spanish training data saved to: {output_path}\")\n\n# === Step 2: Generic training function ===\ndef train_model(json_path, model_checkpoint, save_name):\n    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n\n    df = pd.read_json(json_path)\n    df = df.T\n    df = df[df['lang'] == 'es']\n    df = df.dropna(subset=['label'])\n    df = df[['tweet', 'label']].rename(columns={'tweet': 'text'})\n    df = df.sample(frac=1, random_state=42)\n\n    dataset = Dataset.from_pandas(df)\n    splits = dataset.train_test_split(test_size=0.1)\n\n    def tokenize_fn(example):\n        return tokenizer(example['text'], truncation=True, padding='max_length', max_length=256)\n\n    train_ds = splits['train'].map(tokenize_fn, batched=True)\n    val_ds = splits['test'].map(tokenize_fn, batched=True)\n\n    training_args = TrainingArguments(\n        output_dir=f\"results/{save_name}_es\",\n        num_train_epochs=3,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=8,\n        warmup_steps=100,\n        weight_decay=0.01,\n        logging_dir=f\"logs/{save_name}_es\",\n        logging_steps=50,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        save_total_limit=1,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_ds,\n        eval_dataset=val_ds,\n        tokenizer=tokenizer,\n        data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n    )\n\n    trainer.train()\n\n    model.save_pretrained(f\"{save_name}_es_sexism_classifier\")\n    tokenizer.save_pretrained(f\"{save_name}_es_sexism_classifier\")\n    print(f\"✅ Model saved: {save_name}_es_sexism_classifier\")\n\n# === Step 3: Run everything ===\nprepare_gold_labeled_training_data(\n    \"/kaggle/input/translated/EXIST2025_training_translated_es.json\",\n    \"/kaggle/input/gold-hard/EXIST2025_training_task1_1_gold_hard.json\",\n    \"EXIST2025_training_with_gold_es.json\"\n)\n\n# Train all 3 models\ntrain_model(\"EXIST2025_training_with_gold_es.json\", \"distilroberta-base\", \"distilroberta-base\")\n# train_model(\"EXIST2025_training_with_gold_es.json\", \"bert-base-uncased\", \"bert-base-uncased\")\ntrain_model(\"EXIST2025_training_with_gold_es.json\", \"dccuchile/bert-base-spanish-wwm-cased\", \"bert-spanish\")\ntrain_model(\"EXIST2025_training_with_gold_es.json\", \"xlm-roberta-base\", \"xlm-roberta-base\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:44:43.388211Z","iopub.execute_input":"2025-05-08T08:44:43.388556Z","iopub.status.idle":"2025-05-08T08:59:50.868033Z","shell.execute_reply.started":"2025-05-08T08:44:43.388526Z","shell.execute_reply":"2025-05-08T08:59:50.867149Z"}},"outputs":[{"name":"stdout","text":"✅ Gold-labeled Spanish training data saved to: EXIST2025_training_with_gold_es.json\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a05f9dd0f0447fb82c98b6d1a5bcee7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77687caa22a3414a88cb2fd7c36d13da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15175d7e8865410184bd793969700bc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26b37fccea9d4b9b8d1c4769a561b823"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86dbcd4576c44daa8d5bc13b9901b746"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16645a798b0541b3b362996c607ed66d"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2874 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6fe168bd75148e78f97330875bede6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/320 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8854a7612f6242ad8604adbcab51b5e7"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-5-02775b12cc0e>:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250508_084514-0dc2yrbv</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/faisalsara124-habib-university/huggingface/runs/0dc2yrbv' target=\"_blank\">results/distilroberta-base_es</a></strong> to <a href='https://wandb.ai/faisalsara124-habib-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/faisalsara124-habib-university/huggingface' target=\"_blank\">https://wandb.ai/faisalsara124-habib-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/faisalsara124-habib-university/huggingface/runs/0dc2yrbv' target=\"_blank\">https://wandb.ai/faisalsara124-habib-university/huggingface/runs/0dc2yrbv</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='540' max='540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [540/540 02:33, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.608300</td>\n      <td>0.645480</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.514100</td>\n      <td>0.657212</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.414400</td>\n      <td>0.590656</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model saved: distilroberta-base_es_sexism_classifier\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/364 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b145ddd35bb14e5b8514ce40a876eb8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/648 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a81f25f291b7483b93e5ce222323d112"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/242k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d292176e769047f68a5fe68d77863e02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/480k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fb8afa4cd7e4fab98f0403f904fee9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/134 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54daa6668ac44cb6a3ef6e4831c566b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc1b5bee52bd4de1a53a946265dbf781"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2874 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3ee5a29c3364f91a879ca409596dd91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/320 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b23ce182faf4127a9c0c8c83f20443b"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-5-02775b12cc0e>:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='540' max='540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [540/540 04:43, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.484600</td>\n      <td>0.460834</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.267600</td>\n      <td>0.446400</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.114100</td>\n      <td>0.752613</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model saved: bert-spanish_es_sexism_classifier\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f34dabad68b4a21bafdaa0667ab72f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46bb92cb86304d37bb36ebb03450d601"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c2fdd5c2ead43d5823bf7402fe17ebc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6805ffbe24ca4fe3ad1e14d8d3678692"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd98170c69b54836a65a3502b11f97d2"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2874 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ce2cbb4378e49e2a89157f840eea534"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/320 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fc73c0788da45b8b9a6d9798c3d3702"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-5-02775b12cc0e>:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='540' max='540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [540/540 06:47, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.653700</td>\n      <td>0.544988</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.416200</td>\n      <td>0.465979</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.310000</td>\n      <td>0.498115</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model saved: xlm-roberta-base_es_sexism_classifier\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"train_model(\"EXIST2025_training_with_gold_es.json\", \"PlanTL-GOB-ES/roberta-base-bne\", \"roberta-bne\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T09:00:33.841296Z","iopub.execute_input":"2025-05-08T09:00:33.841642Z","iopub.status.idle":"2025-05-08T09:05:25.598003Z","shell.execute_reply.started":"2025-05-08T09:00:33.841614Z","shell.execute_reply":"2025-05-08T09:05:25.596959Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f5a813b0d004ad186928b347ee7035b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/851k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac0a36401d724daaa0a139b1feeff2b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/509k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5868cf20119341768672805f102283ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.21M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5dbf5cac9b244bb8da48b45b2e6638a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/957 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f2506ffbd054f349b8f329db797f58f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/613 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b153bb615f254136b134b7b8bf08a8d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c728e67e757a4b9d9e21fefe46de3576"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at PlanTL-GOB-ES/roberta-base-bne and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2874 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b3d8293ae664ae7b970d1f66472d3f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/320 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6eb92191f534dc4897c1d6c8005d482"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-5-02775b12cc0e>:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='540' max='540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [540/540 04:41, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.452500</td>\n      <td>0.585962</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.238500</td>\n      <td>0.510444</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.069900</td>\n      <td>0.745420</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Model saved: roberta-bne_es_sexism_classifier\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"##Finds out the best weights for the models and gets the evaluations ensemble (Spanish)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport pandas as pd\nimport json\nimport numpy as np\nfrom sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score, classification_report\nfrom itertools import product\nfrom tqdm import tqdm\n\n# === Load Spanish Dev Set ===\ndf = pd.read_json(\"EXIST2025_dev_cleaned_es.json\").T\ndf = df[df['lang'] == 'es']\n\nwith open(\"/kaggle/input/gold-hard-dev/EXIST2025_dev_task1_1_gold_hard (1).json\", \"r\", encoding=\"utf-8\") as f:\n    gold = json.load(f)\n\nlabel_map = {entry[\"id\"]: 1 if entry[\"value\"] == \"YES\" else 0 for entry in gold}\ndf['label'] = df['id_EXIST'].map(label_map)\ndf = df.dropna(subset=['label'])\ndf['label'] = df['label'].astype(int)\ndf = df[['id_EXIST', 'tweet', 'label']].rename(columns={'id_EXIST': 'id', 'tweet': 'text'})\n\nprint(f\"✅ Total Spanish dev samples with gold labels: {len(df)}\")\n\n# === Load all 3 trained Spanish models ===\nmodel_paths = [\n    # \"distilroberta-base_es_sexism_classifier\",\n     \"roberta-bne_es_sexism_classifier\",\n    # \"bert-base-uncased_es_sexism_classifier\",\n    \"bert-spanish_es_sexism_classifier\",\n    \"xlm-roberta-base_es_sexism_classifier\"\n]\n\nall_model_probs = []\nfor path in model_paths:\n    tokenizer = AutoTokenizer.from_pretrained(path)\n    model = AutoModelForSequenceClassification.from_pretrained(path)\n    model.eval()\n\n    probs = []\n    for text in df['text']:\n        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n        with torch.no_grad():\n            logits = model(**inputs).logits\n            prob = torch.softmax(logits, dim=1).squeeze(0).cpu().numpy()\n            probs.append(prob)\n\n    all_model_probs.append(np.array(probs))  # shape: [num_samples, 2]\n\n# === Grid search to find best weights ===\nall_model_probs = np.array(all_model_probs)  # shape: [3, num_samples, 2]\ntrue_labels = np.array(df['label'].tolist())\n\nsearch_space = np.arange(0.0, 1.1, 0.1)\nbest_f1 = 0\nbest_weights = None\nbest_preds = None\n\nprint(\"🔍 Grid searching for best weights (Spanish)...\")\nfor w1, w2, w3 in tqdm(product(search_space, repeat=3)):\n    weights = np.array([w1, w2, w3])\n    if np.isclose(weights.sum(), 1.0):\n        weighted_probs = np.average(all_model_probs, axis=0, weights=weights)\n        preds = np.argmax(weighted_probs, axis=1)\n        f1 = f1_score(true_labels, preds, average='binary')\n        if f1 > best_f1:\n            best_f1 = f1\n            best_weights = weights\n            best_preds = preds\n\n# === Print best weights and metrics\nprint(\"\\n Best F1 Score (Spanish Ensemble):\")\nprint(f\"Weights → DistilRoBERTa: {best_weights[0]:.2f}, BERT: {best_weights[1]:.2f}, RoBERTa: {best_weights[2]:.2f}\")\nprint(f\"F1 Score: {best_f1:.4f}\")\n\nprecision, recall, _, _ = precision_recall_fscore_support(true_labels, best_preds, average='binary')\naccuracy = accuracy_score(true_labels, best_preds)\n\nprint(f\"Accuracy:  {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall:    {recall:.4f}\")\nprint(\"\\n📄 Classification Report:\")\nprint(classification_report(true_labels, best_preds, target_names=[\"non-sexist\", \"sexist\"]))\n\n# === Save predictions for PyEvALL\noutput = []\nfor tweet_id, pred in zip(df['id'], best_preds):\n    output.append({\n        \"test_case\": \"EXIST2025\",\n        \"id\": str(tweet_id),\n        \"value\": \"YES\" if pred == 1 else \"NO\"\n    })\n\noutput_sorted = sorted(output, key=lambda x: int(x[\"id\"]))\n\nwith open(\"spanish_ensemble_predictions.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(output_sorted, f, indent=2, ensure_ascii=False)\n\nprint(\"✅ Spanish ensemble predictions saved for PyEvALL: 'spanish_ensemble_predictions.json'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T09:06:50.162702Z","iopub.execute_input":"2025-05-08T09:06:50.163038Z","iopub.status.idle":"2025-05-08T09:09:02.083950Z","shell.execute_reply.started":"2025-05-08T09:06:50.163007Z","shell.execute_reply":"2025-05-08T09:09:02.083131Z"}},"outputs":[{"name":"stdout","text":"✅ Total Spanish dev samples with gold labels: 490\n🔍 Grid searching for best weights (Spanish)...\n","output_type":"stream"},{"name":"stderr","text":"1331it [00:00, 9682.30it/s]","output_type":"stream"},{"name":"stdout","text":"\n Best F1 Score (Spanish Ensemble):\nWeights → DistilRoBERTa: 0.20, BERT: 0.30, RoBERTa: 0.50\nF1 Score: 0.8402\nAccuracy:  0.8347\nPrecision: 0.8659\nRecall:    0.8161\n\n📄 Classification Report:\n              precision    recall  f1-score   support\n\n  non-sexist       0.80      0.86      0.83       229\n      sexist       0.87      0.82      0.84       261\n\n    accuracy                           0.83       490\n   macro avg       0.83      0.84      0.83       490\nweighted avg       0.84      0.83      0.83       490\n\n✅ Spanish ensemble predictions saved for PyEvALL: 'spanish_ensemble_predictions.json'\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"##Testing models to see which one performs the best ensemble spanish","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport pandas as pd\nimport json\nimport numpy as np\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_recall_fscore_support,\n    classification_report\n)\n\n# === Load Spanish Dev Set ===\ndf = pd.read_json(\"EXIST2025_dev_cleaned_es.json\").T\ndf = df[df['lang'] == 'es']\n\nwith open(\"/kaggle/input/gold-hard-dev/EXIST2025_dev_task1_1_gold_hard (1).json\", \"r\", encoding=\"utf-8\") as f:\n    gold = json.load(f)\n\nlabel_map = {entry[\"id\"]: 1 if entry[\"value\"] == \"YES\" else 0 for entry in gold}\ndf['label'] = df['id_EXIST'].map(label_map)\ndf = df.dropna(subset=['label'])\ndf['label'] = df['label'].astype(int)\ndf = df[['id_EXIST', 'tweet', 'label']].rename(columns={'id_EXIST': 'id', 'tweet': 'text'})\n\nprint(f\"✅ Total gold-labeled Spanish dev tweets: {len(df)}\")\n\n# === Define model paths and names ===\nmodels_to_evaluate = {\n    # \"DistilRoBERTa (ES)\": \"distilroberta-base_es_sexism_classifier\",\n    # \"BERT-base (ES)\": \"bert-base-uncased_es_sexism_classifier\",\n    \"Roberta-BNE (ES)\": \"roberta-bne_es_sexism_classifier\",\n    \"BERT (ES)\": \"bert-spanish_es_sexism_classifier\",\n    \"XLM-RoBERTa (ES)\": \"xlm-roberta-base_es_sexism_classifier\",\n}\n\n# === Evaluate each model\nfor model_name, model_path in models_to_evaluate.items():\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n    model.eval()\n\n    preds = []\n    for text in df['text']:\n        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n        with torch.no_grad():\n            logits = model(**inputs).logits\n            pred = torch.argmax(logits, dim=1).item()\n            preds.append(pred)\n\n    true_labels = df['label'].tolist()\n    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, preds, average='binary')\n    accuracy = accuracy_score(true_labels, preds)\n\n    print(f\"\\n🔍 Evaluation for {model_name}\")\n    print(f\"Accuracy:  {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall:    {recall:.4f}\")\n    print(f\"F1 Score:  {f1:.4f}\")\n    print(\"\\n📄 Classification Report:\")\n    print(classification_report(true_labels, preds, target_names=[\"non-sexist\", \"sexist\"]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T17:48:05.517117Z","iopub.execute_input":"2025-04-22T17:48:05.517477Z","iopub.status.idle":"2025-04-22T17:50:00.786238Z","shell.execute_reply.started":"2025-04-22T17:48:05.517441Z","shell.execute_reply":"2025-04-22T17:50:00.785394Z"}},"outputs":[{"name":"stdout","text":"✅ Total gold-labeled Spanish dev tweets: 490\n\n🔍 Evaluation for DistilRoBERTa (ES)\nAccuracy:  0.7510\nPrecision: 0.7405\nRecall:    0.8199\nF1 Score:  0.7782\n\n📄 Classification Report:\n              precision    recall  f1-score   support\n\n  non-sexist       0.77      0.67      0.72       229\n      sexist       0.74      0.82      0.78       261\n\n    accuracy                           0.75       490\n   macro avg       0.75      0.75      0.75       490\nweighted avg       0.75      0.75      0.75       490\n\n\n🔍 Evaluation for BERT (ES)\nAccuracy:  0.7980\nPrecision: 0.8716\nRecall:    0.7280\nF1 Score:  0.7933\n\n📄 Classification Report:\n              precision    recall  f1-score   support\n\n  non-sexist       0.74      0.88      0.80       229\n      sexist       0.87      0.73      0.79       261\n\n    accuracy                           0.80       490\n   macro avg       0.81      0.80      0.80       490\nweighted avg       0.81      0.80      0.80       490\n\n\n🔍 Evaluation for XLM-RoBERTa (ES)\nAccuracy:  0.8082\nPrecision: 0.8036\nRecall:    0.8467\nF1 Score:  0.8246\n\n📄 Classification Report:\n              precision    recall  f1-score   support\n\n  non-sexist       0.81      0.76      0.79       229\n      sexist       0.80      0.85      0.82       261\n\n    accuracy                           0.81       490\n   macro avg       0.81      0.81      0.81       490\nweighted avg       0.81      0.81      0.81       490\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"pip install PyEvALL","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T13:45:06.892044Z","iopub.execute_input":"2025-04-23T13:45:06.892353Z","iopub.status.idle":"2025-04-23T13:45:34.550952Z","shell.execute_reply.started":"2025-04-23T13:45:06.892326Z","shell.execute_reply":"2025-04-23T13:45:34.549953Z"}},"outputs":[{"name":"stdout","text":"Collecting PyEvALL\n  Downloading PyEvALL-0.1.78.tar.gz (39 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting jsbeautifier==1.14.9 (from PyEvALL)\n  Downloading jsbeautifier-1.14.9.tar.gz (75 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: jsonschema==4.23.0 in /usr/local/lib/python3.10/dist-packages (from PyEvALL) (4.23.0)\nRequirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (from PyEvALL) (1.26.4)\nRequirement already satisfied: pandas==2.2.3 in /usr/local/lib/python3.10/dist-packages (from PyEvALL) (2.2.3)\nCollecting setuptools==69.5.1 (from PyEvALL)\n  Downloading setuptools-69.5.1-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.10/dist-packages (from PyEvALL) (0.9.0)\nRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from jsbeautifier==1.14.9->PyEvALL) (1.17.0)\nCollecting editorconfig>=0.12.2 (from jsbeautifier==1.14.9->PyEvALL)\n  Downloading EditorConfig-0.17.0-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.23.0->PyEvALL) (25.1.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.23.0->PyEvALL) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.23.0->PyEvALL) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.23.0->PyEvALL) (0.22.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->PyEvALL) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->PyEvALL) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->PyEvALL) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->PyEvALL) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->PyEvALL) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->PyEvALL) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->PyEvALL) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->PyEvALL) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->PyEvALL) (2025.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy==1.26.4->PyEvALL) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy==1.26.4->PyEvALL) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy==1.26.4->PyEvALL) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy==1.26.4->PyEvALL) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy==1.26.4->PyEvALL) (2024.2.0)\nDownloading setuptools-69.5.1-py3-none-any.whl (894 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m894.6/894.6 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading EditorConfig-0.17.0-py3-none-any.whl (16 kB)\nBuilding wheels for collected packages: PyEvALL, jsbeautifier\n  Building wheel for PyEvALL (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for PyEvALL: filename=PyEvALL-0.1.78-py3-none-any.whl size=34777 sha256=91e3a5054280cd2d8c26384d8868fbb5c4079c696d14f2db42a4929f059f8d37\n  Stored in directory: /root/.cache/pip/wheels/f0/3a/51/f8c268e67356c15a602eef8ac7a5e18ba4677b4ec8b45b8a25\n  Building wheel for jsbeautifier (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for jsbeautifier: filename=jsbeautifier-1.14.9-py3-none-any.whl size=94157 sha256=fc9552ba1d6e007a2972d336fd9a871dd4413b8d28f0977dd106a09987a62977\n  Stored in directory: /root/.cache/pip/wheels/c4/5c/25/09f8b2e8dddb4fc3d70817c67b375a9069a2628847ffbdfc65\nSuccessfully built PyEvALL jsbeautifier\nInstalling collected packages: editorconfig, setuptools, jsbeautifier, PyEvALL\n  Attempting uninstall: setuptools\n    Found existing installation: setuptools 75.1.0\n    Uninstalling setuptools-75.1.0:\n      Successfully uninstalled setuptools-75.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed PyEvALL-0.1.78 editorconfig-0.17.0 jsbeautifier-1.14.9 setuptools-69.5.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"##ICM SCORES for ensemble spanish ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyevall.evaluation import PyEvALLEvaluation\nfrom pyevall.utils.utils import PyEvALLUtils\n\nevaluator = PyEvALLEvaluation()\nparams = {PyEvALLUtils.PARAM_REPORT: PyEvALLUtils.PARAM_OPTION_REPORT_EMBEDDED}\nmetrics = [\"ICM\", \"ICMNorm\", \"FMeasure\"]\n\nreport = evaluator.evaluate(\"spanish_ensemble_predictions.json\", \"/kaggle/input/gold-hard-dev/EXIST2025_dev_task1_1_gold_hard (1).json\", metrics, **params)\nreport.print_report()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T09:09:25.986189Z","iopub.execute_input":"2025-05-08T09:09:25.986481Z","iopub.status.idle":"2025-05-08T09:09:27.615760Z","shell.execute_reply.started":"2025-05-08T09:09:25.986458Z","shell.execute_reply":"2025-05-08T09:09:27.614921Z"}},"outputs":[{"name":"stdout","text":"2025-05-08 09:09:26,044 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM', 'ICMNorm', 'FMeasure']\n2025-05-08 09:09:26,158 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-08 09:09:26,500 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Normalized evaluation method\n2025-05-08 09:09:26,504 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-08 09:09:26,854 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-08 09:09:27,264 - pyevall.metrics.metrics - INFO -             evaluate() - Executing fmeasure evaluation method\ncargado 29\n{\n  \"metrics\": {\n    \"ICM\": {\n      \"name\": \"Information Contrast model\",\n      \"acronym\": \"ICM\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": -0.20812318305469338\n        }],\n        \"average_per_test_case\": -0.20812318305469338\n      }\n    },\n    \"ICMNorm\": {\n      \"name\": \"Normalized Information Contrast Model\",\n      \"acronym\": \"ICM-Norm\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 0.3958888156915415\n        }],\n        \"average_per_test_case\": 0.3958888156915415\n      }\n    },\n    \"FMeasure\": {\n      \"name\": \"F-Measure\",\n      \"acronym\": \"F1\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"classes\": {\n            \"YES\": 0.6077032810271041,\n            \"NO\": 0.5421853388658369\n          },\n          \"average\": 0.5749443099464705\n        }],\n        \"average_per_test_case\": 0.5749443099464705\n      }\n    }\n  },\n  \"files\": {\n    \"spanish_ensemble_predictions.json\": {\n      \"name\": \"spanish_ensemble_predictions.json\",\n      \"status\": \"OK\",\n      \"gold\": false,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: spanish_ensemble_predictions.json.\",\n      \"errors\": {}\n    },\n    \"EXIST2025_dev_task1_1_gold_hard (1).json\": {\n      \"name\": \"EXIST2025_dev_task1_1_gold_hard (1).json\",\n      \"status\": \"OK\",\n      \"gold\": true,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_task1_1_gold_hard (1).json.\",\n      \"errors\": {}\n    }\n  }\n}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Trying AEDA+ensemble for spanish","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport numpy as np\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding\n)\nimport random\n\n# === AEDA Function ===\nPUNCTUATIONS = ['.', ',', '!', '?', ';', ':']\n\ndef aeda(sentence, punc_ratio=0.3, max_insert=3):\n    words = sentence.split()\n    n = len(words)\n    num_puncs = min(max_insert, max(1, int(punc_ratio * n)))\n\n    insert_positions = random.sample(range(n), num_puncs)\n    for pos in insert_positions:\n        punct = random.choice(PUNCTUATIONS)\n        words[pos] = words[pos] + punct\n    return ' '.join(words)\n\n# === Step 1: Prepare gold-labeled Spanish training data ===\ndef prepare_gold_labeled_training_data(cleaned_path, gold_path, output_path):\n    with open(cleaned_path, \"r\", encoding=\"utf-8\") as f:\n        cleaned_data = json.load(f)\n\n    with open(gold_path, \"r\", encoding=\"utf-8\") as f:\n        gold_labels = json.load(f)\n\n    label_map = {entry[\"id\"]: 1 if entry[\"value\"] == \"YES\" else 0 for entry in gold_labels}\n\n    updated_data = {}\n    for tweet_id, tweet in cleaned_data.items():\n        tweet = tweet.copy()\n        id_ = tweet.get(\"id_EXIST\")\n        if tweet.get(\"lang\") == \"es\" and id_ in label_map:\n            tweet[\"label\"] = label_map[id_]\n            updated_data[tweet_id] = tweet\n\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(updated_data, f, indent=2, ensure_ascii=False)\n\n    print(f\"✅ Gold-labeled Spanish training data saved to: {output_path}\")\n\n# === Step 2: Generic training function with AEDA ===\ndef train_model(json_path, model_checkpoint, save_name, use_aeda=True):\n    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n\n    df = pd.read_json(json_path)\n    df = df.T\n    df = df[df['lang'] == 'es']\n    df = df.dropna(subset=['label'])\n    df = df[['tweet', 'label']].rename(columns={'tweet': 'text'})\n    df = df.sample(frac=1, random_state=42)\n\n    # Apply AEDA augmentation\n    if use_aeda:\n        augmented_rows = []\n        for _, row in df.iterrows():\n            aug_text = aeda(row['text'])\n            augmented_rows.append({'text': aug_text, 'label': row['label']})\n        aug_df = pd.DataFrame(augmented_rows)\n        df = pd.concat([df, aug_df], ignore_index=True)\n        df = df.sample(frac=1, random_state=42)  # shuffle again\n\n    dataset = Dataset.from_pandas(df)\n    splits = dataset.train_test_split(test_size=0.1)\n\n    def tokenize_fn(example):\n        return tokenizer(example['text'], truncation=True, padding='max_length', max_length=256)\n\n    train_ds = splits['train'].map(tokenize_fn, batched=True)\n    val_ds = splits['test'].map(tokenize_fn, batched=True)\n\n    training_args = TrainingArguments(\n        output_dir=f\"results/{save_name}_es\",\n        num_train_epochs=3,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=8,\n        warmup_steps=100,\n        weight_decay=0.01,\n        logging_dir=f\"logs/{save_name}_es\",\n        logging_steps=50,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        save_total_limit=1,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_ds,\n        eval_dataset=val_ds,\n        tokenizer=tokenizer,\n        data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n    )\n\n    trainer.train()\n\n    model.save_pretrained(f\"{save_name}_es_sexism_classifier_aeda\")\n    tokenizer.save_pretrained(f\"{save_name}_es_sexism_classifier_aeda\")\n    print(f\"✅ Model saved: {save_name}_es_sexism_classifier_aeda\")\n\n# === Step 3: Run everything ===\nprepare_gold_labeled_training_data(\n    \"/kaggle/input/translated/EXIST2025_training_translated_es.json\",\n    \"/kaggle/input/gold-hard/EXIST2025_training_task1_1_gold_hard.json\",\n    \"EXIST2025_training_with_gold_es.json\"\n)\n\n# Train all 3 Spanish models with AEDA\ntrain_model(\"EXIST2025_training_with_gold_es.json\", \"PlanTL-GOB-ES/roberta-base-bne\", \"roberta-bne\")\ntrain_model(\"EXIST2025_training_with_gold_es.json\", \"dccuchile/bert-base-spanish-wwm-cased\", \"bert-spanish\")\ntrain_model(\"EXIST2025_training_with_gold_es.json\", \"xlm-roberta-base\", \"xlm-roberta-base\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport pandas as pd\nimport json\nimport numpy as np\nfrom sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score, classification_report\nfrom itertools import product\nfrom tqdm import tqdm\n\n# === Load Spanish Dev Set ===\ndf = pd.read_json(\"EXIST2025_dev_cleaned_es.json\").T\ndf = df[df['lang'] == 'es']\n\nwith open(\"/kaggle/input/gold-hard-dev/EXIST2025_dev_task1_1_gold_hard (1).json\", \"r\", encoding=\"utf-8\") as f:\n    gold = json.load(f)\n\nlabel_map = {entry[\"id\"]: 1 if entry[\"value\"] == \"YES\" else 0 for entry in gold}\ndf['label'] = df['id_EXIST'].map(label_map)\ndf = df.dropna(subset=['label'])\ndf['label'] = df['label'].astype(int)\ndf = df[['id_EXIST', 'tweet', 'label']].rename(columns={'id_EXIST': 'id', 'tweet': 'text'})\n\nprint(f\"✅ Total Spanish dev samples with gold labels: {len(df)}\")\n\n# === Load all 3 trained Spanish models ===\nmodel_paths = [\n    # \"distilroberta-base_es_sexism_classifier\",\n     \"roberta-bne_es_sexism_classifier_aeda\",\n    # \"bert-base-uncased_es_sexism_classifier\",\n    \"bert-spanish_es_sexism_classifier_aeda\",\n    \"xlm-roberta-base_es_sexism_classifier_aeda\"\n]\n\nall_model_probs = []\nfor path in model_paths:\n    tokenizer = AutoTokenizer.from_pretrained(path)\n    model = AutoModelForSequenceClassification.from_pretrained(path)\n    model.eval()\n\n    probs = []\n    for text in df['text']:\n        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n        with torch.no_grad():\n            logits = model(**inputs).logits\n            prob = torch.softmax(logits, dim=1).squeeze(0).cpu().numpy()\n            probs.append(prob)\n\n    all_model_probs.append(np.array(probs))  # shape: [num_samples, 2]\n\n# === Grid search to find best weights ===\nall_model_probs = np.array(all_model_probs)  # shape: [3, num_samples, 2]\ntrue_labels = np.array(df['label'].tolist())\n\nsearch_space = np.arange(0.0, 1.1, 0.1)\nbest_f1 = 0\nbest_weights = None\nbest_preds = None\n\nprint(\"🔍 Grid searching for best weights (Spanish)...\")\nfor w1, w2, w3 in tqdm(product(search_space, repeat=3)):\n    weights = np.array([w1, w2, w3])\n    if np.isclose(weights.sum(), 1.0):\n        weighted_probs = np.average(all_model_probs, axis=0, weights=weights)\n        preds = np.argmax(weighted_probs, axis=1)\n        f1 = f1_score(true_labels, preds, average='binary')\n        if f1 > best_f1:\n            best_f1 = f1\n            best_weights = weights\n            best_preds = preds\n\n# === Print best weights and metrics\nprint(\"\\n Best F1 Score (Spanish Ensemble):\")\nprint(f\"Weights → DistilRoBERTa: {best_weights[0]:.2f}, BERT: {best_weights[1]:.2f}, RoBERTa: {best_weights[2]:.2f}\")\nprint(f\"F1 Score: {best_f1:.4f}\")\n\nprecision, recall, _, _ = precision_recall_fscore_support(true_labels, best_preds, average='binary')\naccuracy = accuracy_score(true_labels, best_preds)\n\nprint(f\"Accuracy:  {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall:    {recall:.4f}\")\nprint(\"\\n📄 Classification Report:\")\nprint(classification_report(true_labels, best_preds, target_names=[\"non-sexist\", \"sexist\"]))\n\n# === Save predictions for PyEvALL\noutput = []\nfor tweet_id, pred in zip(df['id'], best_preds):\n    output.append({\n        \"test_case\": \"EXIST2025\",\n        \"id\": str(tweet_id),\n        \"value\": \"YES\" if pred == 1 else \"NO\"\n    })\n\noutput_sorted = sorted(output, key=lambda x: int(x[\"id\"]))\n\nwith open(\"spanish_ensemble_predictions.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(output_sorted, f, indent=2, ensure_ascii=False)\n\nprint(\"✅ Spanish ensemble predictions saved for PyEvALL: 'spanish_ensemble_predictions.json'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Results for AEDA+Ensemble spanish","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyevall.evaluation import PyEvALLEvaluation\nfrom pyevall.utils.utils import PyEvALLUtils\n\nevaluator = PyEvALLEvaluation()\nparams = {PyEvALLUtils.PARAM_REPORT: PyEvALLUtils.PARAM_OPTION_REPORT_EMBEDDED}\nmetrics = [\"ICM\", \"ICMNorm\", \"FMeasure\"]\n\nreport = evaluator.evaluate(\"/kaggle/input/ensemble-aeda/spanish_ensemble_predictions_aeda.json\", \"/kaggle/input/gold-hard-dev/EXIST2025_dev_task1_1_gold_hard (1).json\", metrics, **params)\nreport.print_report()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T17:28:18.744586Z","iopub.execute_input":"2025-05-08T17:28:18.745210Z","iopub.status.idle":"2025-05-08T17:28:20.326992Z","shell.execute_reply.started":"2025-05-08T17:28:18.745177Z","shell.execute_reply":"2025-05-08T17:28:20.325999Z"}},"outputs":[{"name":"stdout","text":"2025-05-08 17:28:18,749 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM', 'ICMNorm', 'FMeasure']\n2025-05-08 17:28:18,843 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-08 17:28:19,237 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Normalized evaluation method\n2025-05-08 17:28:19,239 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-08 17:28:19,627 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-08 17:28:20,014 - pyevall.metrics.metrics - INFO -             evaluate() - Executing fmeasure evaluation method\n{\n  \"metrics\": {\n    \"ICM\": {\n      \"name\": \"Information Contrast model\",\n      \"acronym\": \"ICM\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": -0.18939784867265605\n        }],\n        \"average_per_test_case\": -0.18939784867265605\n      }\n    },\n    \"ICMNorm\": {\n      \"name\": \"Normalized Information Contrast Model\",\n      \"acronym\": \"ICM-Norm\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 0.40525594486221866\n        }],\n        \"average_per_test_case\": 0.40525594486221866\n      }\n    },\n    \"FMeasure\": {\n      \"name\": \"F-Measure\",\n      \"acronym\": \"F1\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"classes\": {\n            \"YES\": 0.6084425036390102,\n            \"NO\": 0.5590230664857531\n          },\n          \"average\": 0.5837327850623817\n        }],\n        \"average_per_test_case\": 0.5837327850623817\n      }\n    }\n  },\n  \"files\": {\n    \"spanish_ensemble_predictions_aeda.json\": {\n      \"name\": \"spanish_ensemble_predictions_aeda.json\",\n      \"status\": \"OK\",\n      \"gold\": false,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: spanish_ensemble_predictions_aeda.json.\",\n      \"errors\": {}\n    },\n    \"EXIST2025_dev_task1_1_gold_hard (1).json\": {\n      \"name\": \"EXIST2025_dev_task1_1_gold_hard (1).json\",\n      \"status\": \"OK\",\n      \"gold\": true,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_task1_1_gold_hard (1).json.\",\n      \"errors\": {}\n    }\n  }\n}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"## Final Score for english and spanish just for ensemble","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\n# === Load English & Spanish predictions ===\nwith open(\"/kaggle/input/ensemble-final-preds/ensemble_predictions_output (2).json\", \"r\", encoding=\"utf-8\") as f:\n    en_preds = json.load(f)\n\nwith open(\"/kaggle/input/ensemble-final-preds/spanish_ensemble_predictions (3).json\", \"r\", encoding=\"utf-8\") as f:\n    es_preds = json.load(f)\n\n# === Merge and sort by ID ===\ncombined_preds = en_preds + es_preds\ncombined_preds_sorted = sorted(combined_preds, key=lambda x: int(x[\"id\"]))\n\n# === Save to a single file ===\nwith open(\"combined_ensemble_predictions.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(combined_preds_sorted, f, indent=2, ensure_ascii=False)\n\nprint(\"✅ Combined predictions saved as 'combined_ensemble_predictions.json'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T09:23:28.951911Z","iopub.execute_input":"2025-05-08T09:23:28.952256Z","iopub.status.idle":"2025-05-08T09:23:28.975075Z","shell.execute_reply.started":"2025-05-08T09:23:28.952226Z","shell.execute_reply":"2025-05-08T09:23:28.974156Z"}},"outputs":[{"name":"stdout","text":"✅ Combined predictions saved as 'combined_ensemble_predictions.json'\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from pyevall.evaluation import PyEvALLEvaluation\nfrom pyevall.utils.utils import PyEvALLUtils\n\nevaluator = PyEvALLEvaluation()\nparams = {PyEvALLUtils.PARAM_REPORT: PyEvALLUtils.PARAM_OPTION_REPORT_EMBEDDED}\nmetrics = [\"ICM\", \"ICMNorm\", \"FMeasure\"]\n\nreport = evaluator.evaluate(\n    \"combined_ensemble_predictions.json\",\n    \"/kaggle/input/gold-hard-dev/EXIST2025_dev_task1_1_gold_hard (1).json\",\n    metrics,\n    **params\n)\nreport.print_report()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T09:23:32.782671Z","iopub.execute_input":"2025-05-08T09:23:32.782951Z","iopub.status.idle":"2025-05-08T09:23:34.654325Z","shell.execute_reply.started":"2025-05-08T09:23:32.782928Z","shell.execute_reply":"2025-05-08T09:23:34.653500Z"}},"outputs":[{"name":"stdout","text":"2025-05-08 09:23:32,788 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM', 'ICMNorm', 'FMeasure']\n2025-05-08 09:23:32,903 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-08 09:23:33,311 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Normalized evaluation method\n2025-05-08 09:23:33,314 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-08 09:23:33,750 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-08 09:23:34,178 - pyevall.metrics.metrics - INFO -             evaluate() - Executing fmeasure evaluation method\n{\n  \"metrics\": {\n    \"ICM\": {\n      \"name\": \"Information Contrast model\",\n      \"acronym\": \"ICM\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 0.5318559079156238\n        }],\n        \"average_per_test_case\": 0.5318559079156238\n      }\n    },\n    \"ICMNorm\": {\n      \"name\": \"Normalized Information Contrast Model\",\n      \"acronym\": \"ICM-Norm\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 0.7660546876221599\n        }],\n        \"average_per_test_case\": 0.7660546876221599\n      }\n    },\n    \"FMeasure\": {\n      \"name\": \"F-Measure\",\n      \"acronym\": \"F1\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"classes\": {\n            \"YES\": 0.8327566320645904,\n            \"NO\": 0.8551448551448552\n          },\n          \"average\": 0.8439507436047228\n        }],\n        \"average_per_test_case\": 0.8439507436047228\n      }\n    }\n  },\n  \"files\": {\n    \"combined_ensemble_predictions.json\": {\n      \"name\": \"combined_ensemble_predictions.json\",\n      \"status\": \"OK\",\n      \"gold\": false,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: combined_ensemble_predictions.json.\",\n      \"errors\": {}\n    },\n    \"EXIST2025_dev_task1_1_gold_hard (1).json\": {\n      \"name\": \"EXIST2025_dev_task1_1_gold_hard (1).json\",\n      \"status\": \"OK\",\n      \"gold\": true,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_task1_1_gold_hard (1).json.\",\n      \"errors\": {}\n    }\n  }\n}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"## Final Score for english and spanish for ensemble + AEDA (Better than just ensemble)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\n# === Load English & Spanish predictions ===\nwith open(\"/kaggle/input/ensemble-aeda/ensemble_predictions_output_aeda.json\", \"r\", encoding=\"utf-8\") as f:\n    en_preds = json.load(f)\n\nwith open(\"/kaggle/input/ensemble-aeda/spanish_ensemble_predictions_aeda.json\", \"r\", encoding=\"utf-8\") as f:\n    es_preds = json.load(f)\n\n# === Merge and sort by ID ===\ncombined_preds = en_preds + es_preds\ncombined_preds_sorted = sorted(combined_preds, key=lambda x: int(x[\"id\"]))\n\n# === Save to a single file ===\nwith open(\"combined_ensemble_predictions_aeda.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(combined_preds_sorted, f, indent=2, ensure_ascii=False)\n\nprint(\"✅ Combined predictions saved as 'combined_ensemble_predictions.json'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T17:30:13.256376Z","iopub.execute_input":"2025-05-08T17:30:13.256666Z","iopub.status.idle":"2025-05-08T17:30:13.271609Z","shell.execute_reply.started":"2025-05-08T17:30:13.256645Z","shell.execute_reply":"2025-05-08T17:30:13.270822Z"}},"outputs":[{"name":"stdout","text":"✅ Combined predictions saved as 'combined_ensemble_predictions.json'\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from pyevall.evaluation import PyEvALLEvaluation\nfrom pyevall.utils.utils import PyEvALLUtils\n\nevaluator = PyEvALLEvaluation()\nparams = {PyEvALLUtils.PARAM_REPORT: PyEvALLUtils.PARAM_OPTION_REPORT_EMBEDDED}\nmetrics = [\"ICM\", \"ICMNorm\", \"FMeasure\"]\n\nreport = evaluator.evaluate(\n    \"combined_ensemble_predictions_aeda.json\",\n    \"/kaggle/input/gold-hard-dev/EXIST2025_dev_task1_1_gold_hard (1).json\",\n    metrics,\n    **params\n)\nreport.print_report()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T17:30:16.171685Z","iopub.execute_input":"2025-05-08T17:30:16.171963Z","iopub.status.idle":"2025-05-08T17:30:17.929568Z","shell.execute_reply.started":"2025-05-08T17:30:16.171942Z","shell.execute_reply":"2025-05-08T17:30:17.928743Z"}},"outputs":[{"name":"stdout","text":"2025-05-08 17:30:16,176 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM', 'ICMNorm', 'FMeasure']\n2025-05-08 17:30:16,286 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-08 17:30:16,696 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Normalized evaluation method\n2025-05-08 17:30:16,698 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-08 17:30:17,108 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-08 17:30:17,503 - pyevall.metrics.metrics - INFO -             evaluate() - Executing fmeasure evaluation method\n{\n  \"metrics\": {\n    \"ICM\": {\n      \"name\": \"Information Contrast model\",\n      \"acronym\": \"ICM\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 0.5547872575343015\n        }],\n        \"average_per_test_case\": 0.5547872575343015\n      }\n    },\n    \"ICMNorm\": {\n      \"name\": \"Normalized Information Contrast Model\",\n      \"acronym\": \"ICM-Norm\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 0.7775258266444978\n        }],\n        \"average_per_test_case\": 0.7775258266444978\n      }\n    },\n    \"FMeasure\": {\n      \"name\": \"F-Measure\",\n      \"acronym\": \"F1\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"classes\": {\n            \"YES\": 0.8428246013667425,\n            \"NO\": 0.8606060606060606\n          },\n          \"average\": 0.8517153309864016\n        }],\n        \"average_per_test_case\": 0.8517153309864016\n      }\n    }\n  },\n  \"files\": {\n    \"combined_ensemble_predictions_aeda.json\": {\n      \"name\": \"combined_ensemble_predictions_aeda.json\",\n      \"status\": \"OK\",\n      \"gold\": false,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: combined_ensemble_predictions_aeda.json.\",\n      \"errors\": {}\n    },\n    \"EXIST2025_dev_task1_1_gold_hard (1).json\": {\n      \"name\": \"EXIST2025_dev_task1_1_gold_hard (1).json\",\n      \"status\": \"OK\",\n      \"gold\": true,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_task1_1_gold_hard (1).json.\",\n      \"errors\": {}\n    }\n  }\n}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"### Final Code after all the modifications","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Training aeda+ensemble for english (Custom training and dev sets used instead of splitting)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\nfrom datasets import Dataset\nimport pandas as pd\nimport numpy as np\nimport json\nimport os\n\n# === AEDA Function (stub) ===\ndef aeda(text):\n    # Replace this with your actual AEDA function\n    puncts = [';', ':', '!', '?', ',', '.', '。', '،']\n    words = text.split()\n    n_insertions = max(1, int(0.1 * len(words)))  # Insert punctuation into ~10% of words\n    for _ in range(n_insertions):\n        idx = np.random.randint(0, len(words))\n        punct = np.random.choice(puncts)\n        words.insert(idx, punct)\n    return \" \".join(words)\n\n# === Prepare Gold Labels ===\ndef prepare_gold_dataset(clean_path, gold_path, output_path):\n    with open(clean_path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    with open(gold_path, \"r\", encoding=\"utf-8\") as f:\n        gold_labels = json.load(f)\n    label_dict = {entry[\"id\"]: 1 if entry[\"value\"] == \"YES\" else 0 for entry in gold_labels}\n    updated_data = {}\n    for tweet_id, tweet_info in data.items():\n        tweet_info = tweet_info.copy()\n        gold_id = tweet_info.get(\"id_EXIST\")\n        if gold_id in label_dict:\n            tweet_info[\"label\"] = label_dict[gold_id]\n        updated_data[tweet_id] = tweet_info\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(updated_data, f, indent=2, ensure_ascii=False)\n    print(f\" Gold-labeled dataset saved to {output_path}\")\n\n# === Dataset Loader (for separate train/dev) ===\ndef load_and_tokenize_dataset(json_path, tokenizer, max_length=256, apply_aeda=False):\n    df = pd.read_json(json_path).T\n    df = df[df['lang'] == 'en']\n    df = df.dropna(subset=['label'])\n    df = df[['tweet', 'label']].rename(columns={'tweet': 'text'})\n\n    if apply_aeda:\n        augmented_rows = []\n        for _, row in df.iterrows():\n            aug_text = aeda(row['text'])\n            augmented_rows.append({'text': aug_text, 'label': row['label']})\n        aug_df = pd.DataFrame(augmented_rows)\n        df = pd.concat([df, aug_df], ignore_index=True)\n\n    df = df.sample(frac=1, random_state=42)\n    dataset = Dataset.from_pandas(df)\n\n    def preprocess(example):\n        return tokenizer(example['text'], truncation=True, padding='max_length', max_length=max_length)\n\n    return dataset.map(preprocess, batched=True)\n\n# === Model Training ===\ndef train_model(train_json_path, val_json_path, model_checkpoint, save_name, use_aeda=False):\n    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n\n    train_ds = load_and_tokenize_dataset(train_json_path, tokenizer, apply_aeda=use_aeda)\n    val_ds = load_and_tokenize_dataset(val_json_path, tokenizer, apply_aeda=False)\n\n    training_args = TrainingArguments(\n        output_dir=f\"results/{save_name}\",\n        num_train_epochs=3,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=8,\n        warmup_steps=100,\n        weight_decay=0.01,\n        logging_dir=f\"logs/{save_name}\",\n        logging_steps=50,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_ds,\n        eval_dataset=val_ds,\n        tokenizer=tokenizer,\n        data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n    )\n\n    trainer.train()\n    suffix = \"_aeda\" if use_aeda else \"\"\n    model.save_pretrained(f\"{save_name}_sexism_classifier{suffix}\")\n    tokenizer.save_pretrained(f\"{save_name}_sexism_classifier{suffix}\")\n    print(f\" Model saved to {save_name}_sexism_classifier{suffix}\")\n\n# === Run Pipeline ===\nprepare_gold_dataset(\n    clean_path=\"/kaggle/input/translated/EXIST2025_training_translated_en.json\",\n    gold_path=\"/kaggle/input/gold-hard/EXIST2025_training_task1_1_gold_hard.json\",\n    output_path=\"EXIST2025_training_with_gold.json\"\n)\n\nprepare_gold_dataset(\n    clean_path=\"/kaggle/input/dev-tanslated/EXIST2025_dev_translated_en.json\",\n    gold_path=\"/kaggle/input/gold-hard-dev/EXIST2025_dev_task1_1_gold_hard (1).json\",\n    output_path=\"EXIST2025_dev_with_gold.json\"\n)\n\nmodel_list = [\"distilroberta-base\", \"bert-base-uncased\", \"roberta-base\"]\n\nfor model_checkpoint in model_list:\n    save_name = model_checkpoint.replace(\"/\", \"-\")\n    train_model(\n        train_json_path=\"EXIST2025_training_with_gold.json\",\n        val_json_path=\"EXIST2025_dev_with_gold.json\",\n        model_checkpoint=model_checkpoint,\n        save_name=save_name,\n        use_aeda=True\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:05:32.264995Z","iopub.execute_input":"2025-05-10T14:05:32.265298Z","iopub.status.idle":"2025-05-10T14:30:35.792947Z","shell.execute_reply.started":"2025-05-10T14:05:32.265274Z","shell.execute_reply":"2025-05-10T14:30:35.791818Z"}},"outputs":[{"name":"stdout","text":" Gold-labeled dataset saved to EXIST2025_training_with_gold.json\n Gold-labeled dataset saved to EXIST2025_dev_with_gold.json\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d6be36d78bd4090bcc641b96ad1b5ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"925a48b67a5b4ac0a411b7504be088ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"636218bfff1a47c4945bbdc1df1eacf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6d36e28dce4480da880fce7ce35977b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e54cc76fdc684fbcaf2fa472cc61447e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c08d639a4cdf42c89767b7b1c78ed56a"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5740 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f0bc369cc404cac83d128d9690f097f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/444 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02fbbc52016b467db5dbca5d556788cc"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-5-acbff6eead9a>:83: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250510_140613-jger48s0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/faisalsara124-habib-university/huggingface/runs/jger48s0' target=\"_blank\">results/distilroberta-base</a></strong> to <a href='https://wandb.ai/faisalsara124-habib-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/faisalsara124-habib-university/huggingface' target=\"_blank\">https://wandb.ai/faisalsara124-habib-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/faisalsara124-habib-university/huggingface/runs/jger48s0' target=\"_blank\">https://wandb.ai/faisalsara124-habib-university/huggingface/runs/jger48s0</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1077' max='1077' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1077/1077 05:03, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.348000</td>\n      <td>0.405793</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.199300</td>\n      <td>0.585740</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.066000</td>\n      <td>0.785042</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":" Model saved to distilroberta-base_sexism_classifier_aeda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2fb717d3aac4ee1b4c1c509282cc1fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4d5afad800f442b8b86a9ab5fb02713"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35135dd91e2f4e35bb0b2bf826722a4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e14e730e5054e3fafaf9784c15ddf07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27597220601f46b0834fb0d2dd0e0d4b"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5740 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3000b79ddb6e45668c3736e7d778254b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/444 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"070d224acd2f4cf0b2e9fc6c55b89aa2"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-5-acbff6eead9a>:83: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1077' max='1077' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1077/1077 09:17, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.319600</td>\n      <td>0.353956</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.095200</td>\n      <td>0.760569</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.015800</td>\n      <td>0.819183</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":" Model saved to bert-base-uncased_sexism_classifier_aeda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5ddd576266147a88269d08432f71b4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54ecf1aa98034051882fbde820736b6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3da41335fd1f4ead9a1999b1fae373cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38a893798d894e1cb2b5ecd79307d782"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b3b12ad3306407f902e8c3847d09665"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c8c5dc076664cbeb39d34bc7ff7def5"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5740 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c0d92ff411d4a7baf36792e0b29f2d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/444 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"245a3e4510204672ab7f332b2d6dde0e"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-5-acbff6eead9a>:83: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1077' max='1077' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1077/1077 09:33, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.375700</td>\n      <td>0.375355</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.223000</td>\n      <td>0.505997</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.081600</td>\n      <td>0.658462</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":" Model saved to roberta-base_sexism_classifier_aeda\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport pandas as pd\nimport json\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\nfrom scipy.optimize import minimize\n\n# === Load Gold-Labeled Dev Set ===\ndf = pd.read_json(\"EXIST2025_dev_with_gold.json\").T\ndf = df[df['lang'] == 'en']\ndf = df.dropna(subset=['label'])\ndf['label'] = df['label'].astype(int)\ndf = df[['id_EXIST', 'tweet', 'label']].rename(columns={'id_EXIST': 'id', 'tweet': 'text'})\n\nprint(f\" Total gold-labeled dev tweets: {len(df)}\")\n\n# === Ensemble models ===\nmodel_paths = [\n    \"distilroberta-base_sexism_classifier_aeda\",\n    \"bert-base-uncased_sexism_classifier_aeda\",\n    \"roberta-base_sexism_classifier_aeda\"\n]\n\nall_model_probs = []\n\n# === Collect probabilities from each model ===\nfor path in model_paths:\n    tokenizer = AutoTokenizer.from_pretrained(path)\n    model = AutoModelForSequenceClassification.from_pretrained(path)\n    model.eval()\n\n    model_probs = []\n    for text in df['text']:\n        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n        with torch.no_grad():\n            logits = model(**inputs).logits\n            probs = torch.softmax(logits, dim=1).squeeze(0).cpu().numpy()\n            model_probs.append(probs)\n\n    all_model_probs.append(np.array(model_probs))  # shape: [num_samples, 2]\n\nall_model_probs = np.array(all_model_probs)  # shape: [num_models, num_samples, 2]\ntrue_labels = np.array(df['label'].tolist())\n\n# === Optimize weights using F1 score ===\ndef f1_objective(weights):\n    weighted_probs = np.average(all_model_probs, axis=0, weights=weights)\n    preds = np.argmax(weighted_probs, axis=1)\n    f1 = precision_recall_fscore_support(true_labels, preds, average='binary')[2]\n    return 1 - f1  # because we want to maximize F1\n\n# Constraint: sum of weights = 1\nconstraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n\n# Bounds: all weights between 0 and 1\nbounds = [(0, 1)] * len(model_paths)\n\n# Initial guess: equal weights\ninitial_weights = np.ones(len(model_paths)) / len(model_paths)\n\n# Run optimization\nresult = minimize(f1_objective, initial_weights, bounds=bounds, constraints=constraints)\nbest_weights = result.x\nprint(f\"\\n Optimal Weights Found: {best_weights.round(3)}\")\n\n# === Weighted soft voting with optimized weights ===\nweighted_probs = np.average(all_model_probs, axis=0, weights=best_weights)\nensemble_preds = np.argmax(weighted_probs, axis=1)\n\n# === Evaluation ===\nprecision, recall, f1, _ = precision_recall_fscore_support(true_labels, ensemble_preds, average='binary')\naccuracy = accuracy_score(true_labels, ensemble_preds)\n\nprint(\"\\n Ensemble Evaluation on GOLD Dev Set (Soft Voting, Optimized Weights):\")\nprint(f\"Accuracy:  {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall:    {recall:.4f}\")\nprint(f\"F1 Score:  {f1:.4f}\")\nprint(\"\\n Classification Report:\")\nprint(classification_report(true_labels, ensemble_preds, target_names=[\"non-sexist\", \"sexist\"]))\n\n# === Save Output for PyEvALL ===\noutput = []\nfor tweet_id, pred in zip(df['id'], ensemble_preds):\n    output.append({\n        \"test_case\": \"EXIST2025\",\n        \"id\": str(tweet_id),\n        \"value\": \"YES\" if pred == 1 else \"NO\"\n    })\n\noutput_sorted = sorted(output, key=lambda x: int(x[\"id\"]))\n\nwith open(\"ensemble_predictions_output.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(output_sorted, f, indent=2, ensure_ascii=False)\n\nprint(\"Predictions saved for PyEvALL evaluation\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:37:26.711245Z","iopub.execute_input":"2025-05-10T14:37:26.711572Z","iopub.status.idle":"2025-05-10T14:38:59.953852Z","shell.execute_reply.started":"2025-05-10T14:37:26.711544Z","shell.execute_reply":"2025-05-10T14:38:59.952952Z"}},"outputs":[{"name":"stdout","text":" Total gold-labeled dev tweets: 444\n\n Optimal Weights Found: [0.333 0.333 0.333]\n\n Ensemble Evaluation on GOLD Dev Set (Soft Voting, Optimized Weights):\nAccuracy:  0.8649\nPrecision: 0.8350\nRecall:    0.8608\nF1 Score:  0.8477\n\n Classification Report:\n              precision    recall  f1-score   support\n\n  non-sexist       0.89      0.87      0.88       250\n      sexist       0.83      0.86      0.85       194\n\n    accuracy                           0.86       444\n   macro avg       0.86      0.86      0.86       444\nweighted avg       0.87      0.86      0.87       444\n\nPredictions saved for PyEvALL evaluation\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from pyevall.evaluation import PyEvALLEvaluation\nfrom pyevall.utils.utils import PyEvALLUtils\n\npredictions = \"ensemble_predictions_output.json\"\ngold = \"/kaggle/input/gold-hard-dev/EXIST2025_dev_task1_1_gold_hard (1).json\"\n\n# Initialize evaluator\nevaluator = PyEvALLEvaluation()\n\n# Set parameters\nparams = {\n    PyEvALLUtils.PARAM_REPORT: PyEvALLUtils.PARAM_OPTION_REPORT_EMBEDDED\n}\n\n# Choose metrics (ICM for hard labels)\nmetrics = [\"ICM\", \"ICMNorm\", \"FMeasure\"]  # You can also try ICMSoft for soft scores\n\n# Run evaluation\nreport = evaluator.evaluate(predictions, gold, metrics, **params)\nreport.print_report()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:01:07.136166Z","iopub.execute_input":"2025-05-10T15:01:07.136472Z","iopub.status.idle":"2025-05-10T15:01:08.745759Z","shell.execute_reply.started":"2025-05-10T15:01:07.136441Z","shell.execute_reply":"2025-05-10T15:01:08.745089Z"}},"outputs":[{"name":"stdout","text":"2025-05-10 15:01:07,143 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM', 'ICMNorm', 'FMeasure']\n2025-05-10 15:01:07,260 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-10 15:01:07,654 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Normalized evaluation method\n2025-05-10 15:01:07,657 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-10 15:01:08,010 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-10 15:01:08,422 - pyevall.metrics.metrics - INFO -             evaluate() - Executing fmeasure evaluation method\n{\n  \"metrics\": {\n    \"ICM\": {\n      \"name\": \"Information Contrast model\",\n      \"acronym\": \"ICM\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": -0.24534068637914827\n        }],\n        \"average_per_test_case\": -0.24534068637914827\n      }\n    },\n    \"ICMNorm\": {\n      \"name\": \"Normalized Information Contrast Model\",\n      \"acronym\": \"ICM-Norm\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 0.37727119562999006\n        }],\n        \"average_per_test_case\": 0.37727119562999006\n      }\n    },\n    \"FMeasure\": {\n      \"name\": \"F-Measure\",\n      \"acronym\": \"F1\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"classes\": {\n            \"YES\": 0.5099236641221373,\n            \"NO\": 0.6002766251728907\n          },\n          \"average\": 0.555100144647514\n        }],\n        \"average_per_test_case\": 0.555100144647514\n      }\n    }\n  },\n  \"files\": {\n    \"ensemble_predictions_output.json\": {\n      \"name\": \"ensemble_predictions_output.json\",\n      \"status\": \"OK\",\n      \"gold\": false,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: ensemble_predictions_output.json.\",\n      \"errors\": {}\n    },\n    \"EXIST2025_dev_task1_1_gold_hard (1).json\": {\n      \"name\": \"EXIST2025_dev_task1_1_gold_hard (1).json\",\n      \"status\": \"OK\",\n      \"gold\": true,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_task1_1_gold_hard (1).json.\",\n      \"errors\": {}\n    }\n  }\n}\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"#Training aeda+ensemble for spanish (Custom training and dev sets used instead of splitting)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport numpy as np\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding\n)\nimport random\n\n# === AEDA Function ===\nPUNCTUATIONS = ['.', ',', '!', '?', ';', ':']\n\ndef aeda(sentence, punc_ratio=0.3, max_insert=3):\n    words = sentence.split()\n    n = len(words)\n    num_puncs = min(max_insert, max(1, int(punc_ratio * n)))\n\n    insert_positions = random.sample(range(n), num_puncs)\n    for pos in insert_positions:\n        punct = random.choice(PUNCTUATIONS)\n        words[pos] = words[pos] + punct\n    return ' '.join(words)\n\n# === Gold Label Merger ===\ndef prepare_gold_labeled_data(cleaned_path, gold_path, output_path, lang='es'):\n    with open(cleaned_path, \"r\", encoding=\"utf-8\") as f:\n        cleaned_data = json.load(f)\n    with open(gold_path, \"r\", encoding=\"utf-8\") as f:\n        gold_labels = json.load(f)\n\n    label_map = {entry[\"id\"]: 1 if entry[\"value\"] == \"YES\" else 0 for entry in gold_labels}\n    updated_data = {}\n\n    for tweet_id, tweet in cleaned_data.items():\n        tweet = tweet.copy()\n        id_ = tweet.get(\"id_EXIST\")\n        if tweet.get(\"lang\") == lang and id_ in label_map:\n            tweet[\"label\"] = label_map[id_]\n            updated_data[tweet_id] = tweet\n\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(updated_data, f, indent=2, ensure_ascii=False)\n    print(f\" Gold-labeled {lang.upper()} data saved to: {output_path}\")\n\n# === Load & Tokenize Spanish Dataset ===\ndef load_and_tokenize_dataset(json_path, tokenizer, apply_aeda=False):\n    df = pd.read_json(json_path).T\n    df = df[df['lang'] == 'es']\n    df = df.dropna(subset=['label'])\n    df = df[['tweet', 'label']].rename(columns={'tweet': 'text'})\n\n    if apply_aeda:\n        augmented_rows = []\n        for _, row in df.iterrows():\n            aug_text = aeda(row['text'])\n            augmented_rows.append({'text': aug_text, 'label': row['label']})\n        aug_df = pd.DataFrame(augmented_rows)\n        df = pd.concat([df, aug_df], ignore_index=True)\n        df = df.sample(frac=1, random_state=42)\n\n    dataset = Dataset.from_pandas(df)\n    return dataset.map(lambda x: tokenizer(x['text'], truncation=True, padding='max_length', max_length=256), batched=True)\n\n# === Train Spanish Model ===\ndef train_model(train_json_path, val_json_path, model_checkpoint, save_name, use_aeda=True):\n    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n\n    train_ds = load_and_tokenize_dataset(train_json_path, tokenizer, apply_aeda=use_aeda)\n    val_ds = load_and_tokenize_dataset(val_json_path, tokenizer, apply_aeda=False)\n\n    training_args = TrainingArguments(\n        output_dir=f\"results/{save_name}_es\",\n        num_train_epochs=3,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=8,\n        warmup_steps=100,\n        weight_decay=0.01,\n        logging_dir=f\"logs/{save_name}_es\",\n        logging_steps=50,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        save_total_limit=1,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_ds,\n        eval_dataset=val_ds,\n        tokenizer=tokenizer,\n        data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n    )\n\n    trainer.train()\n\n    model.save_pretrained(f\"{save_name}_es_sexism_classifier_aeda\")\n    tokenizer.save_pretrained(f\"{save_name}_es_sexism_classifier_aeda\")\n    print(f\" Model saved: {save_name}_es_sexism_classifier_aeda\")\n\n# === Run Everything ===\n\n# Step 1: Prepare gold-labeled train and dev sets\nprepare_gold_labeled_data(\n    \"/kaggle/input/translated/EXIST2025_training_translated_es.json\",\n    \"/kaggle/input/gold-hard/EXIST2025_training_task1_1_gold_hard.json\",\n    \"EXIST2025_training_with_gold_es.json\"\n)\n\nprepare_gold_labeled_data(\n    \"/kaggle/input/dev-tanslated/EXIST2025_dev_translated_es.json\",\n    \"/kaggle/input/gold-hard-dev/EXIST2025_dev_task1_1_gold_hard (1).json\",\n    \"EXIST2025_dev_with_gold_es.json\"\n)\n\n# Step 2: Train models\nmodel_list = [\n    (\"PlanTL-GOB-ES/roberta-base-bne\", \"roberta-bne\"),\n    (\"dccuchile/bert-base-spanish-wwm-cased\", \"bert-spanish\"),\n    (\"xlm-roberta-base\", \"xlm-roberta-base\")\n]\n\nfor checkpoint, name in model_list:\n    train_model(\n        train_json_path=\"EXIST2025_training_with_gold_es.json\",\n        val_json_path=\"EXIST2025_dev_with_gold_es.json\",\n        model_checkpoint=checkpoint,\n        save_name=name,\n        use_aeda=True\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:11:15.886038Z","iopub.execute_input":"2025-05-10T15:11:15.886373Z","iopub.status.idle":"2025-05-10T15:45:44.350249Z","shell.execute_reply.started":"2025-05-10T15:11:15.886344Z","shell.execute_reply":"2025-05-10T15:45:44.349328Z"}},"outputs":[{"name":"stdout","text":" Gold-labeled ES data saved to: EXIST2025_training_with_gold_es.json\n Gold-labeled ES data saved to: EXIST2025_dev_with_gold_es.json\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"005b5793cd1c4251bea265f65411d636"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/851k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed2d8edc83e84aae8505e7c1d0dddfc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/509k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2f2065d488c426a8534b5ab7633ba29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.21M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac66fee4f3c241268f322859cb9d850e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/957 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55cb3e72bbdd4e97bd6ec87cae7f00ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/613 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcea49eacff1435fa9047a37204afe27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a74487a9fed43a38f4b508fc4ba251f"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at PlanTL-GOB-ES/roberta-base-bne and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6388 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7840b73999ed489e9b84698063cbe13b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94a63b2df4c24829aa9313119a0a4439"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-5-64df9d378444>:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250510_151157-iz4b6lgk</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/faisalsara124-habib-university/huggingface/runs/iz4b6lgk' target=\"_blank\">results/roberta-bne_es</a></strong> to <a href='https://wandb.ai/faisalsara124-habib-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/faisalsara124-habib-university/huggingface' target=\"_blank\">https://wandb.ai/faisalsara124-habib-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/faisalsara124-habib-university/huggingface/runs/iz4b6lgk' target=\"_blank\">https://wandb.ai/faisalsara124-habib-university/huggingface/runs/iz4b6lgk</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1200/1200 09:37, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.252600</td>\n      <td>0.456768</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.062700</td>\n      <td>0.700236</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.000700</td>\n      <td>0.863545</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":" Model saved: roberta-bne_es_sexism_classifier_aeda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/364 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5ac7613ab8945cc9e2024276df16a70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/648 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17a673aa880d4c91ab6f76eaafa7f1f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/242k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"461224d1f3bf4cb59496d3b0bfdb9ef1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/480k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd401e0a47e44e59b8eb689082fb2bb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/134 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbf2c644d11447d78d5ecd07ec64f23e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c57487f99304692b7c2cd8bc8981fd8"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6388 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da9a49a3b86445e59081b5a8cfe9d3a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2231759febbd456a9bd9b631e1cf6546"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-5-64df9d378444>:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1200/1200 09:48, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.260000</td>\n      <td>0.586947</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.065800</td>\n      <td>0.976499</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.004000</td>\n      <td>1.075608</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":" Model saved: bert-spanish_es_sexism_classifier_aeda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"851becaa6afc44a0b301fbec14a45cce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56dad78ff3cf4198b8c89b783e8fe42c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74a7cf25e69846d1b545e58f407f8e8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcab0f54c3114aa68a6db17c7d08c0ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76d284602c2640fc90380b967fae0a90"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6388 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e41f8b3e830436d8f212d380763c756"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96a5b7d71dc24345b0b69dc3b638967e"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-5-64df9d378444>:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1200/1200 13:44, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.509700</td>\n      <td>0.544351</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.328600</td>\n      <td>0.471760</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.233600</td>\n      <td>0.609794</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":" Model saved: xlm-roberta-base_es_sexism_classifier_aeda\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport pandas as pd\nimport json\nimport numpy as np\nfrom sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score, classification_report\nfrom itertools import product\nfrom tqdm import tqdm\n\n# === Load gold-labeled Spanish dev set ===\ndf = pd.read_json(\"EXIST2025_dev_with_gold_es.json\").T\ndf = df[df['lang'] == 'es']\ndf = df.dropna(subset=['label'])\ndf['label'] = df['label'].astype(int)\ndf = df[['id_EXIST', 'tweet', 'label']].rename(columns={'id_EXIST': 'id', 'tweet': 'text'})\n\nprint(f\"Total Spanish dev samples with gold labels: {len(df)}\")\n\n# === Load trained Spanish models ===\nmodel_paths = [\n    \"roberta-bne_es_sexism_classifier_aeda\",\n    \"bert-spanish_es_sexism_classifier_aeda\",\n    \"xlm-roberta-base_es_sexism_classifier_aeda\"\n]\n\nmodel_labels = [\"RoBERTa-bne\", \"BERT-Spanish\", \"XLM-RoBERTa\"]\n\nall_model_probs = []\nfor path in model_paths:\n    tokenizer = AutoTokenizer.from_pretrained(path)\n    model = AutoModelForSequenceClassification.from_pretrained(path)\n    model.eval()\n\n    probs = []\n    for text in df['text']:\n        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n        with torch.no_grad():\n            logits = model(**inputs).logits\n            prob = torch.softmax(logits, dim=1).squeeze(0).cpu().numpy()\n            probs.append(prob)\n\n    all_model_probs.append(np.array(probs))  # shape: [num_samples, 2]\n\n# === Grid search for best weights ===\nall_model_probs = np.array(all_model_probs)  # shape: [3, num_samples, 2]\ntrue_labels = np.array(df['label'].tolist())\n\nsearch_space = np.arange(0.0, 1.1, 0.1)\nbest_f1 = 0\nbest_weights = None\nbest_preds = None\n\nprint(\"Grid searching for best weights (Spanish)...\")\nfor w1, w2, w3 in tqdm(product(search_space, repeat=3)):\n    weights = np.array([w1, w2, w3])\n    if np.isclose(weights.sum(), 1.0):\n        weighted_probs = np.average(all_model_probs, axis=0, weights=weights)\n        preds = np.argmax(weighted_probs, axis=1)\n        f1 = f1_score(true_labels, preds, average='binary')\n        if f1 > best_f1:\n            best_f1 = f1\n            best_weights = weights\n            best_preds = preds\n\n# === Print best weights and evaluation metrics ===\nprint(\"\\nBest Ensemble F1 Score (Spanish):\")\nfor label, weight in zip(model_labels, best_weights):\n    print(f\"  {label}: {weight:.2f}\")\nprint(f\"\\nF1 Score:  {best_f1:.4f}\")\n\nprecision, recall, _, _ = precision_recall_fscore_support(true_labels, best_preds, average='binary')\naccuracy = accuracy_score(true_labels, best_preds)\n\nprint(f\"Accuracy:  {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall:    {recall:.4f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(true_labels, best_preds, target_names=[\"non-sexist\", \"sexist\"]))\n\n# === Save predictions for PyEvALL ===\noutput = []\nfor tweet_id, pred in zip(df['id'], best_preds):\n    output.append({\n        \"test_case\": \"EXIST2025\",\n        \"id\": str(tweet_id),\n        \"value\": \"YES\" if pred == 1 else \"NO\"\n    })\n\noutput_sorted = sorted(output, key=lambda x: int(x[\"id\"]))\n\nwith open(\"spanish_ensemble_predictions.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(output_sorted, f, indent=2, ensure_ascii=False)\n\nprint(\"Predictions saved for PyEvALL: 'spanish_ensemble_predictions.json'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:51:30.727822Z","iopub.execute_input":"2025-05-10T15:51:30.728076Z","iopub.status.idle":"2025-05-10T15:53:42.905341Z","shell.execute_reply.started":"2025-05-10T15:51:30.728055Z","shell.execute_reply":"2025-05-10T15:53:42.904570Z"}},"outputs":[{"name":"stdout","text":"Total Spanish dev samples with gold labels: 490\nGrid searching for best weights (Spanish)...\n","output_type":"stream"},{"name":"stderr","text":"1331it [00:00, 8892.22it/s]","output_type":"stream"},{"name":"stdout","text":"\nBest Ensemble F1 Score (Spanish):\n  RoBERTa-bne: 0.30\n  BERT-Spanish: 0.40\n  XLM-RoBERTa: 0.30\n\nF1 Score:  0.8642\nAccuracy:  0.8551\nPrecision: 0.8626\nRecall:    0.8659\n\nClassification Report:\n              precision    recall  f1-score   support\n\n  non-sexist       0.85      0.84      0.84       229\n      sexist       0.86      0.87      0.86       261\n\n    accuracy                           0.86       490\n   macro avg       0.85      0.85      0.85       490\nweighted avg       0.86      0.86      0.86       490\n\nPredictions saved for PyEvALL: 'spanish_ensemble_predictions.json'\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from pyevall.evaluation import PyEvALLEvaluation\nfrom pyevall.utils.utils import PyEvALLUtils\n\npredictions = \"spanish_ensemble_predictions.json\"\ngold = \"/kaggle/input/gold-hard-dev/EXIST2025_dev_task1_1_gold_hard (1).json\"\n\n# Initialize evaluator\nevaluator = PyEvALLEvaluation()\n\n# Set parameters\nparams = {\n    PyEvALLUtils.PARAM_REPORT: PyEvALLUtils.PARAM_OPTION_REPORT_EMBEDDED\n}\n\n# Choose metrics (ICM for hard labels)\nmetrics = [\"ICM\", \"ICMNorm\", \"FMeasure\"]  # You can also try ICMSoft for soft scores\n\n# Run evaluation\nreport = evaluator.evaluate(predictions, gold, metrics, **params)\nreport.print_report()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:54:21.316844Z","iopub.execute_input":"2025-05-10T15:54:21.317134Z","iopub.status.idle":"2025-05-10T15:54:23.148905Z","shell.execute_reply.started":"2025-05-10T15:54:21.317113Z","shell.execute_reply":"2025-05-10T15:54:23.148152Z"}},"outputs":[{"name":"stdout","text":"2025-05-10 15:54:21,381 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM', 'ICMNorm', 'FMeasure']\n2025-05-10 15:54:21,509 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-10 15:54:21,882 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Normalized evaluation method\n2025-05-10 15:54:21,885 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-10 15:54:22,267 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-10 15:54:22,727 - pyevall.metrics.metrics - INFO -             evaluate() - Executing fmeasure evaluation method\ncargado 29\n{\n  \"metrics\": {\n    \"ICM\": {\n      \"name\": \"Information Contrast model\",\n      \"acronym\": \"ICM\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": -0.17535276938857217\n        }],\n        \"average_per_test_case\": -0.17535276938857217\n      }\n    },\n    \"ICMNorm\": {\n      \"name\": \"Normalized Information Contrast Model\",\n      \"acronym\": \"ICM-Norm\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 0.41228183124599505\n        }],\n        \"average_per_test_case\": 0.41228183124599505\n      }\n    },\n    \"FMeasure\": {\n      \"name\": \"F-Measure\",\n      \"acronym\": \"F1\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"classes\": {\n            \"YES\": 0.6304044630404463,\n            \"NO\": 0.5459688826025461\n          },\n          \"average\": 0.5881866728214962\n        }],\n        \"average_per_test_case\": 0.5881866728214962\n      }\n    }\n  },\n  \"files\": {\n    \"spanish_ensemble_predictions.json\": {\n      \"name\": \"spanish_ensemble_predictions.json\",\n      \"status\": \"OK\",\n      \"gold\": false,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: spanish_ensemble_predictions.json.\",\n      \"errors\": {}\n    },\n    \"EXIST2025_dev_task1_1_gold_hard (1).json\": {\n      \"name\": \"EXIST2025_dev_task1_1_gold_hard (1).json\",\n      \"status\": \"OK\",\n      \"gold\": true,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_task1_1_gold_hard (1).json.\",\n      \"errors\": {}\n    }\n  }\n}\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import json\n\n# === Load English & Spanish predictions ===\nwith open(\"/kaggle/input/final-predictions/ensemble_predictions_output_final.json\", \"r\", encoding=\"utf-8\") as f:\n    en_preds = json.load(f)\n\nwith open(\"/kaggle/input/final-predictions/spanish_ensemble_predictions_final.json\", \"r\", encoding=\"utf-8\") as f:\n    es_preds = json.load(f)\n\n# === Merge and sort by ID ===\ncombined_preds = en_preds + es_preds\ncombined_preds_sorted = sorted(combined_preds, key=lambda x: int(x[\"id\"]))\n\n# === Save to a single file ===\nwith open(\"combined_ensemble_predictions_aeda.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(combined_preds_sorted, f, indent=2, ensure_ascii=False)\n\nprint(\" Combined predictions saved as 'combined_ensemble_predictions.json'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T17:33:10.133425Z","iopub.execute_input":"2025-05-10T17:33:10.133734Z","iopub.status.idle":"2025-05-10T17:33:10.149259Z","shell.execute_reply.started":"2025-05-10T17:33:10.133706Z","shell.execute_reply":"2025-05-10T17:33:10.148330Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":" Combined predictions saved as 'combined_ensemble_predictions.json'\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#Final Results after all the modifications","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyevall.evaluation import PyEvALLEvaluation\nfrom pyevall.utils.utils import PyEvALLUtils\n\nevaluator = PyEvALLEvaluation()\nparams = {PyEvALLUtils.PARAM_REPORT: PyEvALLUtils.PARAM_OPTION_REPORT_EMBEDDED}\nmetrics = [\"ICM\", \"ICMNorm\", \"FMeasure\"]\n\nreport = evaluator.evaluate(\n    \"combined_ensemble_predictions_aeda.json\",\n    \"/kaggle/input/gold-hard-dev/EXIST2025_dev_task1_1_gold_hard (1).json\",\n    metrics,\n    **params\n)\nreport.print_report()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:57:36.497116Z","iopub.execute_input":"2025-05-10T15:57:36.497423Z","iopub.status.idle":"2025-05-10T15:57:38.235298Z","shell.execute_reply.started":"2025-05-10T15:57:36.497379Z","shell.execute_reply":"2025-05-10T15:57:38.234520Z"}},"outputs":[{"name":"stdout","text":"2025-05-10 15:57:36,502 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM', 'ICMNorm', 'FMeasure']\n2025-05-10 15:57:36,613 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-10 15:57:37,070 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Normalized evaluation method\n2025-05-10 15:57:37,074 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-10 15:57:37,459 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-10 15:57:37,839 - pyevall.metrics.metrics - INFO -             evaluate() - Executing fmeasure evaluation method\n{\n  \"metrics\": {\n    \"ICM\": {\n      \"name\": \"Information Contrast model\",\n      \"acronym\": \"ICM\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 0.5788301998179711\n        }],\n        \"average_per_test_case\": 0.5788301998179711\n      }\n    },\n    \"ICMNorm\": {\n      \"name\": \"Normalized Information Contrast Model\",\n      \"acronym\": \"ICM-Norm\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 0.7895530268759825\n        }],\n        \"average_per_test_case\": 0.7895530268759825\n      }\n    },\n    \"FMeasure\": {\n      \"name\": \"F-Measure\",\n      \"acronym\": \"F1\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"classes\": {\n            \"YES\": 0.8571428571428572,\n            \"NO\": 0.8622502628811778\n          },\n          \"average\": 0.8596965600120174\n        }],\n        \"average_per_test_case\": 0.8596965600120174\n      }\n    }\n  },\n  \"files\": {\n    \"combined_ensemble_predictions_aeda.json\": {\n      \"name\": \"combined_ensemble_predictions_aeda.json\",\n      \"status\": \"OK\",\n      \"gold\": false,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: combined_ensemble_predictions_aeda.json.\",\n      \"errors\": {}\n    },\n    \"EXIST2025_dev_task1_1_gold_hard (1).json\": {\n      \"name\": \"EXIST2025_dev_task1_1_gold_hard (1).json\",\n      \"status\": \"OK\",\n      \"gold\": true,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_task1_1_gold_hard (1).json.\",\n      \"errors\": {}\n    }\n  }\n}\n","output_type":"stream"}],"execution_count":12}]}