{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11769919,"sourceType":"datasetVersion","datasetId":7203709}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets evaluate accelerate sentencepiece","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T16:21:55.907670Z","iopub.execute_input":"2025-05-11T16:21:55.907941Z","iopub.status.idle":"2025-05-11T16:21:59.073197Z","shell.execute_reply.started":"2025-05-11T16:21:55.907922Z","shell.execute_reply":"2025-05-11T16:21:59.072532Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"import os\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport pandas as pd\nimport numpy as np\nimport datasets\nfrom tabulate import tabulate\nimport nltk\nfrom datetime import datetime","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T16:22:02.296039Z","iopub.execute_input":"2025-05-11T16:22:02.296296Z","iopub.status.idle":"2025-05-11T16:22:02.302643Z","shell.execute_reply.started":"2025-05-11T16:22:02.296275Z","shell.execute_reply":"2025-05-11T16:22:02.301953Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"label_classes = ['NO', 'DIRECT','REPORTED','JUDGEMENTAL']\nclass2id = {'NO': 0, 'DIRECT': 1, 'REPORTED': 2, 'JUDGEMENTAL': 3}\nid2class = {v: k for k, v in class2id.items()}\nid2label = id2class\nlabel_mapping = class2id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T16:22:02.668932Z","iopub.execute_input":"2025-05-11T16:22:02.669215Z","iopub.status.idle":"2025-05-11T16:22:02.674829Z","shell.execute_reply.started":"2025-05-11T16:22:02.669194Z","shell.execute_reply":"2025-05-11T16:22:02.674085Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"\n# ! pip install datasets\n# ! pip install sentencepiece\n# ! pip install rouge_score\n! pip install wandb\nimport wandb\n# wandb login}\nwandb.login(key=\"6930a5bf7436e98e8f1d44766c7b999ee9621ba9\")\n# wandb.init(project=\"LLM\", entity=\"sa07424-habib-university\", settings=wandb.Settings(init_timeout=200))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T16:22:04.122798Z","iopub.execute_input":"2025-05-11T16:22:04.123260Z","iopub.status.idle":"2025-05-11T16:22:06.996318Z","shell.execute_reply.started":"2025-05-11T16:22:04.123237Z","shell.execute_reply":"2025-05-11T16:22:06.995656Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.6)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\nRequirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.21.0)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (69.5.1)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.1)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":65},{"cell_type":"markdown","source":"### With AEDA","metadata":{}},{"cell_type":"code","source":"import json\nimport torch\nimport random\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer\nfrom torch.utils.data import Dataset\nfrom collections import defaultdict\n \n# AEDA helper\nPUNCTUATIONS = ['.', ',', '!', '?', ';', ':']\ndef aeda(sentence, num_insertions=3):\n    words = sentence.split()\n    if not words:\n        return sentence\n    new_words = words.copy()\n    for _ in range(num_insertions):\n        insert_pos = random.randint(0, len(new_words))\n        punct = random.choice(PUNCTUATIONS)\n        new_words.insert(insert_pos, punct)\n    return ' '.join(new_words)\n    \n# === Load Tweets ===\nwith open(\"/kaggle/input/existdatasets/EXIST2025_training_translated_en.json\", \"r\", encoding=\"utf-8\") as f:\n    data_en = json.load(f)\n\nwith open(\"/kaggle/input/existdatasets/EXIST2025_training_translated_es.json\", \"r\", encoding=\"utf-8\") as f:\n    data_es = json.load(f)\n\n# === Loading gold_hard_train === (Using hard labels for Task 2 - multi-class classification)\nwith open(\"/kaggle/input/existdatasets/EXIST2025_training_task1_2_gold_hard.json\", \"r\", encoding=\"utf-8\") as f:\n    gold_hard = json.load(f)\n\n# Convert gold_hard to a dict for fast access\ngold_hard_dict = {entry[\"id\"]: entry[\"value\"] for entry in gold_hard}\n\n# Define the label classes\nCORRECT_LABELS = label_classes  # Make sure you define the correct label classes\nNUM_CLASSES = len(CORRECT_LABELS)  # This should be the number of classes\n\n# === Count Label Distribution ===\nlabel_counts = defaultdict(int)\nfor hard in gold_hard_dict.values():\n    label_counts[hard] += 1\n\n# === Identify underrepresented labels (you can tune this threshold) ===\navg_count = np.mean(list(label_counts.values()))\nunderrepresented_labels = [label for label, count in label_counts.items() if count < 1294]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T17:13:01.033796Z","iopub.execute_input":"2025-05-11T17:13:01.034493Z","iopub.status.idle":"2025-05-11T17:13:02.340692Z","shell.execute_reply.started":"2025-05-11T17:13:01.034469Z","shell.execute_reply":"2025-05-11T17:13:02.340144Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"print(label_counts)\nprint(avg_count)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T17:11:26.374862Z","iopub.execute_input":"2025-05-11T17:11:26.375383Z","iopub.status.idle":"2025-05-11T17:11:26.381163Z","shell.execute_reply.started":"2025-05-11T17:11:26.375364Z","shell.execute_reply":"2025-05-11T17:11:26.380573Z"}},"outputs":[{"name":"stdout","text":"defaultdict(<class 'int'>, {'REPORTED': 459, 'NO': 3367, 'DIRECT': 1294, 'JUDGEMENTAL': 376})\n1374.0\n","output_type":"stream"}],"execution_count":86},{"cell_type":"code","source":"# === Process Tweets with Corresponding Hard Labels ===\ndef process_data_with_hard_labels(data, augment=True, augment_n=2):\n    tweets = []\n    labels = []\n    ids = []\n\n    for entry in data.values():\n        tweet_id = entry[\"id_EXIST\"]\n        tweet = entry[\"tweet\"]\n\n        if tweet_id not in gold_hard_dict:\n            continue  # Skip if hard label not found\n\n        # Get the hard label from the dictionary\n        hard_label = gold_hard_dict[tweet_id]\n\n        # Check if the label is in the mapping, if not, raise an error or handle it\n        if hard_label not in label_mapping:\n            raise ValueError(f\"Unexpected label '{hard_label}' for tweet ID {tweet_id}\")\n\n        # Convert the string label to the corresponding integer\n        label_int = label_mapping[hard_label]\n\n        # Tried combining tweet with annotator attributes\n        # annotator_info = {\n        #     \"country\": entry.get(\"countries_annotators\", []),\n        #     \"study_level\": entry.get(\"study_levels_annotators\", []),\n        #     \"ethnicity\": entry.get(\"ethnicities_annotators\", []),\n        #     \"age\": entry.get(\"age_annotators\", []),\n        #     \"gender\": entry.get(\"gender_annotators\", []),\n        # }\n\n        # # Flatten and format metadata into string\n        # annotator_str = \" | \".join(\n        #     f\"{key}: {', '.join(map(str, value))}\" for key, value in annotator_info.items()\n        # )\n        # full_text = f\"{tweet} [ANNOTATORS] {annotator_str}\"\n\n        tweets.append(tweet)  # input everything\n        labels.append(label_int)  # single label for multi-class classification\n        ids.append(tweet_id)\n        # Augment only if underrepresented\n        if augment and hard_label in underrepresented_labels:\n            for i in range(augment_n):\n                augmented_tweet = aeda(tweet)\n                tweets.append(augmented_tweet)\n                labels.append(label_int)\n                ids.append(f\"{tweet_id}_aug{i+1}\")\n\n    return tweets, labels, ids\n# Process both English and Spanish tweets\ntweets_en, labels_en, ids_en = process_data_with_hard_labels(data_en)\ntweets_es, labels_es, ids_es = process_data_with_hard_labels(data_es)\nprint(f\"English train set size: {len(train_dataset_en)} (with augmentation)\")\nprint(f\"Spanish train set size: {len(train_dataset_es)} (with augmentation)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T17:13:22.747947Z","iopub.execute_input":"2025-05-11T17:13:22.748521Z","iopub.status.idle":"2025-05-11T17:13:22.796747Z","shell.execute_reply.started":"2025-05-11T17:13:22.748499Z","shell.execute_reply":"2025-05-11T17:13:22.795895Z"}},"outputs":[{"name":"stdout","text":"English train set size: 7803 (with augmentation)\nSpanish train set size: 7803 (with augmentation)\n","output_type":"stream"}],"execution_count":90},{"cell_type":"code","source":"\n# === Tokenizer ===\ntokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base\")\n\n# === Custom Dataset Class ===\nclass TweetDataset(Dataset):\n    def __init__(self, texts, labels, ids, tokenizer, max_length=256):\n        self.texts = texts\n        self.labels = labels\n        self.ids = ids\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        tweet_id = self.ids[idx]\n        labels = torch.tensor(self.labels[idx], dtype=torch.long)  # Use long for multi-class\n        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n\n        return {\n            \"id\": tweet_id,\n            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n            \"labels\": labels\n        }\n\n# === Train-validation split ===\ndef get_datasets(tweets, labels, ids):\n    train_texts, val_texts, train_labels, val_labels, train_ids, val_ids = train_test_split(\n        tweets, labels, ids, test_size=0.2, random_state=42\n    )\n    train_dataset = TweetDataset(train_texts, train_labels, train_ids, tokenizer)\n    val_dataset = TweetDataset(val_texts, val_labels, val_ids, tokenizer)\n    return train_dataset, val_dataset\n\n# === Create datasets ===\ntrain_dataset_en, val_dataset_en = get_datasets(tweets_en, labels_en, ids_en)\ntrain_dataset_es, val_dataset_es = get_datasets(tweets_es, labels_es, ids_es)\n\n# === Train Model ===\ndef train_model(train_dataset, val_dataset, output_dir):\n    model = AutoModelForSequenceClassification.from_pretrained(\n            \"cardiffnlp/twitter-xlm-roberta-base\",\n            num_labels=NUM_CLASSES,  # Set number of classes for multi-class classification\n            problem_type=\"single_label_classification\"  # For multi-class, use single_label_classification\n        )\n\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        do_train=True,\n        do_eval=True,\n        num_train_epochs=4,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        logging_dir=\"./logs\",\n        logging_steps=100,\n        save_total_limit=1,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset\n    )\n    trainer.train()\n    return trainer\n\n# === Train English and Spanish models ===\ntrainer_en = train_model(train_dataset_en, val_dataset_en, output_dir=\"./results/en_xlm_twt_roberta_hard_aeda\")\ntrainer_es = train_model(train_dataset_es, val_dataset_es, output_dir=\"./results/es_xlm_twt_roberta_hard_aeda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T17:13:50.905927Z","iopub.execute_input":"2025-05-11T17:13:50.906185Z","iopub.status.idle":"2025-05-11T17:38:09.959767Z","shell.execute_reply.started":"2025-05-11T17:13:50.906166Z","shell.execute_reply":"2025-05-11T17:38:09.959216Z"}},"outputs":[{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='720' max='720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [720/720 12:03, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.891900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.664100</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.535200</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.439700</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.304500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.211400</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.145300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='720' max='720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [720/720 12:06, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.902800</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.682300</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.553900</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.466600</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.286700</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.225500</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.147300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":91},{"cell_type":"code","source":"# FacebookAI/xlm-roberta-large","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class2id = {'NO': 0,'DIRECT':1,'REPORTED':2,'JUDGEMENTAL':3}\nid2class = {v: k for k, v in class2id.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T13:24:24.265793Z","iopub.execute_input":"2025-05-11T13:24:24.266074Z","iopub.status.idle":"2025-05-11T13:24:24.284594Z","shell.execute_reply.started":"2025-05-11T13:24:24.266052Z","shell.execute_reply":"2025-05-11T13:24:24.284041Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"df = pd.read_json(\"/kaggle/input/existdatasets/EXIST2025_training_translated_en.json\")\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T13:26:20.342648Z","iopub.execute_input":"2025-05-11T13:26:20.342987Z","iopub.status.idle":"2025-05-11T13:26:22.209454Z","shell.execute_reply.started":"2025-05-11T13:26:20.342958Z","shell.execute_reply":"2025-05-11T13:26:22.208700Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                                              100001  \\\nid_EXIST                                                      100001   \nlang                                                              es   \ntweet              @TheChiflis Ignora to the other, he's a jerk.T...   \nnumber_annotators                                                  6   \nannotators         [Annotator_1, Annotator_2, Annotator_3, Annota...   \n\n                                                              100002  \\\nid_EXIST                                                      100002   \nlang                                                              es   \ntweet              @ultinameda_ If comicsgate looks like somethin...   \nnumber_annotators                                                  6   \nannotators         [Annotator_7, Annotator_8, Annotator_9, Annota...   \n\n                                                              100003  \\\nid_EXIST                                                      100003   \nlang                                                              es   \ntweet              @Steven2897 Read about Gamergate, and as that ...   \nnumber_annotators                                                  6   \nannotators         [Annotator_7, Annotator_8, Annotator_9, Annota...   \n\n                                                              100004  \\\nid_EXIST                                                      100004   \nlang                                                              es   \ntweet              @Lunariita7 A rather unfortunate social retard...   \nnumber_annotators                                                  6   \nannotators         [Annotator_13, Annotator_14, Annotator_15, Ann...   \n\n                                                              100005  \\\nid_EXIST                                                      100005   \nlang                                                              es   \ntweet              @novadragon21 @icep4ck @TvDannyZ Then as this ...   \nnumber_annotators                                                  6   \nannotators         [Annotator_19, Annotator_20, Annotator_21, Ann...   \n\n                                                              100006  \\\nid_EXIST                                                      100006   \nlang                                                              es   \ntweet              @yonkykong Aaah yes. Andrew Dobson. The one wh...   \nnumber_annotators                                                  6   \nannotators         [Annotator_25, Annotator_26, Annotator_27, Ann...   \n\n                                                              100007  \\\nid_EXIST                                                      100007   \nlang                                                              es   \ntweet              @glutamatom @JoaquinAdolphoC I was on the Game...   \nnumber_annotators                                                  6   \nannotators         [Annotator_25, Annotator_26, Annotator_27, Ann...   \n\n                                                              100008  \\\nid_EXIST                                                      100008   \nlang                                                              es   \ntweet              @BestKabest This gringa is still crying for th...   \nnumber_annotators                                                  6   \nannotators         [Annotator_25, Annotator_26, Annotator_27, Ann...   \n\n                                                              100009  \\\nid_EXIST                                                      100009   \nlang                                                              es   \ntweet              Do you know the #DECORATION #GAMER style for #...   \nnumber_annotators                                                  6   \nannotators         [Annotator_31, Annotator_32, Annotator_33, Ann...   \n\n                                                              100010  ...  \\\nid_EXIST                                                      100010  ...   \nlang                                                              es  ...   \ntweet              CES 2022 ASUS ROG Rise of Gamers Launch event ...  ...   \nnumber_annotators                                                  6  ...   \nannotators         [Annotator_37, Annotator_38, Annotator_39, Ann...  ...   \n\n                                                              203251  \\\nid_EXIST                                                      203251   \nlang                                                              en   \ntweet              \"you look like a whore\" I'm literally wearing ...   \nnumber_annotators                                                  6   \nannotators         [Annotator_473, Annotator_474, Annotator_475, ...   \n\n                                                              203252  \\\nid_EXIST                                                      203252   \nlang                                                              en   \ntweet              “You look like a whore” if you think I’m cute ...   \nnumber_annotators                                                  6   \nannotators         [Annotator_617, Annotator_618, Annotator_619, ...   \n\n                                                              203253  \\\nid_EXIST                                                      203253   \nlang                                                              en   \ntweet              Who fucking lied to you? You look like a whore...   \nnumber_annotators                                                  6   \nannotators         [Annotator_617, Annotator_618, Annotator_619, ...   \n\n                                                              203254  \\\nid_EXIST                                                      203254   \nlang                                                              en   \ntweet              @ShefVaidya Ma'am if I say that you look like ...   \nnumber_annotators                                                  6   \nannotators         [Annotator_668, Annotator_669, Annotator_670, ...   \n\n                                                              203255  \\\nid_EXIST                                                      203255   \nlang                                                              en   \ntweet              I forgot I have a m*d that changes the drachen...   \nnumber_annotators                                                  6   \nannotators         [Annotator_674, Annotator_675, Annotator_676, ...   \n\n                                                              203256  \\\nid_EXIST                                                      203256   \nlang                                                              en   \ntweet              idk why y’all bitches think having half your a...   \nnumber_annotators                                                  6   \nannotators         [Annotator_478, Annotator_479, Annotator_480, ...   \n\n                                                              203257  \\\nid_EXIST                                                      203257   \nlang                                                              en   \ntweet              This has been a part of an experiment with @Wo...   \nnumber_annotators                                                  6   \nannotators         [Annotator_668, Annotator_669, Annotator_670, ...   \n\n                                                              203258  \\\nid_EXIST                                                      203258   \nlang                                                              en   \ntweet              \"Take me already\" \"Not yet. You gotta be ready...   \nnumber_annotators                                                  6   \nannotators         [Annotator_467, Annotator_468, Annotator_469, ...   \n\n                                                              203259  \\\nid_EXIST                                                      203259   \nlang                                                              en   \ntweet              @clintneedcoffee why do you look like a whore?...   \nnumber_annotators                                                  6   \nannotators         [Annotator_674, Annotator_675, Annotator_676, ...   \n\n                                                              203260  \nid_EXIST                                                      203260  \nlang                                                              en  \ntweet              ik when mandy says “you look like a whore” i l...  \nnumber_annotators                                                  6  \nannotators         [Annotator_473, Annotator_474, Annotator_475, ...  \n\n[5 rows x 6920 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>100001</th>\n      <th>100002</th>\n      <th>100003</th>\n      <th>100004</th>\n      <th>100005</th>\n      <th>100006</th>\n      <th>100007</th>\n      <th>100008</th>\n      <th>100009</th>\n      <th>100010</th>\n      <th>...</th>\n      <th>203251</th>\n      <th>203252</th>\n      <th>203253</th>\n      <th>203254</th>\n      <th>203255</th>\n      <th>203256</th>\n      <th>203257</th>\n      <th>203258</th>\n      <th>203259</th>\n      <th>203260</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>id_EXIST</th>\n      <td>100001</td>\n      <td>100002</td>\n      <td>100003</td>\n      <td>100004</td>\n      <td>100005</td>\n      <td>100006</td>\n      <td>100007</td>\n      <td>100008</td>\n      <td>100009</td>\n      <td>100010</td>\n      <td>...</td>\n      <td>203251</td>\n      <td>203252</td>\n      <td>203253</td>\n      <td>203254</td>\n      <td>203255</td>\n      <td>203256</td>\n      <td>203257</td>\n      <td>203258</td>\n      <td>203259</td>\n      <td>203260</td>\n    </tr>\n    <tr>\n      <th>lang</th>\n      <td>es</td>\n      <td>es</td>\n      <td>es</td>\n      <td>es</td>\n      <td>es</td>\n      <td>es</td>\n      <td>es</td>\n      <td>es</td>\n      <td>es</td>\n      <td>es</td>\n      <td>...</td>\n      <td>en</td>\n      <td>en</td>\n      <td>en</td>\n      <td>en</td>\n      <td>en</td>\n      <td>en</td>\n      <td>en</td>\n      <td>en</td>\n      <td>en</td>\n      <td>en</td>\n    </tr>\n    <tr>\n      <th>tweet</th>\n      <td>@TheChiflis Ignora to the other, he's a jerk.T...</td>\n      <td>@ultinameda_ If comicsgate looks like somethin...</td>\n      <td>@Steven2897 Read about Gamergate, and as that ...</td>\n      <td>@Lunariita7 A rather unfortunate social retard...</td>\n      <td>@novadragon21 @icep4ck @TvDannyZ Then as this ...</td>\n      <td>@yonkykong Aaah yes. Andrew Dobson. The one wh...</td>\n      <td>@glutamatom @JoaquinAdolphoC I was on the Game...</td>\n      <td>@BestKabest This gringa is still crying for th...</td>\n      <td>Do you know the #DECORATION #GAMER style for #...</td>\n      <td>CES 2022 ASUS ROG Rise of Gamers Launch event ...</td>\n      <td>...</td>\n      <td>\"you look like a whore\" I'm literally wearing ...</td>\n      <td>“You look like a whore” if you think I’m cute ...</td>\n      <td>Who fucking lied to you? You look like a whore...</td>\n      <td>@ShefVaidya Ma'am if I say that you look like ...</td>\n      <td>I forgot I have a m*d that changes the drachen...</td>\n      <td>idk why y’all bitches think having half your a...</td>\n      <td>This has been a part of an experiment with @Wo...</td>\n      <td>\"Take me already\" \"Not yet. You gotta be ready...</td>\n      <td>@clintneedcoffee why do you look like a whore?...</td>\n      <td>ik when mandy says “you look like a whore” i l...</td>\n    </tr>\n    <tr>\n      <th>number_annotators</th>\n      <td>6</td>\n      <td>6</td>\n      <td>6</td>\n      <td>6</td>\n      <td>6</td>\n      <td>6</td>\n      <td>6</td>\n      <td>6</td>\n      <td>6</td>\n      <td>6</td>\n      <td>...</td>\n      <td>6</td>\n      <td>6</td>\n      <td>6</td>\n      <td>6</td>\n      <td>6</td>\n      <td>6</td>\n      <td>6</td>\n      <td>6</td>\n      <td>6</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>annotators</th>\n      <td>[Annotator_1, Annotator_2, Annotator_3, Annota...</td>\n      <td>[Annotator_7, Annotator_8, Annotator_9, Annota...</td>\n      <td>[Annotator_7, Annotator_8, Annotator_9, Annota...</td>\n      <td>[Annotator_13, Annotator_14, Annotator_15, Ann...</td>\n      <td>[Annotator_19, Annotator_20, Annotator_21, Ann...</td>\n      <td>[Annotator_25, Annotator_26, Annotator_27, Ann...</td>\n      <td>[Annotator_25, Annotator_26, Annotator_27, Ann...</td>\n      <td>[Annotator_25, Annotator_26, Annotator_27, Ann...</td>\n      <td>[Annotator_31, Annotator_32, Annotator_33, Ann...</td>\n      <td>[Annotator_37, Annotator_38, Annotator_39, Ann...</td>\n      <td>...</td>\n      <td>[Annotator_473, Annotator_474, Annotator_475, ...</td>\n      <td>[Annotator_617, Annotator_618, Annotator_619, ...</td>\n      <td>[Annotator_617, Annotator_618, Annotator_619, ...</td>\n      <td>[Annotator_668, Annotator_669, Annotator_670, ...</td>\n      <td>[Annotator_674, Annotator_675, Annotator_676, ...</td>\n      <td>[Annotator_478, Annotator_479, Annotator_480, ...</td>\n      <td>[Annotator_668, Annotator_669, Annotator_670, ...</td>\n      <td>[Annotator_467, Annotator_468, Annotator_469, ...</td>\n      <td>[Annotator_674, Annotator_675, Annotator_676, ...</td>\n      <td>[Annotator_473, Annotator_474, Annotator_475, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 6920 columns</p>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# PlanTL-GOB-ES/RoBERTalex\n# FacebookAI/xlm-roberta-base\n# distilbert/distilbert-base-uncased\n# google-bert/bert-base-multilingual-uncased\n# JonatanGk/roberta-base-bne-finetuned-hate-speech-offensive-spanish\n# FacebookAI/xlm-roberta-base\n# distilroberta-base\n# modelname = \"distilroberta-base\"\nmodelname = \"cardiffnlp/twitter-xlm-roberta-base\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T13:26:44.345640Z","iopub.execute_input":"2025-05-11T13:26:44.345817Z","iopub.status.idle":"2025-05-11T13:26:44.372116Z","shell.execute_reply.started":"2025-05-11T13:26:44.345803Z","shell.execute_reply":"2025-05-11T13:26:44.371446Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### Hard predict","metadata":{}},{"cell_type":"code","source":"import json\n\n# Load the dev dataset\nwith open(\"/kaggle/input/existdatasets/EXIST2025_dev.json\", \"r\", encoding=\"utf-8\") as f:\n    dev_data = json.load(f)\n\n# Split into English & Spanish\nenglish_dev_tweets = []\nenglish_dev_ids = []\nspanish_dev_tweets = []\nspanish_dev_ids = []\n\nfor entry in dev_data.values():\n    tweet_id = entry[\"id_EXIST\"]\n    tweet = entry[\"tweet\"]\n    lang = entry[\"lang\"]\n\n    if lang == \"en\":\n        english_dev_tweets.append(tweet)\n        english_dev_ids.append(tweet_id)\n    elif lang == \"es\":\n        spanish_dev_tweets.append(tweet)\n        spanish_dev_ids.append(tweet_id)\n\n# Debugging: Check split sizes\nprint(f\"English Dev Samples: {len(english_dev_tweets)}\")\nprint(f\"Spanish Dev Samples: {len(spanish_dev_tweets)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T15:55:03.148436Z","iopub.execute_input":"2025-05-11T15:55:03.148809Z","iopub.status.idle":"2025-05-11T15:55:03.199311Z","shell.execute_reply.started":"2025-05-11T15:55:03.148786Z","shell.execute_reply":"2025-05-11T15:55:03.198776Z"}},"outputs":[{"name":"stdout","text":"English Dev Samples: 489\nSpanish Dev Samples: 549\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# Check for None or empty tweets in Spanish data\nfor i, (tweet, tweet_id) in enumerate(zip(spanish_dev_tweets, spanish_dev_ids)):\n    if not tweet:\n        print(f\"Empty tweet at index {i}, ID: {tweet_id}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T15:31:58.122199Z","iopub.execute_input":"2025-05-11T15:31:58.122956Z","iopub.status.idle":"2025-05-11T15:31:58.127938Z","shell.execute_reply.started":"2025-05-11T15:31:58.122932Z","shell.execute_reply":"2025-05-11T15:31:58.127164Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"import os\nfrom transformers import BertForSequenceClassification\nimport os\nfrom transformers import BertForSequenceClassification\n\n# Function to get the latest checkpoint\ndef get_latest_checkpoint(directory=\"./results\"):\n    checkpoints = [d for d in os.listdir(directory) if d.startswith(\"checkpoint-\")]\n    if not checkpoints:\n        raise ValueError(f\"No checkpoints found in {directory}\")\n    latest_checkpoint = sorted(checkpoints, key=lambda x: int(x.split('-')[-1]))[-1]\n    return os.path.join(directory, latest_checkpoint)\n\n# Load the best model checkpoint for English and Spanish\nlatest_checkpoint_en = get_latest_checkpoint(\"./results/en_xlm_twt_roberta_hard_aeda\")\nlatest_checkpoint_es = get_latest_checkpoint(\"./results/es_xlm_twt_roberta_hard_aeda\")\n\nprint(f\"Using latest checkpoint for English: {latest_checkpoint_en}\")\nprint(f\"Using latest checkpoint for Spanish: {latest_checkpoint_es}\")\n\n# Load models\n# model_en = BertForSequenceClassification.from_pretrained(latest_checkpoint_en)\n# model_es = BertForSequenceClassification.from_pretrained(latest_checkpoint_es)\n#for non bert models\nmodel_en = AutoModelForSequenceClassification.from_pretrained(latest_checkpoint_en)\nmodel_es = AutoModelForSequenceClassification.from_pretrained(latest_checkpoint_es)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T18:02:29.175514Z","iopub.execute_input":"2025-05-11T18:02:29.176255Z","iopub.status.idle":"2025-05-11T18:02:29.408311Z","shell.execute_reply.started":"2025-05-11T18:02:29.176231Z","shell.execute_reply":"2025-05-11T18:02:29.407553Z"}},"outputs":[{"name":"stdout","text":"Using latest checkpoint for English: ./results/en_xlm_twt_roberta_hard_aeda/checkpoint-976\nUsing latest checkpoint for Spanish: ./results/es_xlm_twt_roberta_hard_aeda/checkpoint-976\n","output_type":"stream"}],"execution_count":98},{"cell_type":"code","source":"def predict_hard_labels(tweets, ids, model, tokenizer, label_classes, output_file):\n    # model.eval()\n    results = []\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")#faster on gpu\n    model.to(device)\n\n    for tweet, tweet_id in zip(tweets, ids):\n        # encoding = tokenizer(text=tweet, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n        encoding = tokenizer(tweet, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n        encoding = {key: val.to(device) for key, val in encoding.items()}\n\n        with torch.no_grad():\n            outputs = model(**encoding)\n\n        logits = outputs.logits.squeeze()\n        probs = torch.sigmoid(logits).cpu().numpy()\n\n        # Pick the label with the highest probability (YES or NO)\n        max_index = int(probs.argmax())\n        predicted_label = label_classes[max_index]\n\n        results.append({\n            \"test_case\": \"EXIST2025\",\n            \"id\": tweet_id,\n            \"value\": [predicted_label]  # Only one label\n        })\n\n    # Save results\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(results, f, indent=4)\n\n    print(f\"Hard label predictions saved to {output_file}\")\n\npredict_hard_labels(english_dev_tweets, english_dev_ids, model_en, tokenizer, label_classes, \"EXIST2025_dev_predictions_hard_merged_en_aeda1.json\")\npredict_hard_labels(spanish_dev_tweets, spanish_dev_ids, model_es, tokenizer, label_classes, \"EXIST2025_dev_predictions_hard_merged_es_aeda1.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T18:02:34.327689Z","iopub.execute_input":"2025-05-11T18:02:34.327966Z","iopub.status.idle":"2025-05-11T18:02:53.313682Z","shell.execute_reply.started":"2025-05-11T18:02:34.327937Z","shell.execute_reply":"2025-05-11T18:02:53.312960Z"}},"outputs":[{"name":"stdout","text":"Hard label predictions saved to EXIST2025_dev_predictions_hard_merged_en_aeda1.json\nHard label predictions saved to EXIST2025_dev_predictions_hard_merged_es_aeda1.json\n","output_type":"stream"}],"execution_count":99},{"cell_type":"code","source":"import json\n\n# Load the Spanish predictions\nwith open(\"/kaggle/working/EXIST2025_dev_predictions_hard_merged_es_aeda1.json\", \"r\", encoding=\"utf-8\") as f:\n    es_data = json.load(f)\n\n# Load the English predictions\nwith open(\"/kaggle/working/EXIST2025_dev_predictions_hard_merged_en_aeda1.json\", \"r\", encoding=\"utf-8\") as f:\n    en_data = json.load(f)\n\n# Assuming both files contain lists of predictions, merge them\nif isinstance(es_data, list) and isinstance(en_data, list):\n    merged_data = es_data + en_data\nelse:\n    raise ValueError(\"JSON structure is not a list. Ensure both files contain lists.\")\n\n# Save to a new file\noutput_filename = \"EXIST2025_dev_predictions_merged_hard_xlmroberta_aeda1.json\"\nwith open(output_filename, \"w\", encoding=\"utf-8\") as f:\n    json.dump(merged_data, f, indent=4, ensure_ascii=False)\n\nprint(f\"Merging complete! Saved to {output_filename}\")\n\nimport json\n\ndef convert_prediction_format(input_file, output_file):\n    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n        predictions = json.load(f)\n\n    converted = []\n    for entry in predictions:\n        # Convert the \"value\" list to a single string (first label only)\n        new_entry = {\n            \"test_case\": entry[\"test_case\"],\n            \"id\": entry[\"id\"],\n            \"value\": entry[\"value\"][0] if isinstance(entry[\"value\"], list) else entry[\"value\"]\n        }\n        converted.append(new_entry)\n\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(converted, f, indent=4)\n\n    print(f\"Predictions converted to gold format and saved to {output_file}\")\n\nconvert_prediction_format(\"EXIST2025_dev_predictions_merged_hard_xlmroberta_aeda1.json\", \"EXIST2025_dev_predictions_merged_hard_flat_aeda1.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T18:04:03.453967Z","iopub.execute_input":"2025-05-11T18:04:03.454239Z","iopub.status.idle":"2025-05-11T18:04:03.484732Z","shell.execute_reply.started":"2025-05-11T18:04:03.454219Z","shell.execute_reply":"2025-05-11T18:04:03.484005Z"}},"outputs":[{"name":"stdout","text":"Merging complete! Saved to EXIST2025_dev_predictions_merged_hard_xlmroberta_aeda1.json\nPredictions converted to gold format and saved to EXIST2025_dev_predictions_merged_hard_flat_aeda1.json\n","output_type":"stream"}],"execution_count":100},{"cell_type":"code","source":"!pip install pyEvall","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T16:59:11.046019Z","iopub.execute_input":"2025-05-11T16:59:11.046282Z","iopub.status.idle":"2025-05-11T16:59:13.944299Z","shell.execute_reply.started":"2025-05-11T16:59:11.046262Z","shell.execute_reply":"2025-05-11T16:59:13.943656Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: pyEvall in /usr/local/lib/python3.11/dist-packages (0.1.78)\nRequirement already satisfied: jsbeautifier==1.14.9 in /usr/local/lib/python3.11/dist-packages (from pyEvall) (1.14.9)\nRequirement already satisfied: jsonschema==4.23.0 in /usr/local/lib/python3.11/dist-packages (from pyEvall) (4.23.0)\nRequirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (from pyEvall) (1.26.4)\nRequirement already satisfied: pandas==2.2.3 in /usr/local/lib/python3.11/dist-packages (from pyEvall) (2.2.3)\nRequirement already satisfied: setuptools==69.5.1 in /usr/local/lib/python3.11/dist-packages (from pyEvall) (69.5.1)\nRequirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.11/dist-packages (from pyEvall) (0.9.0)\nRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from jsbeautifier==1.14.9->pyEvall) (1.17.0)\nRequirement already satisfied: editorconfig>=0.12.2 in /usr/local/lib/python3.11/dist-packages (from jsbeautifier==1.14.9->pyEvall) (0.17.0)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema==4.23.0->pyEvall) (25.3.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema==4.23.0->pyEvall) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema==4.23.0->pyEvall) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema==4.23.0->pyEvall) (0.22.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->pyEvall) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->pyEvall) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->pyEvall) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->pyEvall) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->pyEvall) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->pyEvall) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.3->pyEvall) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.3->pyEvall) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.3->pyEvall) (2025.2)\nRequirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema==4.23.0->pyEvall) (4.13.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.26.4->pyEvall) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.26.4->pyEvall) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy==1.26.4->pyEvall) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy==1.26.4->pyEvall) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy==1.26.4->pyEvall) (2024.2.0)\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"from pyevall.evaluation import PyEvALLEvaluation\nfrom pyevall.utils.utils import PyEvALLUtils","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T16:59:19.791030Z","iopub.execute_input":"2025-05-11T16:59:19.791320Z","iopub.status.idle":"2025-05-11T16:59:19.796780Z","shell.execute_reply.started":"2025-05-11T16:59:19.791296Z","shell.execute_reply":"2025-05-11T16:59:19.796084Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"from pyevall.evaluation import PyEvALLEvaluation\nfrom pyevall.utils.utils import PyEvALLUtils\npredictions = \"/kaggle/working/EXIST2025_dev_predictions_merged_hard_flat_aeda1.json\"         \ngold = \"/kaggle/input/existdatasets/EXIST2025_dev_task1_2_gold_hard.json\" \ntest = PyEvALLEvaluation() \nparams= dict() \nparams[PyEvALLUtils.PARAM_REPORT]= PyEvALLUtils.PARAM_OPTION_REPORT_EMBEDDED  \nmetrics=[\"ICM\", \"ICMNorm\" ,\"FMeasure\"]                  # for hard        \nreport= test.evaluate(predictions, gold, metrics, **params) \nreport.print_report()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T18:04:07.491074Z","iopub.execute_input":"2025-05-11T18:04:07.491691Z","iopub.status.idle":"2025-05-11T18:04:08.952069Z","shell.execute_reply.started":"2025-05-11T18:04:07.491664Z","shell.execute_reply":"2025-05-11T18:04:08.951290Z"}},"outputs":[{"name":"stdout","text":"2025-05-11 18:04:07,497 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM', 'ICMNorm', 'FMeasure']\n2025-05-11 18:04:07,619 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-11 18:04:07,979 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Normalized evaluation method\n2025-05-11 18:04:07,982 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-11 18:04:08,321 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-05-11 18:04:08,606 - pyevall.metrics.metrics - INFO -             evaluate() - Executing fmeasure evaluation method\n{\n  \"metrics\": {\n    \"ICM\": {\n      \"name\": \"Information Contrast model\",\n      \"acronym\": \"ICM\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": -0.20000831303959485\n        }],\n        \"average_per_test_case\": -0.20000831303959485\n      }\n    },\n    \"ICMNorm\": {\n      \"name\": \"Normalized Information Contrast Model\",\n      \"acronym\": \"ICM-Norm\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 0.4374555781655343\n        }],\n        \"average_per_test_case\": 0.4374555781655343\n      }\n    },\n    \"FMeasure\": {\n      \"name\": \"F-Measure\",\n      \"acronym\": \"F1\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"classes\": {\n            \"JUDGEMENTAL\": 0.29032258064516125,\n            \"NO\": 0.793709528214616,\n            \"REPORTED\": 0.3586206896551724,\n            \"DIRECT\": 0.6038543897216274\n          },\n          \"average\": 0.5116267970591443\n        }],\n        \"average_per_test_case\": 0.5116267970591443\n      }\n    }\n  },\n  \"files\": {\n    \"EXIST2025_dev_predictions_merged_hard_flat_aeda1.json\": {\n      \"name\": \"EXIST2025_dev_predictions_merged_hard_flat_aeda1.json\",\n      \"status\": \"OK\",\n      \"gold\": false,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_predictions_merged_hard_flat_aeda1.json.\",\n      \"errors\": {}\n    },\n    \"EXIST2025_dev_task1_2_gold_hard.json\": {\n      \"name\": \"EXIST2025_dev_task1_2_gold_hard.json\",\n      \"status\": \"OK\",\n      \"gold\": true,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_task1_2_gold_hard.json.\",\n      \"errors\": {}\n    }\n  }\n}\n","output_type":"stream"}],"execution_count":101},{"cell_type":"code","source":"import os\nimport shutil\n\ndef remove_folder_contents(folder):\n    if os.path.exists(folder):\n        for the_file in os.listdir(folder):\n            file_path = os.path.join(folder, the_file)\n            try:\n                if os.path.isfile(file_path) or os.path.islink(file_path):\n                    os.unlink(file_path)\n                elif os.path.isdir(file_path):\n                    shutil.rmtree(file_path)\n            except Exception as e:\n                print(f'Failed to delete {file_path}. Reason: {e}')\n    else:\n        print(f\"The folder {folder} does not exist.\")\n\nfolder_path = '/kaggle/working/results/'\nremove_folder_contents(folder_path)\n\n# Only try to remove the folder if it exists\nif os.path.exists(folder_path):\n    os.rmdir(folder_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T16:19:34.736402Z","iopub.execute_input":"2025-05-11T16:19:34.736700Z","iopub.status.idle":"2025-05-11T16:19:36.041173Z","shell.execute_reply.started":"2025-05-11T16:19:34.736679Z","shell.execute_reply":"2025-05-11T16:19:36.040274Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T16:21:49.886492Z","iopub.execute_input":"2025-05-11T16:21:49.886986Z","iopub.status.idle":"2025-05-11T16:21:49.892060Z","shell.execute_reply.started":"2025-05-11T16:21:49.886961Z","shell.execute_reply":"2025-05-11T16:21:49.891480Z"}},"outputs":[],"execution_count":61}]}