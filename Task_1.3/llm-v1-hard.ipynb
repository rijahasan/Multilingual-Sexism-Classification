{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11142046,"sourceType":"datasetVersion","datasetId":6950309},{"sourceId":11159551,"sourceType":"datasetVersion","datasetId":6963199}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T18:16:55.674078Z","iopub.execute_input":"2025-03-26T18:16:55.674342Z","iopub.status.idle":"2025-03-26T18:16:56.035150Z","shell.execute_reply.started":"2025-03-26T18:16:55.674317Z","shell.execute_reply":"2025-03-26T18:16:56.034288Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/exist2025/EXIST2025_training.json\n/kaggle/input/exist2025/EXIST2025_dev.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_majority_class_soft.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_minority_class_hard.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_3_minority_class_hard.json\n/kaggle/input/exist2025-all/EXIST2025_training.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_minority_class_soft.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_3_majority_class_soft.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_3_majority_class_hard.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_3_minority_class_soft.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_majority_class_hard.json\n/kaggle/input/exist2025-all/EXIST2025_dev.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_gold_hard.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_3_gold_soft.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_gold_soft.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_3_gold_hard.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import wandb\n\nwandb.login(key=\"0c5f368f1f51fd942ec7bb3a1c74efb7bdc832d6\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T18:16:56.036095Z","iopub.execute_input":"2025-03-26T18:16:56.036523Z","iopub.status.idle":"2025-03-26T18:17:04.827080Z","shell.execute_reply.started":"2025-03-26T18:16:56.036489Z","shell.execute_reply":"2025-03-26T18:17:04.826213Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmshoaibvohra\u001b[0m (\u001b[33mmshoaibvohra-habib-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import json\nimport torch\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nfrom torch.utils.data import Dataset\nfrom collections import Counter\n\n# Load the dataset\nwith open(\"/kaggle/input/exist2025/EXIST2025_training.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\n# Define correct label classes\nCORRECT_LABELS = [\n    \"IDEOLOGICAL-INEQUALITY\",\n    \"MISOGYNY-NON-SEXUAL-VIOLENCE\",\n    \"OBJECTIFICATION\",\n    \"SEXUAL-VIOLENCE\",\n    \"STEREOTYPING-DOMINANCE\",\n    \"NO\"  # Represents non-sexist tweets\n]\n\n# Assign-majority-label function (Threshold = 1)\ndef assign_majority_label(labels_list):\n    \"\"\"\n    Determines which labels are assigned based on majority voting.\n    - If a label appears more than once, it is included.\n    - If no labels pass the threshold, \"NO\" is assigned.\n    \"\"\"\n    # Flatten and replace \"-\" with \"NO\"\n    flat_labels = [label if label != \"-\" else \"NO\" for sublist in labels_list for label in sublist if label != \"UNKNOWN\"]\n\n    # Count label occurrences\n    label_counts = Counter(flat_labels)\n\n    # Select labels with more than 1 vote\n    majority_labels = [label for label, count in label_counts.items() if count > 1]\n\n    return majority_labels if majority_labels else [\"NO\"]\n\n# Extract relevant fields\ndef process_data(data, lang):\n    tweets = []\n    labels = []\n    ids = []\n\n    for entry in data.values():\n        if entry[\"lang\"] == lang:\n            tweet_id = entry[\"id_EXIST\"]\n            tweet = entry[\"tweet\"]\n            is_sexist = any(label == \"YES\" for label in entry[\"labels_task1_1\"])  # Check if at least one annotator marked it sexist\n            label = entry[\"labels_task1_3\"] if is_sexist else [[\"NO\"]]  # Non-sexist tweets get \"NO\"\n\n            # Get majority labels\n            majority_labels = assign_majority_label(label)\n\n            tweets.append(tweet)\n            labels.append(majority_labels)\n            ids.append(tweet_id)\n\n    return tweets, labels, ids\n\n# Process data for English and Spanish\nenglish_tweets, english_labels, english_ids = process_data(data, \"en\")\nspanish_tweets, spanish_labels, spanish_ids = process_data(data, \"es\")\n\n# MultiLabel Binarizer with Fixed Labels\nmlb = MultiLabelBinarizer(classes=CORRECT_LABELS)  # Force correct label order\nenglish_labels_bin = mlb.fit_transform(english_labels)\nspanish_labels_bin = mlb.transform(spanish_labels)  # Use the same binarizer\n\nlabel_classes = mlb.classes_\nprint(f\"Corrected Label Classes: {label_classes}\")  # Debugging\n\n# Tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n\n# Custom Dataset Class\nclass TweetDataset(Dataset):\n    def __init__(self, texts, labels, ids, tokenizer, max_length=256):\n        self.texts = texts\n        self.labels = labels\n        self.ids = ids\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        tweet_id = self.ids[idx]\n        labels = torch.tensor(self.labels[idx], dtype=torch.float)\n        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n\n        return {\n            \"id\": tweet_id,\n            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n            \"labels\": labels\n        }\n\n# Split into train/test\ndef get_datasets(tweets, labels, ids):\n    train_texts, val_texts, train_labels, val_labels, train_ids, val_ids = train_test_split(tweets, labels, ids, test_size=0.2, random_state=42)\n    train_dataset = TweetDataset(train_texts, train_labels, train_ids, tokenizer)\n    val_dataset = TweetDataset(val_texts, val_labels, val_ids, tokenizer)\n    return train_dataset, val_dataset\n\ntrain_dataset_en, val_dataset_en = get_datasets(english_tweets, english_labels_bin, english_ids)\ntrain_dataset_es, val_dataset_es = get_datasets(spanish_tweets, spanish_labels_bin, spanish_ids)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T18:17:16.870746Z","iopub.execute_input":"2025-03-26T18:17:16.871078Z","iopub.status.idle":"2025-03-26T18:17:43.537026Z","shell.execute_reply.started":"2025-03-26T18:17:16.871047Z","shell.execute_reply":"2025-03-26T18:17:43.536029Z"}},"outputs":[{"name":"stdout","text":"Corrected Label Classes: ['IDEOLOGICAL-INEQUALITY' 'MISOGYNY-NON-SEXUAL-VIOLENCE' 'OBJECTIFICATION'\n 'SEXUAL-VIOLENCE' 'STEREOTYPING-DOMINANCE' 'NO']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e03bc991b594ad1ac05355196b39992"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"864ac467e75a4fa897f79b13c3ddd3dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98fdf19dd97448e3ba3816f171de81cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47c84a2cd7c94fc4870c413b1a911bfc"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"\n# Define and Train Model for English\nmodel_en = BertForSequenceClassification.from_pretrained(\n    \"bert-base-multilingual-cased\", \n    num_labels=len(label_classes), \n    problem_type=\"multi_label_classification\"\n)\n\ntraining_args_en = TrainingArguments(\n    output_dir=\"./results/en\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    num_train_epochs=4,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\"\n)\n\ntrainer_en = Trainer(\n    model=model_en,\n    args=training_args_en,\n    train_dataset=train_dataset_en,\n    eval_dataset=val_dataset_en\n)\n\n# Train English Model\ntrainer_en.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T18:18:27.049029Z","iopub.execute_input":"2025-03-26T18:18:27.049756Z","iopub.status.idle":"2025-03-26T18:25:39.100322Z","shell.execute_reply.started":"2025-03-26T18:18:27.049727Z","shell.execute_reply":"2025-03-26T18:25:39.099642Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2fb6e8da40f4a55b4cc8e27ec8233cd"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250326_181836-ef2vxztf</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mshoaibvohra-habib-university/huggingface/runs/ef2vxztf' target=\"_blank\">./results/en</a></strong> to <a href='https://wandb.ai/mshoaibvohra-habib-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mshoaibvohra-habib-university/huggingface' target=\"_blank\">https://wandb.ai/mshoaibvohra-habib-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mshoaibvohra-habib-university/huggingface/runs/ef2vxztf' target=\"_blank\">https://wandb.ai/mshoaibvohra-habib-university/huggingface/runs/ef2vxztf</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='652' max='652' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [652/652 06:52, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.461900</td>\n      <td>0.447810</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.464800</td>\n      <td>0.449641</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.477800</td>\n      <td>0.448908</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.446700</td>\n      <td>0.448907</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=652, training_loss=0.47103177221274817, metrics={'train_runtime': 422.2682, 'train_samples_per_second': 24.705, 'train_steps_per_second': 1.544, 'total_flos': 1372436553203712.0, 'train_loss': 0.47103177221274817, 'epoch': 4.0})"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"\n# Define and Train Model for Spanish\nmodel_es = BertForSequenceClassification.from_pretrained(\n    \"bert-base-multilingual-cased\", \n    num_labels=len(label_classes), \n    problem_type=\"multi_label_classification\"\n)\n\ntraining_args_es = TrainingArguments(\n    output_dir=\"./results/es\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\"\n)\n\ntrainer_es = Trainer(\n    model=model_es,\n    args=training_args_es,\n    train_dataset=train_dataset_es,\n    eval_dataset=val_dataset_es\n)\n\n# Train Spanish Model\ntrainer_es.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T18:25:43.497999Z","iopub.execute_input":"2025-03-26T18:25:43.498304Z","iopub.status.idle":"2025-03-26T18:31:40.797462Z","shell.execute_reply.started":"2025-03-26T18:25:43.498283Z","shell.execute_reply":"2025-03-26T18:31:40.796608Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='549' max='549' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [549/549 05:55, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.458800</td>\n      <td>0.448043</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.443400</td>\n      <td>0.409747</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.334000</td>\n      <td>0.404565</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=549, training_loss=0.42653483206674264, metrics={'train_runtime': 356.1925, 'train_samples_per_second': 24.661, 'train_steps_per_second': 1.541, 'total_flos': 1155625257222144.0, 'train_loss': 0.42653483206674264, 'epoch': 3.0})"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"with open(\"/kaggle/input/exist2025-all/EXIST2025_dev.json\", \"r\", encoding=\"utf-8\") as f:\n    dev_data = json.load(f)\n\n# Extract tweets and IDs\ndev_tweets = [entry[\"tweet\"] for entry in dev_data.values()]\ndev_ids = [entry[\"id_EXIST\"] for entry in dev_data.values()]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T18:32:09.894314Z","iopub.execute_input":"2025-03-26T18:32:09.894643Z","iopub.status.idle":"2025-03-26T18:32:09.956658Z","shell.execute_reply.started":"2025-03-26T18:32:09.894613Z","shell.execute_reply":"2025-03-26T18:32:09.956020Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import json\n\n# Load the dev dataset\nwith open(\"/kaggle/input/exist2025-all/EXIST2025_dev.json\", \"r\", encoding=\"utf-8\") as f:\n    dev_data = json.load(f)\n\n# Split into English & Spanish\nenglish_dev_tweets = []\nenglish_dev_ids = []\nspanish_dev_tweets = []\nspanish_dev_ids = []\n\nfor entry in dev_data.values():\n    tweet_id = entry[\"id_EXIST\"]\n    tweet = entry[\"tweet\"]\n    lang = entry[\"lang\"]\n\n    if lang == \"en\":\n        english_dev_tweets.append(tweet)\n        english_dev_ids.append(tweet_id)\n    elif lang == \"es\":\n        spanish_dev_tweets.append(tweet)\n        spanish_dev_ids.append(tweet_id)\n\n# Debugging: Check split sizes\nprint(f\"English Dev Samples: {len(english_dev_tweets)}\")\nprint(f\"Spanish Dev Samples: {len(spanish_dev_tweets)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T18:32:13.214655Z","iopub.execute_input":"2025-03-26T18:32:13.215011Z","iopub.status.idle":"2025-03-26T18:32:13.249854Z","shell.execute_reply.started":"2025-03-26T18:32:13.214979Z","shell.execute_reply":"2025-03-26T18:32:13.249004Z"}},"outputs":[{"name":"stdout","text":"English Dev Samples: 489\nSpanish Dev Samples: 549\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nfrom transformers import BertForSequenceClassification\n\n# Function to get the latest checkpoint\ndef get_latest_checkpoint(directory=\"./results\"):\n    checkpoints = [d for d in os.listdir(directory) if d.startswith(\"checkpoint-\")]\n    if not checkpoints:\n        raise ValueError(f\"No checkpoints found in {directory}\")\n    latest_checkpoint = sorted(checkpoints, key=lambda x: int(x.split('-')[-1]))[-1]\n    return os.path.join(directory, latest_checkpoint)\n\n# Load the best model checkpoint for English and Spanish\nlatest_checkpoint_en = get_latest_checkpoint(\"./results/en\")\nlatest_checkpoint_es = get_latest_checkpoint(\"./results/es\")\n\nprint(f\"Using latest checkpoint for English: {latest_checkpoint_en}\")\nprint(f\"Using latest checkpoint for Spanish: {latest_checkpoint_es}\")\n\n# Load models\nmodel_en = BertForSequenceClassification.from_pretrained(latest_checkpoint_en)\nmodel_es = BertForSequenceClassification.from_pretrained(latest_checkpoint_es)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T18:32:51.270301Z","iopub.execute_input":"2025-03-26T18:32:51.270632Z","iopub.status.idle":"2025-03-26T18:32:51.393354Z","shell.execute_reply.started":"2025-03-26T18:32:51.270603Z","shell.execute_reply":"2025-03-26T18:32:51.392496Z"}},"outputs":[{"name":"stdout","text":"Using latest checkpoint for English: ./results/en/checkpoint-652\nUsing latest checkpoint for Spanish: ./results/es/checkpoint-549\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def predict_on_dev(tweets, ids, model, tokenizer, label_classes, output_file):\n    model.eval()\n    results = []\n\n    for tweet, tweet_id in zip(tweets, ids):\n        encoding = tokenizer(tweet, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n\n        with torch.no_grad():\n            outputs = model(**encoding)\n\n        logits = outputs.logits.squeeze()\n        probs = torch.sigmoid(logits).cpu().numpy()\n\n        # Select labels with the highest probabilities (let the model decide)\n        hard_labels = [label_classes[i] for i, prob in enumerate(probs) if prob > 0.5]  # Default threshold\n\n        # If no labels meet the threshold, assign \"NO\"\n        if not hard_labels:\n            hard_labels = [\"NO\"]\n\n        results.append({\n            \"test_case\": \"EXIST2025\",\n            \"id\": tweet_id,\n            \"value\": hard_labels  # The model's final predicted labels\n        })\n\n    # Save results\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(results, f, indent=4)\n\n    print(f\"Predictions saved to {output_file}\")\n\n# Run hard-label predictions using the pretrained model\npredict_on_dev(english_dev_tweets, english_dev_ids, model_en, tokenizer, label_classes, \"EXIST2025_dev_predictions_hard_en.json\")\npredict_on_dev(spanish_dev_tweets, spanish_dev_ids, model_es, tokenizer, label_classes, \"EXIST2025_dev_predictions_hard_es.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T18:38:59.051463Z","iopub.execute_input":"2025-03-26T18:38:59.051780Z","iopub.status.idle":"2025-03-26T18:43:19.345846Z","shell.execute_reply.started":"2025-03-26T18:38:59.051755Z","shell.execute_reply":"2025-03-26T18:43:19.345041Z"}},"outputs":[{"name":"stdout","text":"Predictions saved to EXIST2025_dev_predictions_hard_en.json\nPredictions saved to EXIST2025_dev_predictions_hard_es.json\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import json\n\n# Load the Spanish predictions\nwith open(\"/kaggle/working/EXIST2025_dev_predictions_es.json\", \"r\", encoding=\"utf-8\") as f:\n    es_data = json.load(f)\n\n# Load the English predictions\nwith open(\"/kaggle/working/EXIST2025_dev_predictions_en.json\", \"r\", encoding=\"utf-8\") as f:\n    en_data = json.load(f)\n\n# Assuming both files contain lists of predictions, merge them\nif isinstance(es_data, list) and isinstance(en_data, list):\n    merged_data = es_data + en_data\nelse:\n    raise ValueError(\"JSON structure is not a list. Ensure both files contain lists.\")\n\n# Save to a new file\noutput_filename = \"EXIST2025_dev_predictions_merged2.json\"\nwith open(output_filename, \"w\", encoding=\"utf-8\") as f:\n    json.dump(merged_data, f, indent=4, ensure_ascii=False)\n\nprint(f\"Merging complete! Saved to {output_filename}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T16:06:03.164868Z","iopub.execute_input":"2025-03-26T16:06:03.165137Z","iopub.status.idle":"2025-03-26T16:06:03.199868Z","shell.execute_reply.started":"2025-03-26T16:06:03.165096Z","shell.execute_reply":"2025-03-26T16:06:03.199264Z"}},"outputs":[{"name":"stdout","text":"Merging complete! Saved to EXIST2025_dev_predictions_merged2.json\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"import json\nimport numpy as np\n\n# File paths\npredictions_file = \"/kaggle/working/EXIST2025_dev_predictions_merged.json\"\ngold_labels_file = \"/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_gold_soft.json\"\n\n# Load predictions\nwith open(predictions_file, \"r\", encoding=\"utf-8\") as f:\n    predictions_data = json.load(f)\n\n# Load gold labels\nwith open(gold_labels_file, \"r\", encoding=\"utf-8\") as f:\n    gold_data = json.load(f)\n\n# Convert gold labels into a dictionary for quick lookup\ngold_dict = {entry[\"id\"]: entry[\"value\"] for entry in gold_data}\n\n# Extract all category names\ncategories = [\"IDEOLOGICAL-INEQUALITY\", \"MISOGYNY-NON-SEXUAL-VIOLENCE\", \n              \"OBJECTIFICATION\", \"SEXUAL-VIOLENCE\", \"STEREOTYPING-DOMINANCE\", \"NO\"]\n\n# Compute metrics\nicm_soft_values = []\nicm_soft_norm_values = []\n\nfor entry in predictions_data:\n    pred_id = entry[\"id\"]\n    if pred_id in gold_dict:\n        pred_values = np.array([entry[\"value\"][cat] for cat in categories])\n        gold_values = np.array([gold_dict[pred_id][cat] for cat in categories])\n\n        # ICM Soft (Mean Squared Error)\n        mse = np.mean((pred_values - gold_values) ** 2)\n        icm_soft_values.append(mse)\n\n        # ICM Soft Norm (MSE normalized by gold label mean)\n        norm_factor = np.mean(gold_values ** 2)\n        icm_soft_norm_values.append(mse / norm_factor if norm_factor != 0 else mse)\n\n# Final aggregated scores\nfinal_icm_soft = np.mean(icm_soft_values)\nfinal_icm_soft_norm = np.mean(icm_soft_norm_values)\n\nprint(f\"ICM Soft Score: {final_icm_soft:.4f}\")\nprint(f\"ICM Soft Norm Score: {final_icm_soft_norm:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T15:31:47.435547Z","iopub.execute_input":"2025-03-26T15:31:47.435822Z","iopub.status.idle":"2025-03-26T15:31:47.477590Z","shell.execute_reply.started":"2025-03-26T15:31:47.435799Z","shell.execute_reply":"2025-03-26T15:31:47.476827Z"}},"outputs":[{"name":"stdout","text":"ICM Soft Score: 0.1570\nICM Soft Norm Score: 1.7035\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import json\nimport numpy as np\nfrom scipy.stats import norm\n\n# File paths\npredictions_file = \"/kaggle/working/EXIST2025_dev_predictions_merged1.json\"\ngold_labels_file = \"/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_gold_soft.json\"\n\n# Load predictions\nwith open(predictions_file, \"r\", encoding=\"utf-8\") as f:\n    predictions_data = json.load(f)\n\n# Load gold labels\nwith open(gold_labels_file, \"r\", encoding=\"utf-8\") as f:\n    gold_data = json.load(f)\n\n# Convert gold labels into a dictionary for quick lookup\ngold_dict = {entry[\"id\"]: entry[\"value\"] for entry in gold_data}\n\n# Extract all category names\ncategories = [\"IDEOLOGICAL-INEQUALITY\", \"MISOGYNY-NON-SEXUAL-VIOLENCE\", \n              \"OBJECTIFICATION\", \"SEXUAL-VIOLENCE\", \"STEREOTYPING-DOMINANCE\", \"NO\"]\n\n# Compute mean and std for each category in gold labels\ncategory_means = {cat: np.mean([gold_dict[d][cat] for d in gold_dict]) for cat in categories}\ncategory_stds = {cat: np.std([gold_dict[d][cat] for d in gold_dict]) for cat in categories}\n\n# Function to compute Information Content (IC)\ndef compute_ic(value, category):\n    mean = category_means[category]\n    std = category_stds[category]\n\n    if std == 0:\n        return 0  # No IC if all values are the same\n\n    # Probability of instances in gold standard exceeding the value\n    prob = 1 - norm.cdf(value, mean, std)\n    prob = max(prob, 1e-10)  # Avoid log(0)\n    \n    return -np.log2(prob)\n\n# Compute metrics\nicm_soft_values = []\nicm_soft_norm_values = []\n\nfor entry in predictions_data:\n    pred_id = entry[\"id\"]\n    if pred_id in gold_dict:\n        pred_values = {cat: entry[\"soft_label\"][cat] for cat in categories}\n        gold_values = {cat: gold_dict[pred_id][cat] for cat in categories}\n\n        # Compute IC for system output, gold standard, and their union\n        ic_pred = sum(compute_ic(pred_values[cat], cat) for cat in categories)\n        ic_gold = sum(compute_ic(gold_values[cat], cat) for cat in categories)\n        \n        # Fuzzy union (max values)\n        union_values = {cat: max(pred_values[cat], gold_values[cat]) for cat in categories}\n        ic_union = sum(compute_ic(union_values[cat], cat) for cat in categories)\n\n        # Compute ICM Soft\n        icm_score = 2 * ic_pred + 2 * ic_gold - 3 * ic_union\n        icm_score = max(icm_score, 0)  # Truncate negative scores to 0\n        icm_soft_values.append(icm_score)\n\n        # Compute ICM Soft Norm\n        icm_norm_score = icm_score / ic_gold if ic_gold != 0 else icm_score\n        icm_soft_norm_values.append(icm_norm_score)\n\n# Final aggregated scores\nfinal_icm_soft = np.mean(icm_soft_values)\nfinal_icm_soft_norm = np.mean(icm_soft_norm_values)\n\nprint(f\"ICM Soft Score: {final_icm_soft:.4f}\")\nprint(f\"ICM Soft Norm Score: {final_icm_soft_norm:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T15:25:09.063321Z","iopub.status.idle":"2025-03-26T15:25:09.063600Z","shell.execute_reply":"2025-03-26T15:25:09.063487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install pyevall","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T15:26:01.281838Z","iopub.execute_input":"2025-03-26T15:26:01.282137Z","iopub.status.idle":"2025-03-26T15:26:04.755659Z","shell.execute_reply.started":"2025-03-26T15:26:01.282113Z","shell.execute_reply":"2025-03-26T15:26:04.754766Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyevall in /usr/local/lib/python3.10/dist-packages (0.1.76)\nRequirement already satisfied: jsbeautifier==1.14.9 in /usr/local/lib/python3.10/dist-packages (from pyevall) (1.14.9)\nRequirement already satisfied: jsonschema==4.23.0 in /usr/local/lib/python3.10/dist-packages (from pyevall) (4.23.0)\nRequirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (from pyevall) (1.26.4)\nRequirement already satisfied: pandas==2.2.3 in /usr/local/lib/python3.10/dist-packages (from pyevall) (2.2.3)\nRequirement already satisfied: setuptools==69.5.1 in /usr/local/lib/python3.10/dist-packages (from pyevall) (69.5.1)\nRequirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.10/dist-packages (from pyevall) (0.9.0)\nRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from jsbeautifier==1.14.9->pyevall) (1.17.0)\nRequirement already satisfied: editorconfig>=0.12.2 in /usr/local/lib/python3.10/dist-packages (from jsbeautifier==1.14.9->pyevall) (0.17.0)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.23.0->pyevall) (25.1.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.23.0->pyevall) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.23.0->pyevall) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.23.0->pyevall) (0.22.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->pyevall) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->pyevall) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->pyevall) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->pyevall) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->pyevall) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->pyevall) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->pyevall) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->pyevall) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->pyevall) (2025.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy==1.26.4->pyevall) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy==1.26.4->pyevall) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy==1.26.4->pyevall) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy==1.26.4->pyevall) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy==1.26.4->pyevall) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"from pyevall.evaluation import PyEvALLEvaluation\nfrom pyevall.utils.utils import PyEvALLUtils\n\npredictions = \"/kaggle/working/EXIST2025_dev_predictions_merged2.json\"         \ngold = \"/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_gold_soft.json\" \ntest = PyEvALLEvaluation() \nparams= dict() \nparams[PyEvALLUtils.PARAM_REPORT]= PyEvALLUtils.PARAM_OPTION_REPORT_EMBEDDED  \nmetrics=[\"ICMSoft\", \"ICMSoftNorm\", \"CrossEntropy\"]     # for soft    \n# metrics=[\"ICM\", \"ICMNorm\" ,\"FMeasure\"]           # for hard     \nreport= test.evaluate(predictions, gold, metrics, **params) \nreport.print_report()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T16:06:03.200885Z","iopub.execute_input":"2025-03-26T16:06:03.201131Z","iopub.status.idle":"2025-03-26T16:06:07.807741Z","shell.execute_reply.started":"2025-03-26T16:06:03.201110Z","shell.execute_reply":"2025-03-26T16:06:07.806991Z"}},"outputs":[{"name":"stdout","text":"2025-03-26 16:06:03,206 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICMSoft', 'ICMSoftNorm', 'CrossEntropy']\n2025-03-26 16:06:03,561 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Soft evaluation method\n2025-03-26 16:06:04,703 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM-Soft Normalized evaluation method\n2025-03-26 16:06:04,707 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Soft evaluation method\n2025-03-26 16:06:05,821 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Soft evaluation method\n2025-03-26 16:06:07,330 - pyevall.metrics.metrics - INFO -             evaluate() - Executing Cross Entropy evaluation method\n{\n  \"metrics\": {\n    \"ICMSoft\": {\n      \"name\": \"Information Contrast Model Soft\",\n      \"acronym\": \"ICM-Soft\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": -23.845348686424167\n        }],\n        \"average_per_test_case\": -23.845348686424167\n      }\n    },\n    \"ICMSoftNorm\": {\n      \"name\": \"Normalized Information Contrast Model Soft\",\n      \"acronym\": \"ICM-Soft-Norm\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 0\n        }],\n        \"average_per_test_case\": 0.0\n      }\n    },\n    \"CrossEntropy\": {\n      \"name\": \"Cross Entropy\",\n      \"acronym\": \"CE\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 1.9822057083417357\n        }],\n        \"average_per_test_case\": 1.9822057083417357\n      }\n    }\n  },\n  \"files\": {\n    \"EXIST2025_dev_predictions_merged2.json\": {\n      \"name\": \"EXIST2025_dev_predictions_merged2.json\",\n      \"status\": \"OK\",\n      \"gold\": false,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_predictions_merged2.json.\",\n      \"errors\": {}\n    },\n    \"EXIST2025_dev_task1_3_gold_soft.json\": {\n      \"name\": \"EXIST2025_dev_task1_3_gold_soft.json\",\n      \"status\": \"OK\",\n      \"gold\": true,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_task1_3_gold_soft.json.\",\n      \"errors\": {}\n    }\n  }\n}\n","output_type":"stream"}],"execution_count":32}]}