{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11142046,"sourceType":"datasetVersion","datasetId":6950309},{"sourceId":11159551,"sourceType":"datasetVersion","datasetId":6963199}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T19:08:15.098734Z","iopub.execute_input":"2025-03-26T19:08:15.099019Z","iopub.status.idle":"2025-03-26T19:08:15.411412Z","shell.execute_reply.started":"2025-03-26T19:08:15.098994Z","shell.execute_reply":"2025-03-26T19:08:15.410593Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/exist2025/EXIST2025_training.json\n/kaggle/input/exist2025/EXIST2025_dev.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_majority_class_soft.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_minority_class_hard.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_3_minority_class_hard.json\n/kaggle/input/exist2025-all/EXIST2025_training.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_minority_class_soft.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_3_majority_class_soft.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_3_majority_class_hard.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_3_minority_class_soft.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_majority_class_hard.json\n/kaggle/input/exist2025-all/EXIST2025_dev.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_gold_hard.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_3_gold_soft.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_gold_soft.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_3_gold_hard.json\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import wandb\n\nwandb.login(key=\"0c5f368f1f51fd942ec7bb3a1c74efb7bdc832d6\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T19:08:19.630724Z","iopub.execute_input":"2025-03-26T19:08:19.631155Z","iopub.status.idle":"2025-03-26T19:08:28.512223Z","shell.execute_reply.started":"2025-03-26T19:08:19.631108Z","shell.execute_reply":"2025-03-26T19:08:28.511520Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmshoaibvohra\u001b[0m (\u001b[33mmshoaibvohra-habib-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import json\nimport torch\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nfrom torch.utils.data import Dataset\n\n# Load the dataset\nwith open(\"/kaggle/input/exist2025/EXIST2025_training.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\n# Define correct label classes\nCORRECT_LABELS = [\n    \"IDEOLOGICAL-INEQUALITY\",\n    \"MISOGYNY-NON-SEXUAL-VIOLENCE\",\n    \"OBJECTIFICATION\",\n    \"SEXUAL-VIOLENCE\",\n    \"STEREOTYPING-DOMINANCE\",\n    \"NO\"  # Represents non-sexist tweets (previously \"-\")\n]\n\n# Extract relevant fields\ndef process_data(data, lang):\n    tweets = []\n    labels = []\n    ids = []\n\n    for entry in data.values():\n        if entry[\"lang\"] == lang:\n            tweet_id = entry[\"id_EXIST\"]\n            tweet = entry[\"tweet\"]\n            is_sexist = any(label == \"YES\" for label in entry[\"labels_task1_1\"])  # Check if at least one annotator marked it sexist\n            label = entry[\"labels_task1_3\"] if is_sexist else [[\"NO\"]]  # Non-sexist tweets get \"NO\"\n\n            # Flatten labels\n            processed_labels = [l if l != \"-\" else \"NO\" for sublist in label for l in sublist]\n\n            # Remove \"UNKNOWN\"\n            processed_labels = [l for l in processed_labels if l != \"UNKNOWN\"]\n\n            # Ensure every tweet has at least one label\n            if not processed_labels:\n                processed_labels = [\"NO\"]\n\n            tweets.append(tweet)\n            labels.append(processed_labels)\n            ids.append(tweet_id)\n\n    return tweets, labels, ids\n\n# Process data for English and Spanish\nenglish_tweets, english_labels, english_ids = process_data(data, \"en\")\nspanish_tweets, spanish_labels, spanish_ids = process_data(data, \"es\")\n\n# MultiLabel Binarizer with Fixed Labels\nmlb = MultiLabelBinarizer(classes=CORRECT_LABELS)  # Force correct label order\nenglish_labels_bin = mlb.fit_transform(english_labels)\nspanish_labels_bin = mlb.transform(spanish_labels)  # Use the same binarizer\n\nlabel_classes = mlb.classes_\nprint(f\"Corrected Label Classes: {label_classes}\")  # Debugging\n\n# Tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n\n# Custom Dataset Class\nclass TweetDataset(Dataset):\n    def __init__(self, texts, labels, ids, tokenizer, max_length=256):\n        self.texts = texts\n        self.labels = labels\n        self.ids = ids\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        tweet_id = self.ids[idx]\n        labels = torch.tensor(self.labels[idx], dtype=torch.float)\n        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n\n        return {\n            \"id\": tweet_id,\n            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n            \"labels\": labels\n        }\n\n# Split into train/test\ndef get_datasets(tweets, labels, ids):\n    train_texts, val_texts, train_labels, val_labels, train_ids, val_ids = train_test_split(tweets, labels, ids, test_size=0.2, random_state=42)\n    train_dataset = TweetDataset(train_texts, train_labels, train_ids, tokenizer)\n    val_dataset = TweetDataset(val_texts, val_labels, val_ids, tokenizer)\n    return train_dataset, val_dataset\n\ntrain_dataset_en, val_dataset_en = get_datasets(english_tweets, english_labels_bin, english_ids)\ntrain_dataset_es, val_dataset_es = get_datasets(spanish_tweets, spanish_labels_bin, spanish_ids)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T19:08:28.513378Z","iopub.execute_input":"2025-03-26T19:08:28.513856Z","iopub.status.idle":"2025-03-26T19:08:52.954230Z","shell.execute_reply.started":"2025-03-26T19:08:28.513833Z","shell.execute_reply":"2025-03-26T19:08:52.953208Z"}},"outputs":[{"name":"stdout","text":"Corrected Label Classes: ['IDEOLOGICAL-INEQUALITY' 'MISOGYNY-NON-SEXUAL-VIOLENCE' 'OBJECTIFICATION'\n 'SEXUAL-VIOLENCE' 'STEREOTYPING-DOMINANCE' 'NO']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9067f288342a4b5a8d05bf79eb7b214c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f09eafabd7a4da8864f23a66897c6e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f574d5b431d411795b90c2201231c5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af7a98c871084b7aaa8d48b9bd042f82"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# EN model\nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-multilingual-cased\", \n    num_labels=len(label_classes), \n    problem_type=\"multi_label_classification\"\n)\n\n# Training Arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results/en\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    num_train_epochs=5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset_en,  # Change to train_dataset_es for Spanish\n    eval_dataset=val_dataset_en,  # Change to val_dataset_es for Spanish\n)\n\n# Train the model\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T19:08:52.955679Z","iopub.execute_input":"2025-03-26T19:08:52.956264Z","iopub.status.idle":"2025-03-26T19:18:08.554249Z","shell.execute_reply.started":"2025-03-26T19:08:52.956240Z","shell.execute_reply":"2025-03-26T19:18:08.553431Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f05d25742ab46efb0cd25d6f696aa9f"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250326_190858-u85pj4vy</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mshoaibvohra-habib-university/huggingface/runs/u85pj4vy' target=\"_blank\">./results/en</a></strong> to <a href='https://wandb.ai/mshoaibvohra-habib-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mshoaibvohra-habib-university/huggingface' target=\"_blank\">https://wandb.ai/mshoaibvohra-habib-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mshoaibvohra-habib-university/huggingface/runs/u85pj4vy' target=\"_blank\">https://wandb.ai/mshoaibvohra-habib-university/huggingface/runs/u85pj4vy</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='815' max='815' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [815/815 09:00, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.538100</td>\n      <td>0.548271</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.479900</td>\n      <td>0.500603</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.424100</td>\n      <td>0.485042</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.350000</td>\n      <td>0.510568</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.335100</td>\n      <td>0.519737</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=815, training_loss=0.44129459082714617, metrics={'train_runtime': 550.5446, 'train_samples_per_second': 23.686, 'train_steps_per_second': 1.48, 'total_flos': 1715545691504640.0, 'train_loss': 0.44129459082714617, 'epoch': 5.0})"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Es model\nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-multilingual-cased\", \n    num_labels=len(label_classes), \n    problem_type=\"multi_label_classification\"\n)\n\n# Training Arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results/es\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    num_train_epochs=5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset_es,  # Change to train_dataset_es for Spanish\n    eval_dataset=val_dataset_es,  # Change to val_dataset_es for Spanish\n)\n\n# Train the model\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T19:18:08.555766Z","iopub.execute_input":"2025-03-26T19:18:08.556105Z","iopub.status.idle":"2025-03-26T19:28:26.185879Z","shell.execute_reply.started":"2025-03-26T19:18:08.556068Z","shell.execute_reply":"2025-03-26T19:28:26.185032Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='915' max='915' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [915/915 10:15, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.565200</td>\n      <td>0.559680</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.531800</td>\n      <td>0.533694</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.432300</td>\n      <td>0.530696</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.377000</td>\n      <td>0.535597</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.333500</td>\n      <td>0.543649</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=915, training_loss=0.46197543209367764, metrics={'train_runtime': 616.4443, 'train_samples_per_second': 23.749, 'train_steps_per_second': 1.484, 'total_flos': 1926042095370240.0, 'train_loss': 0.46197543209367764, 'epoch': 5.0})"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"with open(\"/kaggle/input/exist2025-all/EXIST2025_dev.json\", \"r\", encoding=\"utf-8\") as f:\n    dev_data = json.load(f)\n\n# Extract tweets and IDs\ndev_tweets = [entry[\"tweet\"] for entry in dev_data.values()]\ndev_ids = [entry[\"id_EXIST\"] for entry in dev_data.values()]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T19:28:26.186827Z","iopub.execute_input":"2025-03-26T19:28:26.187170Z","iopub.status.idle":"2025-03-26T19:28:26.245213Z","shell.execute_reply.started":"2025-03-26T19:28:26.187110Z","shell.execute_reply":"2025-03-26T19:28:26.244395Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import json\n\n# Load the dev dataset\nwith open(\"/kaggle/input/exist2025-all/EXIST2025_dev.json\", \"r\", encoding=\"utf-8\") as f:\n    dev_data = json.load(f)\n\n# Split into English & Spanish\nenglish_dev_tweets = []\nenglish_dev_ids = []\nspanish_dev_tweets = []\nspanish_dev_ids = []\n\nfor entry in dev_data.values():\n    tweet_id = entry[\"id_EXIST\"]\n    tweet = entry[\"tweet\"]\n    lang = entry[\"lang\"]\n\n    if lang == \"en\":\n        english_dev_tweets.append(tweet)\n        english_dev_ids.append(tweet_id)\n    elif lang == \"es\":\n        spanish_dev_tweets.append(tweet)\n        spanish_dev_ids.append(tweet_id)\n\n# Debugging: Check split sizes\nprint(f\"English Dev Samples: {len(english_dev_tweets)}\")\nprint(f\"Spanish Dev Samples: {len(spanish_dev_tweets)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T19:28:26.246083Z","iopub.execute_input":"2025-03-26T19:28:26.246309Z","iopub.status.idle":"2025-03-26T19:28:28.287659Z","shell.execute_reply.started":"2025-03-26T19:28:26.246289Z","shell.execute_reply":"2025-03-26T19:28:28.286774Z"}},"outputs":[{"name":"stdout","text":"English Dev Samples: 489\nSpanish Dev Samples: 549\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import os\nfrom transformers import BertForSequenceClassification\n\n# Function to get the latest checkpoint\ndef get_latest_checkpoint(directory=\"./results\"):\n    checkpoints = [d for d in os.listdir(directory) if d.startswith(\"checkpoint-\")]\n    if not checkpoints:\n        raise ValueError(f\"No checkpoints found in {directory}\")\n    latest_checkpoint = sorted(checkpoints, key=lambda x: int(x.split('-')[-1]))[-1]\n    return os.path.join(directory, latest_checkpoint)\n\n# Load the best model checkpoint for English and Spanish\nlatest_checkpoint_en = get_latest_checkpoint(\"./results/en\")\nlatest_checkpoint_es = get_latest_checkpoint(\"./results/es\")\n\nprint(f\"Using latest checkpoint for English: {latest_checkpoint_en}\")\nprint(f\"Using latest checkpoint for Spanish: {latest_checkpoint_es}\")\n\n# Load models\nmodel_en = BertForSequenceClassification.from_pretrained(latest_checkpoint_en)\nmodel_es = BertForSequenceClassification.from_pretrained(latest_checkpoint_es)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T19:28:28.289938Z","iopub.execute_input":"2025-03-26T19:28:28.290229Z","iopub.status.idle":"2025-03-26T19:28:29.699891Z","shell.execute_reply.started":"2025-03-26T19:28:28.290195Z","shell.execute_reply":"2025-03-26T19:28:29.699201Z"}},"outputs":[{"name":"stdout","text":"Using latest checkpoint for English: ./results/en/checkpoint-815\nUsing latest checkpoint for Spanish: ./results/es/checkpoint-915\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def predict_on_dev(tweets, ids, model, tokenizer, label_classes, output_file):\n    model.eval()\n    results = []\n\n    for tweet, tweet_id in zip(tweets, ids):\n        encoding = tokenizer(tweet, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n\n        with torch.no_grad():\n            outputs = model(**encoding)\n\n        logits = outputs.logits.squeeze()\n        probs = torch.sigmoid(logits).cpu().numpy()\n\n        # Convert probabilities to dictionary format and sort by highest probability\n        soft_label_dict = {label_classes[i]: float(probs[i]) for i in range(len(label_classes))}\n        sorted_soft_label_dict = dict(sorted(soft_label_dict.items(), key=lambda item: item[1], reverse=True))  # Sort descending\n\n        results.append({\n            \"test_case\": \"EXIST2025\",\n            \"id\": tweet_id,\n            \"value\": sorted_soft_label_dict  # Rename \"soft_label\" to \"value\" and sort it\n        })\n\n    # Save results\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(results, f, indent=4)\n\n    print(f\"Predictions saved to {output_file}\")\n\n# Run predictions\npredict_on_dev(english_dev_tweets, english_dev_ids, model_en, tokenizer, label_classes, \"EXIST2025_dev_predictions_en.json\")\npredict_on_dev(spanish_dev_tweets, spanish_dev_ids, model_es, tokenizer, label_classes, \"EXIST2025_dev_predictions_es.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T16:01:25.796884Z","iopub.execute_input":"2025-03-26T16:01:25.797218Z","iopub.status.idle":"2025-03-26T16:06:03.163476Z","shell.execute_reply.started":"2025-03-26T16:01:25.797188Z","shell.execute_reply":"2025-03-26T16:06:03.162533Z"}},"outputs":[{"name":"stdout","text":"Predictions saved to EXIST2025_dev_predictions_en.json\nPredictions saved to EXIST2025_dev_predictions_es.json\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"def predict_hard_labels_from_soft_model(tweets, ids, model, tokenizer, label_classes, output_file, threshold=0.5):\n    \"\"\"\n    Uses the soft model to predict hard labels by applying a threshold.\n    - Labels are assigned if their probability > threshold.\n    - If no labels pass the threshold, assigns \"NO\".\n    \"\"\"\n    model.eval()\n    results = []\n\n    for tweet, tweet_id in zip(tweets, ids):\n        encoding = tokenizer(tweet, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n\n        with torch.no_grad():\n            outputs = model(**encoding)\n\n        logits = outputs.logits.squeeze()\n        probs = torch.sigmoid(logits).cpu().numpy()\n\n        # Convert probabilities to hard labels using threshold\n        hard_labels = [label_classes[i] for i, prob in enumerate(probs) if prob > threshold]\n\n        # If no labels meet the threshold, assign \"NO\"\n        if not hard_labels:\n            hard_labels = [\"NO\"]\n\n        results.append({\n            \"test_case\": \"EXIST2025\",\n            \"id\": tweet_id,\n            \"value\": hard_labels  # Final hard labels\n        })\n\n    # Save results\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(results, f, indent=4)\n\n    print(f\"Hard label predictions saved to {output_file}\")\n\n# Run hard label prediction using soft model\npredict_hard_labels_from_soft_model(english_dev_tweets, english_dev_ids, model_en, tokenizer, label_classes, \"EXIST2025_dev_predictions_hard_en.json\")\npredict_hard_labels_from_soft_model(spanish_dev_tweets, spanish_dev_ids, model_es, tokenizer, label_classes, \"EXIST2025_dev_predictions_hard_es.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T19:28:29.700803Z","iopub.execute_input":"2025-03-26T19:28:29.701115Z","iopub.status.idle":"2025-03-26T19:32:38.769974Z","shell.execute_reply.started":"2025-03-26T19:28:29.701091Z","shell.execute_reply":"2025-03-26T19:32:38.769019Z"}},"outputs":[{"name":"stdout","text":"Hard label predictions saved to EXIST2025_dev_predictions_hard_en.json\nHard label predictions saved to EXIST2025_dev_predictions_hard_es.json\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import json\n\n# Load the Spanish predictions\nwith open(\"/kaggle/working/EXIST2025_dev_predictions_hard_es.json\", \"r\", encoding=\"utf-8\") as f:\n    es_data = json.load(f)\n\n# Load the English predictions\nwith open(\"/kaggle/working/EXIST2025_dev_predictions_hard_en.json\", \"r\", encoding=\"utf-8\") as f:\n    en_data = json.load(f)\n\n# Assuming both files contain lists of predictions, merge them\nif isinstance(es_data, list) and isinstance(en_data, list):\n    merged_data = es_data + en_data\nelse:\n    raise ValueError(\"JSON structure is not a list. Ensure both files contain lists.\")\n\n# Save to a new file\noutput_filename = \"EXIST2025_dev_predictions_merged2.json\"\nwith open(output_filename, \"w\", encoding=\"utf-8\") as f:\n    json.dump(merged_data, f, indent=4, ensure_ascii=False)\n\nprint(f\"Merging complete! Saved to {output_filename}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T19:33:51.066283Z","iopub.execute_input":"2025-03-26T19:33:51.066619Z","iopub.status.idle":"2025-03-26T19:33:51.087083Z","shell.execute_reply.started":"2025-03-26T19:33:51.066594Z","shell.execute_reply":"2025-03-26T19:33:51.086025Z"}},"outputs":[{"name":"stdout","text":"Merging complete! Saved to EXIST2025_dev_predictions_merged2.json\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import json\nimport numpy as np\n\n# File paths\npredictions_file = \"/kaggle/working/EXIST2025_dev_predictions_merged.json\"\ngold_labels_file = \"/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_gold_soft.json\"\n\n# Load predictions\nwith open(predictions_file, \"r\", encoding=\"utf-8\") as f:\n    predictions_data = json.load(f)\n\n# Load gold labels\nwith open(gold_labels_file, \"r\", encoding=\"utf-8\") as f:\n    gold_data = json.load(f)\n\n# Convert gold labels into a dictionary for quick lookup\ngold_dict = {entry[\"id\"]: entry[\"value\"] for entry in gold_data}\n\n# Extract all category names\ncategories = [\"IDEOLOGICAL-INEQUALITY\", \"MISOGYNY-NON-SEXUAL-VIOLENCE\", \n              \"OBJECTIFICATION\", \"SEXUAL-VIOLENCE\", \"STEREOTYPING-DOMINANCE\", \"NO\"]\n\n# Compute metrics\nicm_soft_values = []\nicm_soft_norm_values = []\n\nfor entry in predictions_data:\n    pred_id = entry[\"id\"]\n    if pred_id in gold_dict:\n        pred_values = np.array([entry[\"value\"][cat] for cat in categories])\n        gold_values = np.array([gold_dict[pred_id][cat] for cat in categories])\n\n        # ICM Soft (Mean Squared Error)\n        mse = np.mean((pred_values - gold_values) ** 2)\n        icm_soft_values.append(mse)\n\n        # ICM Soft Norm (MSE normalized by gold label mean)\n        norm_factor = np.mean(gold_values ** 2)\n        icm_soft_norm_values.append(mse / norm_factor if norm_factor != 0 else mse)\n\n# Final aggregated scores\nfinal_icm_soft = np.mean(icm_soft_values)\nfinal_icm_soft_norm = np.mean(icm_soft_norm_values)\n\nprint(f\"ICM Soft Score: {final_icm_soft:.4f}\")\nprint(f\"ICM Soft Norm Score: {final_icm_soft_norm:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T15:31:47.435547Z","iopub.execute_input":"2025-03-26T15:31:47.435822Z","iopub.status.idle":"2025-03-26T15:31:47.477590Z","shell.execute_reply.started":"2025-03-26T15:31:47.435799Z","shell.execute_reply":"2025-03-26T15:31:47.476827Z"}},"outputs":[{"name":"stdout","text":"ICM Soft Score: 0.1570\nICM Soft Norm Score: 1.7035\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"pip install pyevall","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T19:34:07.321489Z","iopub.execute_input":"2025-03-26T19:34:07.321775Z","iopub.status.idle":"2025-03-26T19:34:33.988414Z","shell.execute_reply.started":"2025-03-26T19:34:07.321752Z","shell.execute_reply":"2025-03-26T19:34:33.987563Z"}},"outputs":[{"name":"stdout","text":"Collecting pyevall\n  Downloading PyEvALL-0.1.76.tar.gz (39 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting jsbeautifier==1.14.9 (from pyevall)\n  Downloading jsbeautifier-1.14.9.tar.gz (75 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: jsonschema==4.23.0 in /usr/local/lib/python3.10/dist-packages (from pyevall) (4.23.0)\nRequirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (from pyevall) (1.26.4)\nRequirement already satisfied: pandas==2.2.3 in /usr/local/lib/python3.10/dist-packages (from pyevall) (2.2.3)\nCollecting setuptools==69.5.1 (from pyevall)\n  Downloading setuptools-69.5.1-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.10/dist-packages (from pyevall) (0.9.0)\nRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from jsbeautifier==1.14.9->pyevall) (1.17.0)\nCollecting editorconfig>=0.12.2 (from jsbeautifier==1.14.9->pyevall)\n  Downloading EditorConfig-0.17.0-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.23.0->pyevall) (25.1.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.23.0->pyevall) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.23.0->pyevall) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.23.0->pyevall) (0.22.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->pyevall) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->pyevall) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->pyevall) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->pyevall) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->pyevall) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->pyevall) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->pyevall) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->pyevall) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->pyevall) (2025.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy==1.26.4->pyevall) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy==1.26.4->pyevall) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy==1.26.4->pyevall) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy==1.26.4->pyevall) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy==1.26.4->pyevall) (2024.2.0)\nDownloading setuptools-69.5.1-py3-none-any.whl (894 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m894.6/894.6 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading EditorConfig-0.17.0-py3-none-any.whl (16 kB)\nBuilding wheels for collected packages: pyevall, jsbeautifier\n  Building wheel for pyevall (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pyevall: filename=PyEvALL-0.1.76-py3-none-any.whl size=34657 sha256=ed6f6e2654538abaf89647828b136844ba005cb70023c094d42a7603891cac5e\n  Stored in directory: /root/.cache/pip/wheels/63/ab/1c/bf3f9f06fe2ba80564911b92b1fcdd3bdac5420657b7755fb2\n  Building wheel for jsbeautifier (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for jsbeautifier: filename=jsbeautifier-1.14.9-py3-none-any.whl size=94157 sha256=595b07eabee51527e56da3d27339ec84d22f6f454b16ba0b09d4bc3e28d92c70\n  Stored in directory: /root/.cache/pip/wheels/c4/5c/25/09f8b2e8dddb4fc3d70817c67b375a9069a2628847ffbdfc65\nSuccessfully built pyevall jsbeautifier\nInstalling collected packages: editorconfig, setuptools, jsbeautifier, pyevall\n  Attempting uninstall: setuptools\n    Found existing installation: setuptools 75.1.0\n    Uninstalling setuptools-75.1.0:\n      Successfully uninstalled setuptools-75.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed editorconfig-0.17.0 jsbeautifier-1.14.9 pyevall-0.1.76 setuptools-69.5.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from pyevall.evaluation import PyEvALLEvaluation\nfrom pyevall.utils.utils import PyEvALLUtils\n\npredictions = \"/kaggle/working/EXIST2025_dev_predictions_merged2.json\"         \ngold = \"/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_gold_hard.json\" \ntest = PyEvALLEvaluation() \nparams= dict() \nparams[PyEvALLUtils.PARAM_REPORT]= PyEvALLUtils.PARAM_OPTION_REPORT_EMBEDDED  \n# metrics=[\"ICMSoft\", \"ICMSoftNorm\", \"CrossEntropy\"]     # for soft    \nmetrics=[\"ICM\", \"ICMNorm\" ,\"FMeasure\"] \nTASK1_3_HIERARCHY = {\"YES\":[\"IDEOLOGICAL-INEQUALITY\",\"STEREOTYPING-DOMINANCE\",\"OBJECTIFICATION\", \"SEXUAL-VIOLENCE\", \"MISOGYNY-NON-SEXUAL-VIOLENCE\"], \"NO\":[]}\nparams[PyEvALLUtils.PARAM_HIERARCHY]= TASK1_3_HIERARCHY  \nreport= test.evaluate(predictions, gold, metrics, **params) \nreport.print_report()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T19:37:39.274843Z","iopub.execute_input":"2025-03-26T19:37:39.275182Z","iopub.status.idle":"2025-03-26T19:37:39.937336Z","shell.execute_reply.started":"2025-03-26T19:37:39.275149Z","shell.execute_reply":"2025-03-26T19:37:39.936511Z"}},"outputs":[{"name":"stdout","text":"2025-03-26 19:37:39,281 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM', 'ICMNorm', 'FMeasure']\n2025-03-26 19:37:39,461 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-03-26 19:37:39,466 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Normalized evaluation method\n2025-03-26 19:37:39,471 - pyevall.metrics.metrics - INFO -             evaluate() - Executing fmeasure evaluation method\n{\n  \"metrics\": {\n    \"ICM\": {\n      \"name\": \"Information Contrast model\",\n      \"acronym\": \"ICM\",\n      \"description\": \"Coming soon!\\\\nThe evaluation FAIL.\",\n      \"status\": \"FAIL\",\n      \"results\": {\n        \"test_cases\": [],\n        \"average_per_test_case\": null\n      },\n      \"preconditions\": {\n        \"METRIC_PRECONDITION_NOT_IMPLEMENTED_EVALUATION_CONTEXT\": {\n          \"name\": \"METRIC_PRECONDITION_NOT_IMPLEMENTED_EVALUATION_CONTEXT\",\n          \"description\": \" The selected context of evaluation for this metric is not implemented.\\\\nThe metric name is: Information Contrast model.\\\\nTest case(s) name: EXIST2025.\",\n          \"status\": \"FAIL\",\n          \"test_cases\": [\"EXIST2025\"]\n        }\n      }\n    },\n    \"ICMNorm\": {\n      \"name\": \"Normalized Information Contrast Model\",\n      \"acronym\": \"ICM-Norm\",\n      \"description\": \"Coming soon!\\\\nThe evaluation FAIL.\",\n      \"status\": \"FAIL\",\n      \"results\": {\n        \"test_cases\": [],\n        \"average_per_test_case\": null\n      },\n      \"preconditions\": {\n        \"METRIC_PRECONDITION_NOT_IMPLEMENTED_EVALUATION_CONTEXT\": {\n          \"name\": \"METRIC_PRECONDITION_NOT_IMPLEMENTED_EVALUATION_CONTEXT\",\n          \"description\": \" The selected context of evaluation for this metric is not implemented.\\\\nThe metric name is: Normalized Information Contrast Model.\\\\nTest case(s) name: EXIST2025.\",\n          \"status\": \"FAIL\",\n          \"test_cases\": [\"EXIST2025\"]\n        }\n      }\n    },\n    \"FMeasure\": {\n      \"name\": \"F-Measure\",\n      \"acronym\": \"F1\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"classes\": {\n            \"IDEOLOGICAL-INEQUALITY\": 0.48773448773448774,\n            \"STEREOTYPING-DOMINANCE\": 0.5367647058823529,\n            \"MISOGYNY-NON-SEXUAL-VIOLENCE\": 0.4133858267716535,\n            \"NO\": 0.6606397774687065,\n            \"SEXUAL-VIOLENCE\": 0.4404761904761905,\n            \"OBJECTIFICATION\": 0.4722662440570523\n          },\n          \"average\": 0.501877872065074\n        }],\n        \"average_per_test_case\": 0.501877872065074\n      }\n    }\n  },\n  \"files\": {\n    \"EXIST2025_dev_predictions_merged2.json\": {\n      \"name\": \"EXIST2025_dev_predictions_merged2.json\",\n      \"status\": \"OK\",\n      \"gold\": false,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_predictions_merged2.json.\",\n      \"errors\": {}\n    },\n    \"EXIST2025_dev_task1_3_gold_hard.json\": {\n      \"name\": \"EXIST2025_dev_task1_3_gold_hard.json\",\n      \"status\": \"OK\",\n      \"gold\": true,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_task1_3_gold_hard.json.\",\n      \"errors\": {}\n    }\n  }\n}\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from pyevall.evaluation import PyEvALLEvaluation\nfrom pyevall.utils.utils import PyEvALLUtils\n\n# Define file paths\npredictions = \"/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_majority_class_hard.json\"  # Change to your actual prediction file\ngold = \"/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_gold_hard.json\"   # Change to your actual gold file\n\n# Define hierarchical structure for subtask 1.3\nTASK1_3_HIERARCHY = {\n    \"YES\": [\"IDEOLOGICAL-INEQUALITY\", \"STEREOTYPING-DOMINANCE\",\n            \"OBJECTIFICATION\", \"SEXUAL-VIOLENCE\", \"MISOGYNY-NON-SEXUAL-VIOLENCE\"],\n    \"NO\": []\n}\n\n# Initialize PyEvALL evaluation\nevaluator = PyEvALLEvaluation()\n\n# Set evaluation parameters\nparams = dict()\nparams[PyEvALLUtils.PARAM_HIERARCHY] = TASK1_3_HIERARCHY\nparams[PyEvALLUtils.PARAM_REPORT] = PyEvALLUtils.PARAM_OPTION_REPORT_EMBEDDED  # Embedded report\n\n# Define evaluation metrics\nmetrics = [\"ICM\", \"ICMNorm\", \"FMeasure\"]\n# metrics=[\"ICMSoft\", \"ICMSoftNorm\"]\n\n# Run evaluation\nreport = evaluator.evaluate(predictions, gold, metrics, **params)\n\n# Print evaluation report\nreport.print_report()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T19:53:25.350973Z","iopub.execute_input":"2025-03-26T19:53:25.351362Z","iopub.status.idle":"2025-03-26T19:53:27.449003Z","shell.execute_reply.started":"2025-03-26T19:53:25.351335Z","shell.execute_reply":"2025-03-26T19:53:27.448191Z"}},"outputs":[{"name":"stdout","text":"2025-03-26 19:53:25,358 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM', 'ICMNorm', 'FMeasure']\n2025-03-26 19:53:25,541 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-03-26 19:53:26,014 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Normalized evaluation method\n2025-03-26 19:53:26,017 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-03-26 19:53:26,489 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-03-26 19:53:26,951 - pyevall.metrics.metrics - INFO -             evaluate() - Executing fmeasure evaluation method\n{\n  \"metrics\": {\n    \"ICM\": {\n      \"name\": \"Information Contrast model\",\n      \"acronym\": \"ICM\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": -1.7237204351739308\n        }],\n        \"average_per_test_case\": -1.7237204351739308\n      }\n    },\n    \"ICMNorm\": {\n      \"name\": \"Normalized Information Contrast Model\",\n      \"acronym\": \"ICM-Norm\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 0.11606465282839987\n        }],\n        \"average_per_test_case\": 0.11606465282839987\n      }\n    },\n    \"FMeasure\": {\n      \"name\": \"F-Measure\",\n      \"acronym\": \"F1\",\n      \"description\": \"Coming soon!\\\\nThe evaluation WARNING.\",\n      \"status\": \"WARNING\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"classes\": {\n            \"IDEOLOGICAL-INEQUALITY\": 0,\n            \"STEREOTYPING-DOMINANCE\": 0,\n            \"MISOGYNY-NON-SEXUAL-VIOLENCE\": 0,\n            \"NO\": 0.6315095583388266,\n            \"SEXUAL-VIOLENCE\": 0,\n            \"OBJECTIFICATION\": 0\n          },\n          \"average\": 0.1052515930564711\n        }],\n        \"average_per_test_case\": 0.1052515930564711\n      },\n      \"preconditions\": {\n        \"METRIC_PRECONDITION_HIERARCHY_NOT_VALID_FOR_METRIC\": {\n          \"name\": \"METRIC_PRECONDITION_HIERARCHY_NOT_VALID_FOR_METRIC\",\n          \"description\": \"The hierarchy is provided for the evaluation but this metric does not allow to use it. Hierarchy is ignored.\\\\nThe metric name is: F-Measure.\\\\nTest case(s) name: EXIST2025.\",\n          \"status\": \"WARNING\",\n          \"test_cases\": [\"EXIST2025\"]\n        }\n      }\n    }\n  },\n  \"files\": {\n    \"EXIST2025_dev_task1_3_majority_class_hard.json\": {\n      \"name\": \"EXIST2025_dev_task1_3_majority_class_hard.json\",\n      \"status\": \"OK\",\n      \"gold\": false,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_task1_3_majority_class_hard.json.\",\n      \"errors\": {}\n    },\n    \"EXIST2025_dev_task1_3_gold_hard.json\": {\n      \"name\": \"EXIST2025_dev_task1_3_gold_hard.json\",\n      \"status\": \"OK\",\n      \"gold\": true,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_task1_3_gold_hard.json.\",\n      \"errors\": {}\n    }\n  }\n}\n","output_type":"stream"}],"execution_count":21}]}