{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11142046,"sourceType":"datasetVersion","datasetId":6950309},{"sourceId":11182195,"sourceType":"datasetVersion","datasetId":6963199}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T00:15:13.659574Z","iopub.execute_input":"2025-04-14T00:15:13.659919Z","iopub.status.idle":"2025-04-14T00:15:13.679633Z","shell.execute_reply.started":"2025-04-14T00:15:13.659896Z","shell.execute_reply":"2025-04-14T00:15:13.678970Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_majority_class_soft.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_minority_class_hard.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_3_minority_class_hard.json\n/kaggle/input/exist2025-all/EXIST2025_training.json\n/kaggle/input/exist2025-all/EXIST2025_dev_en.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_minority_class_soft.json\n/kaggle/input/exist2025-all/EXIST2025_training_EASE_es.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_3_majority_class_soft.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_3_majority_class_hard.json\n/kaggle/input/exist2025-all/EXIST2025_training_translated_en.json\n/kaggle/input/exist2025-all/EXIST2025_training_translated_es.json\n/kaggle/input/exist2025-all/EXIST2025_dev_es.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_3_minority_class_soft.json\n/kaggle/input/exist2025-all/EXIST2025_training_EASE_en.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_majority_class_hard.json\n/kaggle/input/exist2025-all/EXIST2025_dev.json\n/kaggle/input/exist2025-all/EXIST2025_training_augmented_S_es.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_gold_hard.json\n/kaggle/input/exist2025-all/EXIST2025_training_augmented_S_en.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_3_gold_soft.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_gold_soft.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_3_gold_hard.json\n/kaggle/input/exist2025/EXIST2025_training.json\n/kaggle/input/exist2025/EXIST2025_dev.json\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import wandb\n\nwandb.login(key=\"0c5f368f1f51fd942ec7bb3a1c74efb7bdc832d6\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T00:16:07.065628Z","iopub.execute_input":"2025-04-14T00:16:07.065933Z","iopub.status.idle":"2025-04-14T00:16:07.127878Z","shell.execute_reply.started":"2025-04-14T00:16:07.065913Z","shell.execute_reply":"2025-04-14T00:16:07.127192Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# import json\n# import torch\n# import numpy as np\n# from sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import MultiLabelBinarizer\n# from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n# from torch.utils.data import Dataset\n\n# # Load English and Spanish datasets separately\n# with open(\"/kaggle/input/exist2025-all/EXIST2025_training_translated_en.json\", \"r\", encoding=\"utf-8\") as f:\n#     data_en = json.load(f)\n\n# with open(\"/kaggle/input/exist2025-all/EXIST2025_training_translated_es.json\", \"r\", encoding=\"utf-8\") as f:\n#     data_es = json.load(f)\n\n# Define correct label classes\nCORRECT_LABELS = [\n    \"IDEOLOGICAL-INEQUALITY\",\n    \"MISOGYNY-NON-SEXUAL-VIOLENCE\",\n    \"OBJECTIFICATION\",\n    \"SEXUAL-VIOLENCE\",\n    \"STEREOTYPING-DOMINANCE\",\n    \"NO\"  # Represents non-sexist tweets (previously \"-\")\n]\n\n# # Function to process dataset\n# def process_data(data):\n#     tweets = []\n#     labels = []\n#     ids = []\n\n#     for entry in data.values():\n#         tweet_id = entry[\"id_EXIST\"]\n#         tweet = entry[\"tweet\"]\n#         is_sexist = any(label == \"YES\" for label in entry[\"labels_task1_1\"])  # Check if at least one annotator marked it sexist\n#         label = entry[\"labels_task1_3\"] if is_sexist else [[\"NO\"]]  # Non-sexist tweets get \"NO\"\n\n#         # Flatten labels\n#         processed_labels = [l if l != \"-\" else \"NO\" for sublist in label for l in sublist]\n\n#         # Remove \"UNKNOWN\"\n#         processed_labels = [l for l in processed_labels if l != \"UNKNOWN\"]\n\n#         # Ensure every tweet has at least one label\n#         if not processed_labels:\n#             processed_labels = [\"NO\"]\n\n#         tweets.append(tweet)\n#         labels.append(processed_labels)\n#         ids.append(tweet_id)\n\n#     return tweets, labels, ids\n\n# # Process data for English and Spanish\n# english_tweets, english_labels, english_ids = process_data(data_en)\n# spanish_tweets, spanish_labels, spanish_ids = process_data(data_es)\n\n# # MultiLabel Binarizer with Fixed Labels\n# mlb = MultiLabelBinarizer(classes=CORRECT_LABELS)  # Force correct label order\n# english_labels_bin = mlb.fit_transform(english_labels)\n# spanish_labels_bin = mlb.transform(spanish_labels)  # Use the same binarizer\n\n# label_classes = mlb.classes_\n# print(f\"Corrected Label Classes: {label_classes}\")  # Debugging\n\n# # Tokenizer\n# tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n\n# # Custom Dataset Class\n# class TweetDataset(Dataset):\n#     def __init__(self, texts, labels, ids, tokenizer, max_length=256):\n#         self.texts = texts\n#         self.labels = labels\n#         self.ids = ids\n#         self.tokenizer = tokenizer\n#         self.max_length = max_length\n\n#     def __len__(self):\n#         return len(self.texts)\n\n#     def __getitem__(self, idx):\n#         text = self.texts[idx]\n#         tweet_id = self.ids[idx]\n#         labels = torch.tensor(self.labels[idx], dtype=torch.float)\n#         encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n\n#         return {\n#             \"id\": tweet_id,\n#             \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n#             \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n#             \"labels\": labels\n#         }\n\n# # **Train-validation split**\n# def get_datasets(tweets, labels, ids):\n#     train_texts, val_texts, train_labels, vallabels, train_ids, val_ids = train_test_split(\n#         tweets, labels, ids, test_size=0.2, random_state=42\n#     )\n#     train_dataset = TweetDataset(train_texts, train_labels, train_ids, tokenizer)\n#     val_dataset = TweetDataset(val_texts, val_labels, val_ids, tokenizer)\n#     return train_dataset, val_dataset\n\n# # Split English and Spanish datasets\n# train_dataset_en, val_dataset_en = get_datasets(english_tweets, english_labels_bin, english_ids)\n# train_dataset_es, val_dataset_es = get_datasets(spanish_tweets, spanish_labels_bin, spanish_ids)\n\n# # Define and Train Model for English\n# model_en = BertForSequenceClassification.from_pretrained(\n#     \"bert-base-multilingual-cased\", \n#     num_labels=len(label_classes), \n#     problem_type=\"multi_label_classification\"\n# )\n\n# training_args_en = TrainingArguments(\n#     output_dir=\"./results/en\",\n#     evaluation_strategy=\"epoch\",\n#     save_strategy=\"epoch\",\n#     num_train_epochs=4,\n#     per_device_train_batch_size=16,\n#     per_device_eval_batch_size=16,\n#     logging_dir=\"./logs\",\n#     logging_steps=10,\n#     save_total_limit=2,\n#     load_best_model_at_end=True,\n#     metric_for_best_model=\"eval_loss\"\n# )\n\n# trainer_en = Trainer(\n#     model=model_en,\n#     args=training_args_en,\n#     train_dataset=train_dataset_en,  \n#     eval_dataset=val_dataset_en  # Now includes validation set\n# )\n\n# # Train English Model\n# trainer_en.train()\n\n# # Define and Train Model for Spanish\n# model_es = BertForSequenceClassification.from_pretrained(\n#     \"bert-base-multilingual-cased\", \n#     num_labels=len(label_classes), \n#     problem_type=\"multi_label_classification\"\n# )\n\n# training_args_es = TrainingArguments(\n#     output_dir=\"./results/es\",\n#     evaluation_strategy=\"epoch\",\n#     save_strategy=\"epoch\",\n#     num_train_epochs=3,\n#     per_device_train_batch_size=16,\n#     per_device_eval_batch_size=16,\n#     logging_dir=\"./logs\",\n#     logging_steps=10,\n#     save_total_limit=2,\n#     load_best_model_at_end=True,\n#     metric_for_best_model=\"eval_loss\"\n# )\n\n# trainer_es = Trainer(\n#     model=model_es,\n#     args=training_args_es,\n#     train_dataset=train_dataset_es,  \n#     eval_dataset=val_dataset_es  # Now includes validation set\n# )\n\n# # Train Spanish Model\n# trainer_es.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T00:56:29.873793Z","iopub.execute_input":"2025-04-14T00:56:29.874130Z","iopub.status.idle":"2025-04-14T00:56:29.880637Z","shell.execute_reply.started":"2025-04-14T00:56:29.874107Z","shell.execute_reply":"2025-04-14T00:56:29.879656Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Newer version","metadata":{}},{"cell_type":"code","source":"import json\nimport torch\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nfrom torch.utils.data import Dataset\n\n# === Load Tweets ===\nwith open(\"/kaggle/input/exist2025-all/EXIST2025_training_translated_en.json\", \"r\", encoding=\"utf-8\") as f:\n    data_en = json.load(f)\n\nwith open(\"/kaggle/input/exist2025-all/EXIST2025_training_translated_es.json\", \"r\", encoding=\"utf-8\") as f:\n    data_es = json.load(f)\n\n# === Load gold_soft_train ===\nwith open(\"/kaggle/input/exist2025-all/EXIST2025_training_task1_3_gold_soft.json\", \"r\", encoding=\"utf-8\") as f:\n    gold_soft = json.load(f)\n\n# Convert gold_soft to a dict for fast access\ngold_soft_dict = {entry[\"id\"]: entry[\"value\"] for entry in gold_soft}\n\n# Define correct label classes in fixed order\nCORRECT_LABELS = [\n    \"IDEOLOGICAL-INEQUALITY\",\n    \"MISOGYNY-NON-SEXUAL-VIOLENCE\",\n    \"OBJECTIFICATION\",\n    \"SEXUAL-VIOLENCE\",\n    \"STEREOTYPING-DOMINANCE\",\n    \"NO\"\n]\n\n# === Process Tweets with Corresponding Soft Labels ===\ndef process_data_with_soft_labels(data):\n    tweets = []\n    labels = []\n    ids = []\n\n    for entry in data.values():\n        tweet_id = entry[\"id_EXIST\"]\n        tweet = entry[\"tweet\"]\n\n        if tweet_id not in gold_soft_dict:\n            continue  # Skip if soft label not found\n\n        soft_label_dict = gold_soft_dict[tweet_id]\n\n        # Build soft label vector in fixed order\n        soft_label_vector = [soft_label_dict.get(label, 0.0) for label in CORRECT_LABELS]\n\n        tweets.append(tweet)\n        labels.append(soft_label_vector)\n        ids.append(tweet_id)\n\n    return tweets, labels, ids\n\n# Process both English and Spanish tweets\ntweets_en, labels_en, ids_en = process_data_with_soft_labels(data_en)\ntweets_es, labels_es, ids_es = process_data_with_soft_labels(data_es)\n\n# === Tokenizer ===\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n\n# === Custom Dataset Class ===\nclass TweetDataset(Dataset):\n    def __init__(self, texts, labels, ids, tokenizer, max_length=256):\n        self.texts = texts\n        self.labels = labels\n        self.ids = ids\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        tweet_id = self.ids[idx]\n        labels = torch.tensor(self.labels[idx], dtype=torch.float)\n        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n\n        return {\n            \"id\": tweet_id,\n            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n            \"labels\": labels\n        }\n\n# === Train-validation split ===\ndef get_datasets(tweets, labels, ids):\n    train_texts, val_texts, train_labels, val_labels, train_ids, val_ids = train_test_split(\n        tweets, labels, ids, test_size=0.2, random_state=42\n    )\n    train_dataset = TweetDataset(train_texts, train_labels, train_ids, tokenizer)\n    val_dataset = TweetDataset(val_texts, val_labels, val_ids, tokenizer)\n    return train_dataset, val_dataset\n\n# === Create datasets ===\ntrain_dataset_en, val_dataset_en = get_datasets(tweets_en, labels_en, ids_en)\ntrain_dataset_es, val_dataset_es = get_datasets(tweets_es, labels_es, ids_es)\n\n# === Train Model ===\ndef train_model(train_dataset, val_dataset, output_dir):\n    model = BertForSequenceClassification.from_pretrained(\n        \"bert-base-multilingual-cased\",\n        num_labels=len(CORRECT_LABELS),\n        problem_type=\"multi_label_classification\"\n    )\n\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        num_train_epochs=4,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        logging_dir=\"./logs\",\n        logging_steps=10,\n        save_total_limit=2,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\"\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset\n    )\n\n    trainer.train()\n    return trainer\n\n# === Train English and Spanish models ===\ntrainer_en = train_model(train_dataset_en, val_dataset_en, output_dir=\"./results/en\")\ntrainer_es = train_model(train_dataset_es, val_dataset_es, output_dir=\"./results/es\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T00:29:27.251160Z","iopub.execute_input":"2025-04-14T00:29:27.251541Z","iopub.status.idle":"2025-04-14T00:53:53.789514Z","shell.execute_reply.started":"2025-04-14T00:29:27.251517Z","shell.execute_reply":"2025-04-14T00:53:53.788580Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69a6a76340cd4a1193bd2df9d06b8126"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95090131fddc49459a8e1c755348fe23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1f4d1de15ab4fd1bebaacfa83105e8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa6f4731899b4da2a5375ae788d1b4f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74030c59829243d89f4142ac12f29ad1"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250414_003001-kyjqjjf4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mshoaibvohra-habib-university/huggingface/runs/kyjqjjf4' target=\"_blank\">./results/en</a></strong> to <a href='https://wandb.ai/mshoaibvohra-habib-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mshoaibvohra-habib-university/huggingface' target=\"_blank\">https://wandb.ai/mshoaibvohra-habib-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mshoaibvohra-habib-university/huggingface/runs/kyjqjjf4' target=\"_blank\">https://wandb.ai/mshoaibvohra-habib-university/huggingface/runs/kyjqjjf4</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='692' max='692' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [692/692 11:41, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.368000</td>\n      <td>0.375906</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.366900</td>\n      <td>0.365231</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.339000</td>\n      <td>0.362227</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.325700</td>\n      <td>0.364116</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='692' max='692' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [692/692 11:58, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.371000</td>\n      <td>0.384112</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.367400</td>\n      <td>0.368844</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.344200</td>\n      <td>0.366302</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.333400</td>\n      <td>0.367768</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# **Dev Testing starts from here**","metadata":{}},{"cell_type":"code","source":"with open(\"/kaggle/input/exist2025-all/EXIST2025_dev.json\", \"r\", encoding=\"utf-8\") as f:\n    dev_data = json.load(f)\n\n# Extract tweets and IDs\ndev_tweets = [entry[\"tweet\"] for entry in dev_data.values()]\ndev_ids = [entry[\"id_EXIST\"] for entry in dev_data.values()]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T00:53:56.572858Z","iopub.execute_input":"2025-04-14T00:53:56.573192Z","iopub.status.idle":"2025-04-14T00:53:56.621589Z","shell.execute_reply.started":"2025-04-14T00:53:56.573148Z","shell.execute_reply":"2025-04-14T00:53:56.619945Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import json\n\n# Load the dev dataset\nwith open(\"/kaggle/input/exist2025-all/EXIST2025_dev.json\", \"r\", encoding=\"utf-8\") as f:\n    dev_data = json.load(f)\n\n# Split into English & Spanish\nenglish_dev_tweets = []\nenglish_dev_ids = []\nspanish_dev_tweets = []\nspanish_dev_ids = []\n\nfor entry in dev_data.values():\n    tweet_id = entry[\"id_EXIST\"]\n    tweet = entry[\"tweet\"]\n    lang = entry[\"lang\"]\n\n    if lang == \"en\":\n        english_dev_tweets.append(tweet)\n        english_dev_ids.append(tweet_id)\n    elif lang == \"es\":\n        spanish_dev_tweets.append(tweet)\n        spanish_dev_ids.append(tweet_id)\n\n# Debugging: Check split sizes\nprint(f\"English Dev Samples: {len(english_dev_tweets)}\")\nprint(f\"Spanish Dev Samples: {len(spanish_dev_tweets)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T00:54:00.936125Z","iopub.execute_input":"2025-04-14T00:54:00.936532Z","iopub.status.idle":"2025-04-14T00:54:00.969196Z","shell.execute_reply.started":"2025-04-14T00:54:00.936489Z","shell.execute_reply":"2025-04-14T00:54:00.968513Z"}},"outputs":[{"name":"stdout","text":"English Dev Samples: 489\nSpanish Dev Samples: 549\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nfrom transformers import BertForSequenceClassification\n\n# Function to get the latest checkpoint\ndef get_latest_checkpoint(directory=\"./results\"):\n    checkpoints = [d for d in os.listdir(directory) if d.startswith(\"checkpoint-\")]\n    if not checkpoints:\n        raise ValueError(f\"No checkpoints found in {directory}\")\n    latest_checkpoint = sorted(checkpoints, key=lambda x: int(x.split('-')[-1]))[-1]\n    return os.path.join(directory, latest_checkpoint)\n\n# Load the best model checkpoint for English and Spanish\nlatest_checkpoint_en = get_latest_checkpoint(\"./results/en\")\nlatest_checkpoint_es = get_latest_checkpoint(\"./results/es\")\n\nprint(f\"Using latest checkpoint for English: {latest_checkpoint_en}\")\nprint(f\"Using latest checkpoint for Spanish: {latest_checkpoint_es}\")\n\n# Load models\nmodel_en = BertForSequenceClassification.from_pretrained(latest_checkpoint_en)\nmodel_es = BertForSequenceClassification.from_pretrained(latest_checkpoint_es)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T00:54:10.502049Z","iopub.execute_input":"2025-04-14T00:54:10.502418Z","iopub.status.idle":"2025-04-14T00:54:10.624544Z","shell.execute_reply.started":"2025-04-14T00:54:10.502391Z","shell.execute_reply":"2025-04-14T00:54:10.623765Z"}},"outputs":[{"name":"stdout","text":"Using latest checkpoint for English: ./results/en/checkpoint-692\nUsing latest checkpoint for Spanish: ./results/es/checkpoint-692\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# This is Soft Soft","metadata":{}},{"cell_type":"code","source":"def predict_on_dev(tweets, ids, model, tokenizer, label_classes, output_file):\n    model.eval()\n    results = []\n\n    for tweet, tweet_id in zip(tweets, ids):\n        encoding = tokenizer(tweet, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n\n        with torch.no_grad():\n            outputs = model(**encoding)\n\n        logits = outputs.logits.squeeze()\n        probs = torch.sigmoid(logits).cpu().numpy()\n\n        # Convert probabilities to dictionary format and sort by highest probability\n        soft_label_dict = {label_classes[i]: float(probs[i]) for i in range(len(label_classes))}\n        sorted_soft_label_dict = dict(sorted(soft_label_dict.items(), key=lambda item: item[1], reverse=True))  # Sort descending\n\n        results.append({\n            \"test_case\": \"EXIST2025\",\n            \"id\": tweet_id,\n            \"value\": sorted_soft_label_dict  # Rename \"soft_label\" to \"value\" and sort it\n        })\n\n    # Save results\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(results, f, indent=4)\n\n    print(f\"Predictions saved to {output_file}\")\n    \nlabel_classes = CORRECT_LABELS\n\n\n# Run predictions\npredict_on_dev(english_dev_tweets, english_dev_ids, model_en, tokenizer, label_classes, \"EXIST2025_dev_predictions_en.json\")\npredict_on_dev(spanish_dev_tweets, spanish_dev_ids, model_es, tokenizer, label_classes, \"EXIST2025_dev_predictions_es.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T00:59:08.648891Z","iopub.execute_input":"2025-04-14T00:59:08.649356Z","iopub.status.idle":"2025-04-14T01:03:39.636961Z","shell.execute_reply.started":"2025-04-14T00:59:08.649314Z","shell.execute_reply":"2025-04-14T01:03:39.636238Z"}},"outputs":[{"name":"stdout","text":"Predictions saved to EXIST2025_dev_predictions_en.json\nPredictions saved to EXIST2025_dev_predictions_es.json\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# This be Hard Hard","metadata":{}},{"cell_type":"code","source":"def predict_hard_labels_from_soft_model(tweets, ids, model, tokenizer, label_classes, output_file, threshold=0.33):\n    \"\"\"\n    Uses the soft model to predict hard labels by applying a threshold.\n    - Labels are assigned if their probability > threshold.\n    - If no labels pass the threshold, assigns \"NO\".\n    \"\"\"\n    model.eval()\n    results = []\n\n    for tweet, tweet_id in zip(tweets, ids):\n        encoding = tokenizer(tweet, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n\n        with torch.no_grad():\n            outputs = model(**encoding)\n\n        logits = outputs.logits.squeeze()\n        probs = torch.sigmoid(logits).cpu().numpy()\n\n        # Convert probabilities to hard labels using threshold\n        hard_labels = [label_classes[i] for i, prob in enumerate(probs) if prob > threshold]\n\n        # If no labels meet the threshold, assign \"NO\"\n        if not hard_labels:\n            hard_labels = [\"NO\"]\n\n        results.append({\n            \"test_case\": \"EXIST2025\",\n            \"id\": tweet_id,\n            \"value\": hard_labels  # Final hard labels\n        })\n\n    # Save results\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(results, f, indent=4)\n\n    print(f\"Hard label predictions saved to {output_file}\")\n\n# Run hard label prediction using soft model\npredict_hard_labels_from_soft_model(english_dev_tweets, english_dev_ids, model_en, tokenizer, label_classes, \"EXIST2025_dev_predictions_hard_en.json\")\npredict_hard_labels_from_soft_model(spanish_dev_tweets, spanish_dev_ids, model_es, tokenizer, label_classes, \"EXIST2025_dev_predictions_hard_es.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:26:26.719684Z","iopub.execute_input":"2025-04-14T01:26:26.720063Z","iopub.status.idle":"2025-04-14T01:30:54.200676Z","shell.execute_reply.started":"2025-04-14T01:26:26.720031Z","shell.execute_reply":"2025-04-14T01:30:54.199799Z"}},"outputs":[{"name":"stdout","text":"Hard label predictions saved to EXIST2025_dev_predictions_hard_en.json\nHard label predictions saved to EXIST2025_dev_predictions_hard_es.json\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Merging soft models ka dev set predictions","metadata":{}},{"cell_type":"code","source":"import json\n\n# Load the Spanish predictions\nwith open(\"/kaggle/working/EXIST2025_dev_predictions_es.json\", \"r\", encoding=\"utf-8\") as f:\n    es_data = json.load(f)\n\n# Load the English predictions\nwith open(\"/kaggle/working/EXIST2025_dev_predictions_en.json\", \"r\", encoding=\"utf-8\") as f:\n    en_data = json.load(f)\n\n# Assuming both files contain lists of predictions, merge them\nif isinstance(es_data, list) and isinstance(en_data, list):\n    merged_data = es_data + en_data\nelse:\n    raise ValueError(\"JSON structure is not a list. Ensure both files contain lists.\")\n\n# Save to a new file\noutput_filename = \"EXIST2025_dev_predictions_merged_soft.json\"\nwith open(output_filename, \"w\", encoding=\"utf-8\") as f:\n    json.dump(merged_data, f, indent=4, ensure_ascii=False)\n\nprint(f\"Merging complete! Saved to {output_filename}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:03:54.298587Z","iopub.execute_input":"2025-04-14T01:03:54.298875Z","iopub.status.idle":"2025-04-14T01:03:54.332858Z","shell.execute_reply.started":"2025-04-14T01:03:54.298854Z","shell.execute_reply":"2025-04-14T01:03:54.332082Z"}},"outputs":[{"name":"stdout","text":"Merging complete! Saved to EXIST2025_dev_predictions_merged_soft.json\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import json\nimport numpy as np\n\n# Load your predictions file\nwith open('EXIST2025_dev_predictions_merged_soft.json', 'r') as f:\n    predictions = json.load(f)\n\n# Define the snapping values (multiples of 1/6)\nsnap_vals = np.array([i / 6 for i in range(7)])  # [0.0, 0.1667, ..., 1.0]\n\ndef snap_to_nearest_sixth(value):\n    return float(snap_vals[np.argmin(np.abs(snap_vals - value))])\n\n# Snap each value in the 'value' dict\nfor entry in predictions:\n    entry['value'] = {k: snap_to_nearest_sixth(v) for k, v in entry['value'].items()}\n\n# Save the snapped predictions to a new file\nwith open('EXIST2025_dev_predictions_snapped_soft.json', 'w') as f:\n    json.dump(predictions, f, indent=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:12:54.783904Z","iopub.execute_input":"2025-04-14T01:12:54.784288Z","iopub.status.idle":"2025-04-14T01:12:54.841691Z","shell.execute_reply.started":"2025-04-14T01:12:54.784262Z","shell.execute_reply":"2025-04-14T01:12:54.840824Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# Merging hard models ka dev set predictions","metadata":{}},{"cell_type":"code","source":"import json\n\n# Load the Spanish predictions\nwith open(\"/kaggle/working/EXIST2025_dev_predictions_hard_es.json\", \"r\", encoding=\"utf-8\") as f:\n    es_data = json.load(f)\n\n# Load the English predictions\nwith open(\"/kaggle/working/EXIST2025_dev_predictions_hard_en.json\", \"r\", encoding=\"utf-8\") as f:\n    en_data = json.load(f)\n\n# Assuming both files contain lists of predictions, merge them\nif isinstance(es_data, list) and isinstance(en_data, list):\n    merged_data = es_data + en_data\nelse:\n    raise ValueError(\"JSON structure is not a list. Ensure both files contain lists.\")\n\n# Save to a new file\noutput_filename = \"EXIST2025_dev_predictions_merged_hard.json\"\nwith open(output_filename, \"w\", encoding=\"utf-8\") as f:\n    json.dump(merged_data, f, indent=4, ensure_ascii=False)\n\nprint(f\"Merging complete! Saved to {output_filename}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:30:54.201755Z","iopub.execute_input":"2025-04-14T01:30:54.202066Z","iopub.status.idle":"2025-04-14T01:30:54.220841Z","shell.execute_reply.started":"2025-04-14T01:30:54.202044Z","shell.execute_reply":"2025-04-14T01:30:54.219996Z"}},"outputs":[{"name":"stdout","text":"Merging complete! Saved to EXIST2025_dev_predictions_merged_hard.json\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# Metric Calculation","metadata":{}},{"cell_type":"code","source":"pip install pyevall","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:04:40.800229Z","iopub.execute_input":"2025-04-14T01:04:40.800573Z","iopub.status.idle":"2025-04-14T01:05:09.892343Z","shell.execute_reply.started":"2025-04-14T01:04:40.800547Z","shell.execute_reply":"2025-04-14T01:05:09.891331Z"}},"outputs":[{"name":"stdout","text":"Collecting pyevall\n  Downloading PyEvALL-0.1.78.tar.gz (39 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting jsbeautifier==1.14.9 (from pyevall)\n  Downloading jsbeautifier-1.14.9.tar.gz (75 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: jsonschema==4.23.0 in /usr/local/lib/python3.10/dist-packages (from pyevall) (4.23.0)\nRequirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (from pyevall) (1.26.4)\nRequirement already satisfied: pandas==2.2.3 in /usr/local/lib/python3.10/dist-packages (from pyevall) (2.2.3)\nCollecting setuptools==69.5.1 (from pyevall)\n  Downloading setuptools-69.5.1-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.10/dist-packages (from pyevall) (0.9.0)\nRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from jsbeautifier==1.14.9->pyevall) (1.17.0)\nCollecting editorconfig>=0.12.2 (from jsbeautifier==1.14.9->pyevall)\n  Downloading EditorConfig-0.17.0-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.23.0->pyevall) (25.1.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.23.0->pyevall) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.23.0->pyevall) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.23.0->pyevall) (0.22.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->pyevall) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->pyevall) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->pyevall) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->pyevall) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->pyevall) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->pyevall) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->pyevall) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->pyevall) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->pyevall) (2025.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy==1.26.4->pyevall) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy==1.26.4->pyevall) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy==1.26.4->pyevall) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy==1.26.4->pyevall) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy==1.26.4->pyevall) (2024.2.0)\nDownloading setuptools-69.5.1-py3-none-any.whl (894 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m894.6/894.6 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading EditorConfig-0.17.0-py3-none-any.whl (16 kB)\nBuilding wheels for collected packages: pyevall, jsbeautifier\n  Building wheel for pyevall (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pyevall: filename=PyEvALL-0.1.78-py3-none-any.whl size=34777 sha256=b6509b169da725acfcc9b3e75fecd5cafff970bb0b6e201f0aa38769348154be\n  Stored in directory: /root/.cache/pip/wheels/f0/3a/51/f8c268e67356c15a602eef8ac7a5e18ba4677b4ec8b45b8a25\n  Building wheel for jsbeautifier (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for jsbeautifier: filename=jsbeautifier-1.14.9-py3-none-any.whl size=94157 sha256=53111ec2c7e0f439f2be4ae9c13588a5aa3756bb5f783e58680ed6048e72d6b0\n  Stored in directory: /root/.cache/pip/wheels/c4/5c/25/09f8b2e8dddb4fc3d70817c67b375a9069a2628847ffbdfc65\nSuccessfully built pyevall jsbeautifier\nInstalling collected packages: editorconfig, setuptools, jsbeautifier, pyevall\n  Attempting uninstall: setuptools\n    Found existing installation: setuptools 75.1.0\n    Uninstalling setuptools-75.1.0:\n      Successfully uninstalled setuptools-75.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed editorconfig-0.17.0 jsbeautifier-1.14.9 pyevall-0.1.78 setuptools-69.5.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# This is hard metrics","metadata":{}},{"cell_type":"code","source":"from pyevall.evaluation import PyEvALLEvaluation\nfrom pyevall.utils.utils import PyEvALLUtils\n\npredictions = \"/kaggle/working/EXIST2025_dev_predictions_merged_hard.json\"         \ngold = \"/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_gold_hard.json\" \ntest = PyEvALLEvaluation() \nparams= dict() \nparams[PyEvALLUtils.PARAM_REPORT]= PyEvALLUtils.PARAM_OPTION_REPORT_EMBEDDED  \n# metrics=[\"ICMSoft\", \"ICMSoftNorm\", \"CrossEntropy\"]     # for soft    \nmetrics=[\"ICM\", \"ICMNorm\" ,\"FMeasure\"] \nTASK1_3_HIERARCHY = {\"YES\":[\"IDEOLOGICAL-INEQUALITY\",\"STEREOTYPING-DOMINANCE\",\"OBJECTIFICATION\", \"SEXUAL-VIOLENCE\", \"MISOGYNY-NON-SEXUAL-VIOLENCE\"], \"NO\":[]}\nparams[PyEvALLUtils.PARAM_HIERARCHY]= TASK1_3_HIERARCHY  \nreport= test.evaluate(predictions, gold, metrics, **params) \nreport.print_report()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:30:54.221834Z","iopub.execute_input":"2025-04-14T01:30:54.222043Z","iopub.status.idle":"2025-04-14T01:30:56.480868Z","shell.execute_reply.started":"2025-04-14T01:30:54.222025Z","shell.execute_reply":"2025-04-14T01:30:56.479898Z"}},"outputs":[{"name":"stdout","text":"2025-04-14 01:30:54,227 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM', 'ICMNorm', 'FMeasure']\n2025-04-14 01:30:54,406 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-04-14 01:30:54,954 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Normalized evaluation method\n2025-04-14 01:30:54,958 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-04-14 01:30:55,488 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-04-14 01:30:56,002 - pyevall.metrics.metrics - INFO -             evaluate() - Executing fmeasure evaluation method\n{\n  \"metrics\": {\n    \"ICM\": {\n      \"name\": \"Information Contrast model\",\n      \"acronym\": \"ICM\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": -0.30018264585449583\n        }],\n        \"average_per_test_case\": -0.30018264585449583\n      }\n    },\n    \"ICMNorm\": {\n      \"name\": \"Normalized Information Contrast Model\",\n      \"acronym\": \"ICM-Norm\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 0.4331383871773812\n        }],\n        \"average_per_test_case\": 0.4331383871773812\n      }\n    },\n    \"FMeasure\": {\n      \"name\": \"F-Measure\",\n      \"acronym\": \"F1\",\n      \"description\": \"Coming soon!\\\\nThe evaluation WARNING.\",\n      \"status\": \"WARNING\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"classes\": {\n            \"IDEOLOGICAL-INEQUALITY\": 0.4709141274238227,\n            \"STEREOTYPING-DOMINANCE\": 0.5346534653465347,\n            \"MISOGYNY-NON-SEXUAL-VIOLENCE\": 0.22115384615384612,\n            \"NO\": 0.7544303797468355,\n            \"SEXUAL-VIOLENCE\": 0.4120603015075377,\n            \"OBJECTIFICATION\": 0.40939597315436244\n          },\n          \"average\": 0.46710134888882315\n        }],\n        \"average_per_test_case\": 0.46710134888882315\n      },\n      \"preconditions\": {\n        \"METRIC_PRECONDITION_HIERARCHY_NOT_VALID_FOR_METRIC\": {\n          \"name\": \"METRIC_PRECONDITION_HIERARCHY_NOT_VALID_FOR_METRIC\",\n          \"description\": \"The hierarchy is provided for the evaluation but this metric does not allow to use it. Hierarchy is ignored.\\\\nThe metric name is: F-Measure.\\\\nTest case(s) name: EXIST2025.\",\n          \"status\": \"WARNING\",\n          \"test_cases\": [\"EXIST2025\"]\n        }\n      }\n    }\n  },\n  \"files\": {\n    \"EXIST2025_dev_predictions_merged_hard.json\": {\n      \"name\": \"EXIST2025_dev_predictions_merged_hard.json\",\n      \"status\": \"OK\",\n      \"gold\": false,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_predictions_merged_hard.json.\",\n      \"errors\": {}\n    },\n    \"EXIST2025_dev_task1_3_gold_hard.json\": {\n      \"name\": \"EXIST2025_dev_task1_3_gold_hard.json\",\n      \"status\": \"OK\",\n      \"gold\": true,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_task1_3_gold_hard.json.\",\n      \"errors\": {}\n    }\n  }\n}\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# These are soft metrics","metadata":{}},{"cell_type":"code","source":"from pyevall.evaluation import PyEvALLEvaluation\nfrom pyevall.utils.utils import PyEvALLUtils\n\n# Define file paths\npredictions = \"/kaggle/working/EXIST2025_dev_predictions_snapped_soft.json\"  # Change to your actual prediction file\ngold = \"/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_gold_soft.json\"   # Change to your actual gold file\n\n# Define hierarchical structure for subtask 1.3\nTASK1_3_HIERARCHY = {\n    \"YES\": [\"IDEOLOGICAL-INEQUALITY\", \"STEREOTYPING-DOMINANCE\",\n            \"OBJECTIFICATION\", \"SEXUAL-VIOLENCE\", \"MISOGYNY-NON-SEXUAL-VIOLENCE\"],\n    \"NO\": []\n}\n\n# Initialize PyEvALL evaluation\nevaluator = PyEvALLEvaluation()\n\n# Set evaluation parameters\nparams = dict()\nparams[PyEvALLUtils.PARAM_HIERARCHY] = TASK1_3_HIERARCHY\nparams[PyEvALLUtils.PARAM_REPORT] = PyEvALLUtils.PARAM_OPTION_REPORT_EMBEDDED  # Embedded report\n\n# Define evaluation metrics\n# metrics = [\"ICM\", \"ICMNorm\", \"FMeasure\"]\nmetrics=[\"ICMSoft\", \"ICMSoftNorm\"]\n\n# Run evaluation\nreport = evaluator.evaluate(predictions, gold, metrics, **params)\n\n# Print evaluation report\nreport.print_report()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:17:36.329012Z","iopub.execute_input":"2025-04-14T01:17:36.329404Z","iopub.status.idle":"2025-04-14T01:17:41.012112Z","shell.execute_reply.started":"2025-04-14T01:17:36.329374Z","shell.execute_reply":"2025-04-14T01:17:41.011260Z"}},"outputs":[{"name":"stdout","text":"2025-04-14 01:17:36,335 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICMSoft', 'ICMSoftNorm']\n2025-04-14 01:17:36,685 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Soft evaluation method\n2025-04-14 01:17:38,174 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM-Soft Normalized evaluation method\n2025-04-14 01:17:38,177 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Soft evaluation method\n2025-04-14 01:17:39,617 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Soft evaluation method\n{\n  \"metrics\": {\n    \"ICMSoft\": {\n      \"name\": \"Information Contrast Model Soft\",\n      \"acronym\": \"ICM-Soft\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": -2.588298008906674\n        }],\n        \"average_per_test_case\": -2.588298008906674\n      }\n    },\n    \"ICMSoftNorm\": {\n      \"name\": \"Normalized Information Contrast Model Soft\",\n      \"acronym\": \"ICM-Soft-Norm\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 0.36281778598075376\n        }],\n        \"average_per_test_case\": 0.36281778598075376\n      }\n    }\n  },\n  \"files\": {\n    \"EXIST2025_dev_predictions_snapped_soft.json\": {\n      \"name\": \"EXIST2025_dev_predictions_snapped_soft.json\",\n      \"status\": \"OK\",\n      \"gold\": false,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_predictions_snapped_soft.json.\",\n      \"errors\": {}\n    },\n    \"EXIST2025_dev_task1_3_gold_soft.json\": {\n      \"name\": \"EXIST2025_dev_task1_3_gold_soft.json\",\n      \"status\": \"OK\",\n      \"gold\": true,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_task1_3_gold_soft.json.\",\n      \"errors\": {}\n    }\n  }\n}\n","output_type":"stream"}],"execution_count":19}]}