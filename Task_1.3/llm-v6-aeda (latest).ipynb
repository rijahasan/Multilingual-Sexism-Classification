{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11142046,"sourceType":"datasetVersion","datasetId":6950309},{"sourceId":11540096,"sourceType":"datasetVersion","datasetId":6963199}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-24T07:57:35.787066Z","iopub.execute_input":"2025-04-24T07:57:35.787364Z","iopub.status.idle":"2025-04-24T07:57:35.832383Z","shell.execute_reply.started":"2025-04-24T07:57:35.787343Z","shell.execute_reply":"2025-04-24T07:57:35.831661Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/exist2025/EXIST2025_training.json\n/kaggle/input/exist2025/EXIST2025_dev.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_2_gold_hard.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_majority_class_soft.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_minority_class_hard.json\n/kaggle/input/exist2025-all/training_gold_augmented_en.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_1_gold_soft.json\n/kaggle/input/exist2025-all/EXIST2025_training_augmented_gold_en_v3.json\n/kaggle/input/exist2025-all/EXIST2025_training_augmented_gold_AEDA_en.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_2_gold_soft.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_3_minority_class_hard.json\n/kaggle/input/exist2025-all/EXIST2025_training.json\n/kaggle/input/exist2025-all/EXIST2025_training_augmented_S2_es.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_1_gold_hard.json\n/kaggle/input/exist2025-all/EXIST2025_dev_en.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_minority_class_soft.json\n/kaggle/input/exist2025-all/EXIST2025_training_EASE_es.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_3_majority_class_soft.json\n/kaggle/input/exist2025-all/EXIST2025_training_augmented_AEDA_en.json\n/kaggle/input/exist2025-all/EXIST2025_training_augmented_gold_es_v3.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_3_majority_class_hard.json\n/kaggle/input/exist2025-all/EXIST2025_training_translated_en.json\n/kaggle/input/exist2025-all/EXIST2025_training_augmented_S_es_v3.json\n/kaggle/input/exist2025-all/EXIST2025_training_translated_es.json\n/kaggle/input/exist2025-all/EXIST2025_dev_es.json\n/kaggle/input/exist2025-all/EXIST2025_training_augmented_AEDA_es.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_3_minority_class_soft.json\n/kaggle/input/exist2025-all/EXIST2025_training_EASE_en.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_majority_class_hard.json\n/kaggle/input/exist2025-all/EXIST2025_dev.json\n/kaggle/input/exist2025-all/EXIST2025_training_augmented_S2_en.json\n/kaggle/input/exist2025-all/EXIST2025_training_augmented_S_en_v3.json\n/kaggle/input/exist2025-all/training_gold_augmented_es.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_2_gold_soft.json\n/kaggle/input/exist2025-all/EXIST2025_training_augmented_S_es.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_2_gold_hard.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_gold_hard.json\n/kaggle/input/exist2025-all/EXIST2025_training_augmented_S_en.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_3_gold_soft.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_gold_soft.json\n/kaggle/input/exist2025-all/EXIST2025_training_augmented_gold_AEDA_es.json\n/kaggle/input/exist2025-all/EXIST2025_training_task1_3_gold_hard.json\n/kaggle/input/exist2025-all/EXIST2025_dev_task1_1_gold_soft.json\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import wandb\n\nwandb.login(key=\"0c5f368f1f51fd942ec7bb3a1c74efb7bdc832d6\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T07:57:39.615595Z","iopub.execute_input":"2025-04-24T07:57:39.615903Z","iopub.status.idle":"2025-04-24T07:57:39.623032Z","shell.execute_reply.started":"2025-04-24T07:57:39.615880Z","shell.execute_reply":"2025-04-24T07:57:39.622348Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"CORRECT_LABELS = [\n    \"IDEOLOGICAL-INEQUALITY\",\n    \"MISOGYNY-NON-SEXUAL-VIOLENCE\",\n    \"OBJECTIFICATION\",\n    \"SEXUAL-VIOLENCE\",\n    \"STEREOTYPING-DOMINANCE\",\n    \"NO\"  # Represents non-sexist tweets (previously \"-\")\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T07:57:42.769374Z","iopub.execute_input":"2025-04-24T07:57:42.769724Z","iopub.status.idle":"2025-04-24T07:57:42.774266Z","shell.execute_reply.started":"2025-04-24T07:57:42.769697Z","shell.execute_reply":"2025-04-24T07:57:42.773429Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"# Newer version","metadata":{}},{"cell_type":"code","source":"import json\nimport torch\nimport random\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer\nfrom torch.utils.data import Dataset\nfrom collections import defaultdict\n\n# === AEDA helper ===\nPUNCTUATIONS = ['.', ',', '!', '?', ';', ':']\ndef aeda(sentence, num_insertions=3):\n    words = sentence.split()\n    if not words:\n        return sentence\n    new_words = words.copy()\n    for _ in range(num_insertions):\n        insert_pos = random.randint(0, len(new_words))\n        punct = random.choice(PUNCTUATIONS)\n        new_words.insert(insert_pos, punct)\n    return ' '.join(new_words)\n\n# === Load Data ===\nwith open(\"/kaggle/input/exist2025-all/EXIST2025_training_translated_en.json\", \"r\", encoding=\"utf-8\") as f:\n    data_en = json.load(f)\nwith open(\"/kaggle/input/exist2025-all/EXIST2025_training_translated_es.json\", \"r\", encoding=\"utf-8\") as f:\n    data_es = json.load(f)\nwith open(\"/kaggle/input/exist2025-all/EXIST2025_training_task1_3_gold_soft.json\", \"r\", encoding=\"utf-8\") as f:\n    gold_soft = json.load(f)\n\ngold_soft_dict = {entry[\"id\"]: entry[\"value\"] for entry in gold_soft}\nlabel_classes = [\n    \"IDEOLOGICAL-INEQUALITY\",\n    \"MISOGYNY-NON-SEXUAL-VIOLENCE\",\n    \"OBJECTIFICATION\",\n    \"SEXUAL-VIOLENCE\",\n    \"STEREOTYPING-DOMINANCE\",\n    \"NO\"  # Represents non-sexist tweets (previously \"-\")\n]\n\n# === Count Label Distribution ===\nlabel_counts = defaultdict(int)\nfor soft in gold_soft_dict.values():\n    max_label = max(soft, key=soft.get)\n    label_counts[max_label] += 1\n\n# === Identify underrepresented labels (you can tune this threshold) ===\navg_count = np.mean(list(label_counts.values()))\nunderrepresented_labels = [label for label, count in label_counts.items() if count < avg_count]\n\n# === Process Tweets & Augment Underrepresented Only ===\ndef process_data_with_soft_labels(data, augment=True, augment_n=2):\n    tweets, labels, ids = [], [], []\n\n    for entry in data.values():\n        tweet_id = entry[\"id_EXIST\"]\n        tweet = entry[\"tweet\"]\n\n        if tweet_id not in gold_soft_dict:\n            continue\n\n        soft_label_dict = gold_soft_dict[tweet_id]\n        soft_label_vector = [soft_label_dict.get(label, 0.0) for label in label_classes]\n\n        # Original tweet\n        tweets.append(tweet)\n        labels.append(soft_label_vector)\n        ids.append(tweet_id)\n\n        # Determine primary label\n        main_label = max(soft_label_dict, key=soft_label_dict.get)\n\n        # Augment only if underrepresented\n        if augment and main_label in underrepresented_labels:\n            for i in range(augment_n):\n                augmented_tweet = aeda(tweet)\n                tweets.append(augmented_tweet)\n                labels.append(soft_label_vector)\n                ids.append(f\"{tweet_id}_aug{i+1}\")\n\n    return tweets, labels, ids\n\n# Process English and Spanish data\ntweets_en, labels_en, ids_en = process_data_with_soft_labels(data_en)\ntweets_es, labels_es, ids_es = process_data_with_soft_labels(data_es)\n\nclass TweetDataset(Dataset):\n    def __init__(self, texts, labels, ids, tokenizer, max_length=256):\n        self.texts = texts\n        self.labels = labels\n        self.ids = ids\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        tweet_id = self.ids[idx]\n        label = torch.tensor(self.labels[idx], dtype=torch.float)\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n\n        return {\n            \"id\": tweet_id,\n            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n            \"labels\": label\n        }\n\n# === Train-validation split ===\ndef get_datasets(tweets, labels, ids):\n    train_texts, val_texts, train_labels, val_labels, train_ids, val_ids = train_test_split(\n        tweets, labels, ids, test_size=0.2, random_state=42\n    )\n    train_dataset = TweetDataset(train_texts, train_labels, train_ids, tokenizer)\n    val_dataset = TweetDataset(val_texts, val_labels, val_ids, tokenizer)\n    return train_dataset, val_dataset\n\n# === Create datasets ===\ntrain_dataset_en, val_dataset_en = get_datasets(tweets_en, labels_en, ids_en)\ntrain_dataset_es, val_dataset_es = get_datasets(tweets_es, labels_es, ids_es)\n\nprint(f\"English train set size: {len(train_dataset_en)} (with selective augmentation)\")\nprint(f\"Spanish train set size: {len(train_dataset_es)} (with selective augmentation)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T07:58:59.213130Z","iopub.execute_input":"2025-04-24T07:58:59.213455Z","iopub.status.idle":"2025-04-24T07:59:00.982497Z","shell.execute_reply.started":"2025-04-24T07:58:59.213409Z","shell.execute_reply":"2025-04-24T07:59:00.981580Z"}},"outputs":[{"name":"stdout","text":"English train set size: 9731 (with selective augmentation)\nSpanish train set size: 9731 (with selective augmentation)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import json\nimport torch\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nfrom torch.utils.data import Dataset\n\n\n# Define correct label classes in fixed order\nCORRECT_LABELS = [\n    \"IDEOLOGICAL-INEQUALITY\",\n    \"MISOGYNY-NON-SEXUAL-VIOLENCE\",\n    \"OBJECTIFICATION\",\n    \"SEXUAL-VIOLENCE\",\n    \"STEREOTYPING-DOMINANCE\",\n    \"NO\"\n]\n\n\n# === Train Model ===\ndef train_model(train_dataset, val_dataset, output_dir):\n    model = BertForSequenceClassification.from_pretrained(\n        \"bert-base-multilingual-cased\",\n        num_labels=len(CORRECT_LABELS),\n        problem_type=\"multi_label_classification\"\n    )\n\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        num_train_epochs=5,\n        per_device_train_batch_size=32,\n        per_device_eval_batch_size=32,\n        logging_dir=\"./logs\",\n        logging_steps=10,\n        save_total_limit=2,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\"\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset\n    )\n\n    trainer.train()\n    return trainer\n\n# === Train English and Spanish models ===\ntrainer_en = train_model(train_dataset_en, val_dataset_en, output_dir=\"./results/en\")\ntrainer_es = train_model(train_dataset_es, val_dataset_es, output_dir=\"./results/es\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T08:00:17.550603Z","iopub.execute_input":"2025-04-24T08:00:17.550997Z","iopub.status.idle":"2025-04-24T08:42:53.851941Z","shell.execute_reply.started":"2025-04-24T08:00:17.550966Z","shell.execute_reply":"2025-04-24T08:42:53.851155Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1220' max='1220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1220/1220 21:08, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.411700</td>\n      <td>0.405875</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.378200</td>\n      <td>0.385621</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.344900</td>\n      <td>0.373079</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.359700</td>\n      <td>0.368770</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1220' max='1220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1220/1220 21:12, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.425700</td>\n      <td>0.418609</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.385100</td>\n      <td>0.392061</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.350300</td>\n      <td>0.379482</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.360900</td>\n      <td>0.372128</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"# **Dev Testing starts from here**","metadata":{}},{"cell_type":"code","source":"with open(\"/kaggle/input/exist2025-all/EXIST2025_dev.json\", \"r\", encoding=\"utf-8\") as f:\n    dev_data = json.load(f)\n\n# Extract tweets and IDs\ndev_tweets = [entry[\"tweet\"] for entry in dev_data.values()]\ndev_ids = [entry[\"id_EXIST\"] for entry in dev_data.values()]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T08:43:59.353537Z","iopub.execute_input":"2025-04-24T08:43:59.353862Z","iopub.status.idle":"2025-04-24T08:43:59.408220Z","shell.execute_reply.started":"2025-04-24T08:43:59.353839Z","shell.execute_reply":"2025-04-24T08:43:59.407455Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"import json\n\n# Load the dev dataset\nwith open(\"/kaggle/input/exist2025-all/EXIST2025_dev.json\", \"r\", encoding=\"utf-8\") as f:\n    dev_data = json.load(f)\n\n# Split into English & Spanish\nenglish_dev_tweets = []\nenglish_dev_ids = []\nspanish_dev_tweets = []\nspanish_dev_ids = []\n\nfor entry in dev_data.values():\n    tweet_id = entry[\"id_EXIST\"]\n    tweet = entry[\"tweet\"]\n    lang = entry[\"lang\"]\n\n    if lang == \"en\":\n        english_dev_tweets.append(tweet)\n        english_dev_ids.append(tweet_id)\n    elif lang == \"es\":\n        spanish_dev_tweets.append(tweet)\n        spanish_dev_ids.append(tweet_id)\n\n# Debugging: Check split sizes\nprint(f\"English Dev Samples: {len(english_dev_tweets)}\")\nprint(f\"Spanish Dev Samples: {len(spanish_dev_tweets)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T08:44:03.300048Z","iopub.execute_input":"2025-04-24T08:44:03.300375Z","iopub.status.idle":"2025-04-24T08:44:03.337460Z","shell.execute_reply.started":"2025-04-24T08:44:03.300339Z","shell.execute_reply":"2025-04-24T08:44:03.336354Z"}},"outputs":[{"name":"stdout","text":"English Dev Samples: 489\nSpanish Dev Samples: 549\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"import os\nfrom transformers import BertForSequenceClassification\n\n# Function to get the latest checkpoint\ndef get_latest_checkpoint(directory=\"./results\"):\n    checkpoints = [d for d in os.listdir(directory) if d.startswith(\"checkpoint-\")]\n    if not checkpoints:\n        raise ValueError(f\"No checkpoints found in {directory}\")\n    latest_checkpoint = sorted(checkpoints, key=lambda x: int(x.split('-')[-1]))[-1]\n    return os.path.join(directory, latest_checkpoint)\n\n# Load the best model checkpoint for English and Spanish\nlatest_checkpoint_en = get_latest_checkpoint(\"./results/en\")\nlatest_checkpoint_es = get_latest_checkpoint(\"./results/es\")\n\nprint(f\"Using latest checkpoint for English: {latest_checkpoint_en}\")\nprint(f\"Using latest checkpoint for Spanish: {latest_checkpoint_es}\")\n\n# Load models\nmodel_en = BertForSequenceClassification.from_pretrained(latest_checkpoint_en)\nmodel_es = BertForSequenceClassification.from_pretrained(latest_checkpoint_es)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T08:44:10.160501Z","iopub.execute_input":"2025-04-24T08:44:10.160843Z","iopub.status.idle":"2025-04-24T08:44:10.605522Z","shell.execute_reply.started":"2025-04-24T08:44:10.160816Z","shell.execute_reply":"2025-04-24T08:44:10.604881Z"}},"outputs":[{"name":"stdout","text":"Using latest checkpoint for English: ./results/en/checkpoint-1220\nUsing latest checkpoint for Spanish: ./results/es/checkpoint-1220\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"# This is Soft Soft","metadata":{}},{"cell_type":"code","source":"def predict_on_dev(tweets, ids, model, tokenizer, label_classes, output_file):\n    model.eval()\n    results = []\n\n    for tweet, tweet_id in zip(tweets, ids):\n        encoding = tokenizer(tweet, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n\n        with torch.no_grad():\n            outputs = model(**encoding)\n\n        logits = outputs.logits.squeeze()\n        probs = torch.sigmoid(logits).cpu().numpy()\n\n        # Convert probabilities to dictionary format and sort by highest probability\n        soft_label_dict = {label_classes[i]: float(probs[i]) for i in range(len(label_classes))}\n        sorted_soft_label_dict = dict(sorted(soft_label_dict.items(), key=lambda item: item[1], reverse=True))  # Sort descending\n\n        results.append({\n            \"test_case\": \"EXIST2025\",\n            \"id\": tweet_id,\n            \"value\": sorted_soft_label_dict  # Rename \"soft_label\" to \"value\" and sort it\n        })\n\n    # Save results\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(results, f, indent=4)\n\n    print(f\"Predictions saved to {output_file}\")\n    \nlabel_classes = CORRECT_LABELS\n\n\n# Run predictions\npredict_on_dev(english_dev_tweets, english_dev_ids, model_en, tokenizer, label_classes, \"EXIST2025_dev_predictions_en.json\")\npredict_on_dev(spanish_dev_tweets, spanish_dev_ids, model_es, tokenizer, label_classes, \"EXIST2025_dev_predictions_es.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T08:44:14.871836Z","iopub.execute_input":"2025-04-24T08:44:14.872166Z","iopub.status.idle":"2025-04-24T08:48:42.532724Z","shell.execute_reply.started":"2025-04-24T08:44:14.872142Z","shell.execute_reply":"2025-04-24T08:48:42.531868Z"}},"outputs":[{"name":"stdout","text":"Predictions saved to EXIST2025_dev_predictions_en.json\nPredictions saved to EXIST2025_dev_predictions_es.json\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"# This be Hard Hard","metadata":{}},{"cell_type":"code","source":"def predict_hard_labels_from_soft_model(tweets, ids, model, tokenizer, label_classes, output_file, threshold=0.33):\n    \"\"\"\n    Uses the soft model to predict hard labels by applying a threshold.\n    - Labels are assigned if their probability > threshold.\n    - If no labels pass the threshold, assigns \"NO\".\n    \"\"\"\n    model.eval()\n    results = []\n\n    for tweet, tweet_id in zip(tweets, ids):\n        encoding = tokenizer(tweet, truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n\n        with torch.no_grad():\n            outputs = model(**encoding)\n\n        logits = outputs.logits.squeeze()\n        probs = torch.sigmoid(logits).cpu().numpy()\n\n        # Convert probabilities to hard labels using threshold\n        hard_labels = [label_classes[i] for i, prob in enumerate(probs) if prob > threshold]\n\n        # If no labels meet the threshold, assign \"NO\"\n        if not hard_labels:\n            hard_labels = [\"NO\"]\n\n        results.append({\n            \"test_case\": \"EXIST2025\",\n            \"id\": tweet_id,\n            \"value\": hard_labels  # Final hard labels\n        })\n\n    # Save results\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(results, f, indent=4)\n\n    print(f\"Hard label predictions saved to {output_file}\")\n\n# Run hard label prediction using soft model\npredict_hard_labels_from_soft_model(english_dev_tweets, english_dev_ids, model_en, tokenizer, label_classes, \"EXIST2025_dev_predictions_hard_en.json\")\npredict_hard_labels_from_soft_model(spanish_dev_tweets, spanish_dev_ids, model_es, tokenizer, label_classes, \"EXIST2025_dev_predictions_hard_es.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T08:48:42.533982Z","iopub.execute_input":"2025-04-24T08:48:42.534291Z","iopub.status.idle":"2025-04-24T08:53:12.570663Z","shell.execute_reply.started":"2025-04-24T08:48:42.534259Z","shell.execute_reply":"2025-04-24T08:53:12.569802Z"}},"outputs":[{"name":"stdout","text":"Hard label predictions saved to EXIST2025_dev_predictions_hard_en.json\nHard label predictions saved to EXIST2025_dev_predictions_hard_es.json\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"# Merging soft models ka dev set predictions","metadata":{}},{"cell_type":"code","source":"import json\n\n# Load the Spanish predictions\nwith open(\"/kaggle/working/EXIST2025_dev_predictions_es.json\", \"r\", encoding=\"utf-8\") as f:\n    es_data = json.load(f)\n\n# Load the English predictions\nwith open(\"/kaggle/working/EXIST2025_dev_predictions_en.json\", \"r\", encoding=\"utf-8\") as f:\n    en_data = json.load(f)\n\n# Assuming both files contain lists of predictions, merge them\nif isinstance(es_data, list) and isinstance(en_data, list):\n    merged_data = es_data + en_data\nelse:\n    raise ValueError(\"JSON structure is not a list. Ensure both files contain lists.\")\n\n# Save to a new file\noutput_filename = \"EXIST2025_dev_predictions_merged_soft.json\"\nwith open(output_filename, \"w\", encoding=\"utf-8\") as f:\n    json.dump(merged_data, f, indent=4, ensure_ascii=False)\n\nprint(f\"Merging complete! Saved to {output_filename}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T08:53:22.321288Z","iopub.execute_input":"2025-04-24T08:53:22.321617Z","iopub.status.idle":"2025-04-24T08:53:22.356953Z","shell.execute_reply.started":"2025-04-24T08:53:22.321593Z","shell.execute_reply":"2025-04-24T08:53:22.356207Z"}},"outputs":[{"name":"stdout","text":"Merging complete! Saved to EXIST2025_dev_predictions_merged_soft.json\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"import json\nimport numpy as np\n\n# Load your predictions file\nwith open('EXIST2025_dev_predictions_merged_soft.json', 'r') as f:\n    predictions = json.load(f)\n\n# Define the snapping values (multiples of 1/6)\nsnap_vals = np.array([i / 6 for i in range(7)])  # [0.0, 0.1667, ..., 1.0]\n\ndef snap_to_nearest_sixth(value):\n    return float(snap_vals[np.argmin(np.abs(snap_vals - value))])\n\n# Snap each value in the 'value' dict\nfor entry in predictions:\n    entry['value'] = {k: snap_to_nearest_sixth(v) for k, v in entry['value'].items()}\n\n# Save the snapped predictions to a new file\nwith open('EXIST2025_dev_predictions_snapped_soft.json', 'w') as f:\n    json.dump(predictions, f, indent=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T08:53:27.554949Z","iopub.execute_input":"2025-04-24T08:53:27.555232Z","iopub.status.idle":"2025-04-24T08:53:27.611406Z","shell.execute_reply.started":"2025-04-24T08:53:27.555210Z","shell.execute_reply":"2025-04-24T08:53:27.610799Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"# Merging hard models ka dev set predictions","metadata":{}},{"cell_type":"code","source":"import json\n\n# Load the Spanish predictions\nwith open(\"/kaggle/working/EXIST2025_dev_predictions_hard_es.json\", \"r\", encoding=\"utf-8\") as f:\n    es_data = json.load(f)\n\n# Load the English predictions\nwith open(\"/kaggle/working/EXIST2025_dev_predictions_hard_en.json\", \"r\", encoding=\"utf-8\") as f:\n    en_data = json.load(f)\n\n# Assuming both files contain lists of predictions, merge them\nif isinstance(es_data, list) and isinstance(en_data, list):\n    merged_data = es_data + en_data\nelse:\n    raise ValueError(\"JSON structure is not a list. Ensure both files contain lists.\")\n\n# Save to a new file\noutput_filename = \"EXIST2025_dev_predictions_merged_hard.json\"\nwith open(output_filename, \"w\", encoding=\"utf-8\") as f:\n    json.dump(merged_data, f, indent=4, ensure_ascii=False)\n\nprint(f\"Merging complete! Saved to {output_filename}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T08:53:33.748006Z","iopub.execute_input":"2025-04-24T08:53:33.748313Z","iopub.status.idle":"2025-04-24T08:53:33.765985Z","shell.execute_reply.started":"2025-04-24T08:53:33.748287Z","shell.execute_reply":"2025-04-24T08:53:33.765286Z"}},"outputs":[{"name":"stdout","text":"Merging complete! Saved to EXIST2025_dev_predictions_merged_hard.json\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"# Metric Calculation","metadata":{}},{"cell_type":"code","source":"pip install pyevall","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T08:53:38.970879Z","iopub.execute_input":"2025-04-24T08:53:38.971278Z","iopub.status.idle":"2025-04-24T08:53:42.397442Z","shell.execute_reply.started":"2025-04-24T08:53:38.971246Z","shell.execute_reply":"2025-04-24T08:53:42.396481Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyevall in /usr/local/lib/python3.10/dist-packages (0.1.78)\nRequirement already satisfied: jsbeautifier==1.14.9 in /usr/local/lib/python3.10/dist-packages (from pyevall) (1.14.9)\nRequirement already satisfied: jsonschema==4.23.0 in /usr/local/lib/python3.10/dist-packages (from pyevall) (4.23.0)\nRequirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (from pyevall) (1.26.4)\nRequirement already satisfied: pandas==2.2.3 in /usr/local/lib/python3.10/dist-packages (from pyevall) (2.2.3)\nRequirement already satisfied: setuptools==69.5.1 in /usr/local/lib/python3.10/dist-packages (from pyevall) (69.5.1)\nRequirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.10/dist-packages (from pyevall) (0.9.0)\nRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from jsbeautifier==1.14.9->pyevall) (1.17.0)\nRequirement already satisfied: editorconfig>=0.12.2 in /usr/local/lib/python3.10/dist-packages (from jsbeautifier==1.14.9->pyevall) (0.17.0)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.23.0->pyevall) (25.1.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.23.0->pyevall) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.23.0->pyevall) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema==4.23.0->pyevall) (0.22.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->pyevall) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->pyevall) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->pyevall) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->pyevall) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->pyevall) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->pyevall) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->pyevall) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->pyevall) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->pyevall) (2025.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy==1.26.4->pyevall) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy==1.26.4->pyevall) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy==1.26.4->pyevall) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy==1.26.4->pyevall) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy==1.26.4->pyevall) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"# This is hard metrics","metadata":{}},{"cell_type":"code","source":"from pyevall.evaluation import PyEvALLEvaluation\nfrom pyevall.utils.utils import PyEvALLUtils\n\npredictions = \"/kaggle/working/EXIST2025_dev_predictions_merged_hard.json\"         \ngold = \"/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_gold_hard.json\" \ntest = PyEvALLEvaluation() \nparams= dict() \nparams[PyEvALLUtils.PARAM_REPORT]= PyEvALLUtils.PARAM_OPTION_REPORT_EMBEDDED  \n# metrics=[\"ICMSoft\", \"ICMSoftNorm\", \"CrossEntropy\"]     # for soft    \nmetrics=[\"ICM\", \"ICMNorm\" ,\"FMeasure\"] \nTASK1_3_HIERARCHY = {\"YES\":[\"IDEOLOGICAL-INEQUALITY\",\"STEREOTYPING-DOMINANCE\",\"OBJECTIFICATION\", \"SEXUAL-VIOLENCE\", \"MISOGYNY-NON-SEXUAL-VIOLENCE\"], \"NO\":[]}\nparams[PyEvALLUtils.PARAM_HIERARCHY]= TASK1_3_HIERARCHY  \nreport= test.evaluate(predictions, gold, metrics, **params) \nreport.print_report()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T08:54:16.789572Z","iopub.execute_input":"2025-04-24T08:54:16.789968Z","iopub.status.idle":"2025-04-24T08:54:19.036296Z","shell.execute_reply.started":"2025-04-24T08:54:16.789934Z","shell.execute_reply":"2025-04-24T08:54:19.035421Z"}},"outputs":[{"name":"stdout","text":"2025-04-24 08:54:16,800 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM', 'ICMNorm', 'FMeasure']\n2025-04-24 08:54:16,986 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-04-24 08:54:17,489 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Normalized evaluation method\n2025-04-24 08:54:17,492 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-04-24 08:54:18,058 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n2025-04-24 08:54:18,546 - pyevall.metrics.metrics - INFO -             evaluate() - Executing fmeasure evaluation method\n{\n  \"metrics\": {\n    \"ICM\": {\n      \"name\": \"Information Contrast model\",\n      \"acronym\": \"ICM\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": -0.26853334421003194\n        }],\n        \"average_per_test_case\": -0.26853334421003194\n      }\n    },\n    \"ICMNorm\": {\n      \"name\": \"Normalized Information Contrast Model\",\n      \"acronym\": \"ICM-Norm\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 0.44018783984189047\n        }],\n        \"average_per_test_case\": 0.44018783984189047\n      }\n    },\n    \"FMeasure\": {\n      \"name\": \"F-Measure\",\n      \"acronym\": \"F1\",\n      \"description\": \"Coming soon!\\\\nThe evaluation WARNING.\",\n      \"status\": \"WARNING\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"classes\": {\n            \"IDEOLOGICAL-INEQUALITY\": 0.4610951008645533,\n            \"STEREOTYPING-DOMINANCE\": 0.5728155339805825,\n            \"MISOGYNY-NON-SEXUAL-VIOLENCE\": 0.29824561403508776,\n            \"NO\": 0.7412935323383085,\n            \"SEXUAL-VIOLENCE\": 0.39593908629441626,\n            \"OBJECTIFICATION\": 0.4203821656050955\n          },\n          \"average\": 0.4816285055196739\n        }],\n        \"average_per_test_case\": 0.4816285055196739\n      },\n      \"preconditions\": {\n        \"METRIC_PRECONDITION_HIERARCHY_NOT_VALID_FOR_METRIC\": {\n          \"name\": \"METRIC_PRECONDITION_HIERARCHY_NOT_VALID_FOR_METRIC\",\n          \"description\": \"The hierarchy is provided for the evaluation but this metric does not allow to use it. Hierarchy is ignored.\\\\nThe metric name is: F-Measure.\\\\nTest case(s) name: EXIST2025.\",\n          \"status\": \"WARNING\",\n          \"test_cases\": [\"EXIST2025\"]\n        }\n      }\n    }\n  },\n  \"files\": {\n    \"EXIST2025_dev_predictions_merged_hard.json\": {\n      \"name\": \"EXIST2025_dev_predictions_merged_hard.json\",\n      \"status\": \"OK\",\n      \"gold\": false,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_predictions_merged_hard.json.\",\n      \"errors\": {}\n    },\n    \"EXIST2025_dev_task1_3_gold_hard.json\": {\n      \"name\": \"EXIST2025_dev_task1_3_gold_hard.json\",\n      \"status\": \"OK\",\n      \"gold\": true,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_task1_3_gold_hard.json.\",\n      \"errors\": {}\n    }\n  }\n}\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"# These are soft metrics","metadata":{}},{"cell_type":"code","source":"from pyevall.evaluation import PyEvALLEvaluation\nfrom pyevall.utils.utils import PyEvALLUtils\n\n# Define file paths\npredictions = \"/kaggle/working/EXIST2025_dev_predictions_merged_soft.json\"  # Change to your actual prediction file\ngold = \"/kaggle/input/exist2025-all/EXIST2025_dev_task1_3_gold_soft.json\"   # Change to your actual gold file\n\n# Define hierarchical structure for subtask 1.3\nTASK1_3_HIERARCHY = {\n    \"YES\": [\"IDEOLOGICAL-INEQUALITY\", \"STEREOTYPING-DOMINANCE\",\n            \"OBJECTIFICATION\", \"SEXUAL-VIOLENCE\", \"MISOGYNY-NON-SEXUAL-VIOLENCE\"],\n    \"NO\": []\n}\n\n# Initialize PyEvALL evaluation\nevaluator = PyEvALLEvaluation()\n\n# Set evaluation parameters\nparams = dict()\nparams[PyEvALLUtils.PARAM_HIERARCHY] = TASK1_3_HIERARCHY\nparams[PyEvALLUtils.PARAM_REPORT] = PyEvALLUtils.PARAM_OPTION_REPORT_EMBEDDED  # Embedded report\n\n# Define evaluation metrics\n# metrics = [\"ICM\", \"ICMNorm\", \"FMeasure\"]\nmetrics=[\"ICMSoft\", \"ICMSoftNorm\"]\n\n# Run evaluation\nreport = evaluator.evaluate(predictions, gold, metrics, **params)\n\n# Print evaluation report\nreport.print_report()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T08:57:36.126694Z","iopub.execute_input":"2025-04-24T08:57:36.127027Z","iopub.status.idle":"2025-04-24T08:57:40.633747Z","shell.execute_reply.started":"2025-04-24T08:57:36.127005Z","shell.execute_reply":"2025-04-24T08:57:40.632852Z"}},"outputs":[{"name":"stdout","text":"2025-04-24 08:57:36,133 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICMSoft', 'ICMSoftNorm']\n2025-04-24 08:57:36,483 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Soft evaluation method\n2025-04-24 08:57:37,877 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM-Soft Normalized evaluation method\n2025-04-24 08:57:37,880 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Soft evaluation method\n2025-04-24 08:57:39,252 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM Soft evaluation method\n{\n  \"metrics\": {\n    \"ICMSoft\": {\n      \"name\": \"Information Contrast Model Soft\",\n      \"acronym\": \"ICM-Soft\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": -2.98347663559963\n        }],\n        \"average_per_test_case\": -2.98347663559963\n      }\n    },\n    \"ICMSoftNorm\": {\n      \"name\": \"Normalized Information Contrast Model Soft\",\n      \"acronym\": \"ICM-Soft-Norm\",\n      \"description\": \"Coming soon!\",\n      \"status\": \"OK\",\n      \"results\": {\n        \"test_cases\": [{\n          \"name\": \"EXIST2025\",\n          \"average\": 0.3418729493520982\n        }],\n        \"average_per_test_case\": 0.3418729493520982\n      }\n    }\n  },\n  \"files\": {\n    \"EXIST2025_dev_predictions_merged_soft.json\": {\n      \"name\": \"EXIST2025_dev_predictions_merged_soft.json\",\n      \"status\": \"OK\",\n      \"gold\": false,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_predictions_merged_soft.json.\",\n      \"errors\": {}\n    },\n    \"EXIST2025_dev_task1_3_gold_soft.json\": {\n      \"name\": \"EXIST2025_dev_task1_3_gold_soft.json\",\n      \"status\": \"OK\",\n      \"gold\": true,\n      \"description\": \"The file is correctly parser without errors or warnings.\\\\nFile name: EXIST2025_dev_task1_3_gold_soft.json.\",\n      \"errors\": {}\n    }\n  }\n}\n","output_type":"stream"}],"execution_count":39}]}